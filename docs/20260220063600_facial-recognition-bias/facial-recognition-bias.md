---
marp: true
theme: gaia
class: invert
size: 16:9
paginate: true
header: "顔認識AIとバイアス"
footer: "© 2026"
style: |
  section pre code { font-size: 0.58em; line-height: 1.4; }
  
---

<!-- _class: lead -->
# 顔認識AIのバイアス
テクノロジーは差別を増幅するか

- 白人男性の認識精度99% vs 黒人女性の認識精度65%
- AIは「公平」ではない ― 学習データの偏りが差別を再生産する
- 技術と公正の間で揺れる社会的議論


---

# アジェンダ

- 1. 顔認識AIの仕組みと普及
- 2. ジョイ・ブオラムウィニの告発
- 3. バイアスの構造的原因
- 4. 現実世界での被害事例
- 5. 規制と企業の対応
- 6. 公平なAIは実現可能か


---

<!-- _class: lead -->
# 顔認識AIの仕組みと普及


---

# 顔認識AIの現在地

- **技術の基盤：** 深層学習（CNN）による顔の特徴点抽出
- 数百万枚の顔画像で学習し、個人を識別
- **普及状況（2025年時点）：**
- 空港のゲート（成田・羽田で導入済み）
- スマートフォンのFace ID（Apple、10億台以上）
- 中国の監視カメラ網（6億台以上、全国民を追跡可能）
- → **世界で最も広く展開されたAIアプリケーション**


---

<!-- _class: lead -->
# ジョイ・ブオラムウィニの告発


---

# Gender Shades プロジェクト（2018年）

- **MIT研究者ジョイ・ブオラムウィニの画期的な研究：**
- Microsoft、IBM、Face++の顔認識AIを評価
- 白人男性の精度：**99.2%**
- 黒人女性の精度：**65.3%** ← 3人に1人を間違える
- 肌の色が濃いほど、女性であるほど精度が低下
- → **「AI is not neutral」** ― AIは中立ではない


---

# なぜ精度に差が出るのか

- **学習データの偏り：** 訓練画像の70%以上が白人男性
- インターネット上の画像データ自体が欧米中心
- **開発チームの偏り：** AI研究者の80%以上が男性、大多数が白人
- テスト段階で多様な顔でテストしていない
- **評価指標の偏り：** 「全体精度」だけ見ると問題が隠れる
- 全体99%でも特定集団で65% → 「平均」が差別を隠蔽する


---

<!-- _class: lead -->
# バイアスの構造的原因


---

# 3層のバイアス構造

- **Layer 1: データバイアス** ― 学習データが特定の属性に偏っている
- **Layer 2: アルゴリズムバイアス** ― モデルが多数派に最適化される
- **Layer 3: 社会バイアス** ― 技術を使う文脈が差別を増幅する
- ---
- 例：犯罪予測AI → 過去の逮捕データ（人種バイアスあり）で学習
- → 黒人地区を「危険」と予測 → 警察が重点配備 → 逮捕増加
- → **AIが差別のフィードバックループを作る**


---

<!-- _class: lead -->
# 現実世界での被害事例


---

# 顔認識AIによる誤認逮捕

- **ロバート・ウィリアムズ（2020年、デトロイト）**
- 顔認識AIが万引き容疑者として誤認 → 無実の黒人男性を逮捕
- 家族の前で手錠をかけられ、30時間拘留された
- **米国では少なくとも7件の顔認識誤認逮捕が公表**（全て黒人）
- ---
- **中国：** 顔認識で横断歩道違反者の顔を公開表示 → 広告の顔を誤認
- → **精度の問題が人権侵害に直結する**


---

<!-- _class: lead -->
# 規制と企業の対応


---

# 各国・企業の対応状況

- **EU AI Act（2024年）** ― 公共空間でのリアルタイム顔認識を原則禁止
- **米国** ― サンフランシスコ等の都市が警察の顔認識利用を禁止
- **日本** ― 明確な規制なし（個人情報保護法の範囲内で運用）
- **IBM** ― 2020年に顔認識事業から撤退
- **Microsoft** ― 警察への販売を一時停止（連邦法整備まで）
- **Amazon** ― Rekognitionの警察利用を一時モラトリアム


---

<!-- _class: lead -->
# 公平なAIは実現可能か


---

# 技術的・制度的アプローチ

- **1. 多様なデータセット** ― 全人種・性別を均等に含む訓練データ
- **2. 公平性の定量指標** ― サブグループごとの精度を個別に評価
- **3. 独立監査** ― 第三者によるバイアス監査を義務化
- **4. 開発チームの多様性** ― 作る人が多様でなければ公平な製品は生まれない
- **5. 用途の制限** ― 高リスク用途（法執行等）での使用を規制
- → **技術だけでは解決しない。制度と文化の変革が必要**


---

<!-- _class: lead -->
# まとめ

- 顔認識AIのバイアスは**技術的問題であり、社会的問題**
- 学習データ・開発チーム・評価方法の全てに偏りがある
- 誤認逮捕という形で現実の人権侵害が発生している
- EU・米国で規制が進むが、グローバルな統一基準はない
- **問い：** 「高精度」は「公平」を意味するか？全体99%で十分か？


---

# 参考文献

- - **学術研究:**
- - [Gender Shades (Buolamwini & Gebru, 2018)](http://gendershades.org/)
- - [Coded Bias (Netflix Documentary)](https://www.netflix.com/title/81328723)
- - **規制:**
- - [EU AI Act Official Text](https://artificialintelligenceact.eu/)
- - [NIST Face Recognition Vendor Test](https://www.nist.gov/programs-projects/face-recognition-vendor-test-frvt)

