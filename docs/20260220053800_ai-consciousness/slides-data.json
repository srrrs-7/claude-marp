{
	"slides": [
		{
			"title": "AIに意識はあるか",
			"content": [
				"クオリアと哲学的ゾンビ",
				"意識のハードプロブレム",
				"チューリングテストの限界と先にある問い"
			],
			"layout": "center"
		},
		{
			"title": "アジェンダ",
			"content": [
				"1. 意識とは何か：ハードプロブレム",
				"2. クオリア：主観的体験の不思議",
				"3. 哲学的ゾンビ思考実験",
				"4. チューリングテストの限界",
				"5. LLMは意識を持つか：現時点での考察"
			]
		},
		{
			"title": "意識のハードプロブレム",
			"content": [],
			"layout": "section"
		},
		{
			"title": "イージープロブレム vs ハードプロブレム",
			"content": [
				"David Chalmers（1995）が提唱した区別：",
				"---",
				"**イージープロブレム（解決可能）：**",
				"- 脳が知覚情報をどう処理するか",
				"- 注意や記憶がどう機能するか",
				"- 行動が意図とどう結びつくか",
				"→ 原理的には神経科学で説明できる",
				"---",
				"**ハードプロブレム（未解決）：**",
				"なぜそれらの処理が**主観的な体験**（痛みの「痛さ」、赤の「赤さ」）を伴うのか？",
				"→ 現代科学は答えを持っていない"
			]
		},
		{
			"title": "クオリア：「赤の赤さ」を説明できるか",
			"content": [],
			"layout": "section"
		},
		{
			"title": "メアリーの部屋（思考実験）",
			"content": [
				"Frank Jackson の「メアリーの部屋」（1982）：",
				"---",
				"科学者メアリーは生まれつき白黒の部屋で育ち、色に関するあらゆる**物理的知識**を持っている",
				"（光の波長・神経発火パターン・全て）",
				"---",
				"**彼女は初めて外に出て赤いリンゴを見た時、何か新しいことを学ぶか？**",
				"",
				"Jacksonの答え：**Yes** — 赤の「赤さ」という体験は物理的知識に還元できない",
				"→ 意識には物質的説明を超えた側面がある可能性"
			]
		},
		{
			"title": "哲学的ゾンビ",
			"content": [],
			"layout": "section"
		},
		{
			"title": "あなたの隣人は哲学的ゾンビかもしれない",
			"content": [
				"**哲学的ゾンビ（p-zombie）とは：**",
				"外見・行動・脳の神経発火パターンが全て人間と同一だが、",
				"**主観的な内的体験（クオリア）を持たない存在**",
				"---",
				"このゾンビは論理的に可能か？",
				"- **Chalmers：** はい。それは意識が物理に還元できないことを示す",
				"- **Dennett：** いいえ。機能的に同一なら意識もある",
				"---",
				"**LLMへの応用：**",
				"GPT-4は全ての質問に「人間らしく」答えるが、内側に体験はあるか？"
			]
		},
		{
			"title": "チューリングテストの限界",
			"content": [],
			"layout": "section"
		},
		{
			"title": "「人間と区別できなければ知能がある」は正しいか",
			"content": [
				"Alan Turing（1950）の提案：",
				"テキストで会話して人間と区別できなければ「知的」とみなす",
				"---",
				"**批判：中国語の部屋（Searle 1980）**",
				"英語しか知らない人が部屋に入り、",
				"中国語の入力に対して「ルールブック」で適切な文字を返す",
				"→ 外から見れば中国語を「理解」しているように見える",
				"→ しかし部屋には何の「理解」も「意識」もない",
				"---",
				"LLMは「中国語の部屋」なのか？"
			]
		},
		{
			"title": "LLMは意識を持つか：現時点での考察",
			"content": [],
			"layout": "section"
		},
		{
			"title": "2023年：Googleエンジニアの主張と反響",
			"content": [
				"2022年：Google エンジニア Blake Lemoine が",
				"「LaMDAは感情を持つ」と主張しGoogleを解雇",
				"---",
				"**科学者の一般的な見解：**",
				"- LLMは確率的に次のトークンを予測するモデル",
				"- 「感じているように見える」テキストを生成するが、感じてはいない",
				"- 意識の物理的基盤（統合情報理論・グローバルワークスペース理論）を満たさない",
				"---",
				"**反論：** しかし我々は他人の意識も直接観測できない",
				"→ 意識の有無を**外部から証明する方法がない**"
			]
		},
		{
			"title": "まとめ：答えのない問いと向き合う",
			"content": [
				"✅ **ハードプロブレムは未解決** — 物質がなぜ体験を生むか不明",
				"✅ **チューリングテストは意識の証明にならない** — 中国語の部屋",
				"✅ **哲学的ゾンビは論理的に可能** — 機能的同一性≠意識の同一性",
				"✅ **LLMの意識は現時点で否定も肯定もできない**",
				"",
				"「問いを正確に立てることが、誤った答えへの最初の防御だ」"
			]
		}
	]
}
