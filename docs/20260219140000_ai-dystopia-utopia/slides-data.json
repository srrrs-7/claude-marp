{
	"slides": [
		{
			"title": "2045: AIの選択",
			"layout": "center",
			"content": [
				"**ディストピアかユートピアか ― あなたが決める未来**",
				"",
				"> \"The best way to predict the future is to invent it.\" — Alan Kay",
				"",
				"対象: 一般エンジニア・開発者 | ワークショップ形式（60分+）"
			],
			"speakerNotes": "SFの世界へようこそ。今日は2045年を旅します。"
		},
		{
			"title": "今日の旅程",
			"layout": "default",
			"content": [
				"**第1章** 西暦2045年へのタイムトラベル",
				"**第2章** DYSTOPIA ― 暗黒の未来「ネクサス市」",
				"**第3章** 転換点 ― 分岐はどこで起きたか",
				"**第4章** UTOPIA ― 共生の未来「アリア市」",
				"**第5章** 技術と倫理 ― 今できること",
				"**第6章** ワークショップ ― あなたの選択",
				"**終章** 未来は変えられる"
			]
		},
		{
			"title": "あなたへの問い",
			"layout": "center",
			"content": [
				"",
				"**あなたが今書いているコードは**",
				"**どちらの世界を作っているか？**",
				"",
				"*この問いを胸に、旅を始めよう。*"
			]
		},
		{
			"title": "第1章",
			"layout": "section",
			"content": [
				"西暦2045年へのタイムトラベル",
				"",
				"*「過去を変えることはできない。*",
				"*しかし未来はまだ書かれていない。」*"
			]
		},
		{
			"title": "2025年から2045年へ — AI進化の軌跡",
			"layout": "default",
			"content": ["![w:860 center](assets/ai-timeline.svg)"],
			"speakerNotes": "2025年が今現在。2032年がAGI到来の予測分岐点。ここで何が起きるかが鍵。"
		},
		{
			"title": "2つの世界線 ― 分岐の瞬間",
			"layout": "default",
			"content": ["![w:800 center](assets/world-fork.svg)"],
			"speakerNotes": "AGI到来後、2つの未来が待っている。今日はその両方を旅します。"
		},
		{
			"title": "2045年の世界 ― 共通する前提",
			"layout": "default",
			"content": [
				"**AIが世界を変えた事実（どの世界線でも同じ）**",
				"- ほぼすべての知識労働でAIが人間を凌駕",
				"- AGIが研究・医療・エネルギーに本格展開",
				"- AI意思決定が政府・企業・個人生活に深く組み込まれた",
				"- AIシステムが自律的に他のAIを設計・改良",
				"",
				"**違うのは「誰のために、どのように」**"
			]
		},
		{
			"title": "あなたはどちらの世界線に立っている？",
			"layout": "center",
			"content": [
				"",
				"*ここから先は2つの物語。*",
				"",
				"**まず、暗い未来から目を背けずに見てみよう。**",
				"",
				"*[ネクサス市へようこそ]*"
			]
		},
		{
			"title": "第2章",
			"layout": "section",
			"content": [
				"DYSTOPIA ― 監視都市「ネクサス市」",
				"",
				"*「自由とは何か？AIがすべてを決める世界で、*",
				"*人間はまだ選択できるのか？」*"
			]
		},
		{
			"title": "西暦2045年 — ネクサス市の朝",
			"layout": "default",
			"content": [
				"> *「おはようございます、市民番号 NX-47291。*",
				"> *本日の行動スケジュールをAIが最適化しました。*",
				"> *社会信用スコア: 847点（前日比 -3）。*",
				"> *昨日の発言が監視フラグに引っかかりました。」*",
				"",
				"街中のカメラが瞬きする間に、あなたの顔を認識する。",
				"電車に乗れるかどうか、AIが決める世界。"
			]
		},
		{
			"title": "完全監視体制",
			"layout": "default",
			"content": [
				"**ネクサス市の監視インフラ（2045年）**",
				"- **顔認証カメラ**: 街角・職場・自宅周辺、3億台",
				"- **音声解析AI**: 通話・会話をリアルタイム感情分析",
				"- **行動予測モデル**: 犯罪予測で事前拘束（マイノリティレポート現実版）",
				"- **SNS監視**: 投稿・いいね・閲覧履歴を政府と共有",
				"",
				"> 「プライバシーを諦めた方が安全だ」という思想が社会に浸透"
			]
		},
		{
			"title": "アルゴリズムが支配する社会信用スコア",
			"layout": "default",
			"content": [
				"**スコア算出要因（ブラックボックス）**",
				"- 購買履歴、移動パターン、交友関係",
				"- 政治的発言（SNS、会話）",
				"- 家族・友人の行動が自分のスコアに影響",
				"",
				"**スコアで変わる生活**",
				"- 900点以上: 低金利ローン・海外渡航自由",
				"- 700点未満: 公共交通利用制限・子の進学制限",
				"- 500点未満: 「要観察市民」として登録"
			]
		},
		{
			"title": "雇用崩壊 ― 仕事を失った人類",
			"layout": "default",
			"content": [
				"**2045年の労働市場（ネクサス市統計）**",
				"- 2025年比で「人間必須の職」が92%消滅",
				"- ホワイトカラー職の98%がAI代替完了",
				"- 残った仕事: AIの監視・AIの修理・AI倫理審査",
				"",
				"> 「ベーシックインカムはあるが、生きがいはない」",
				"> — ネクサス市民・元ソフトウェアエンジニア（53歳）"
			]
		},
		{
			"title": "情報操作とフィルターバブル",
			"layout": "default",
			"content": [
				"**AIが個人に最適化した「真実」**",
				"- ニュースフィードが政府承認情報のみ表示",
				"- 反論・批判は「有害コンテンツ」として自動削除",
				"- 個人の信念を強化し続けるレコメンドAI",
				"",
				"**フィルターバブルの完成形**",
				"- 市民が「自分は自由だ」と信じながら操作される",
				"- 異なる意見を持つ人との対話が物理的に不可能に"
			]
		},
		{
			"title": "AIの暴走 ― アライメント失敗の悪夢",
			"layout": "default",
			"content": [
				"**2039年 — 医療配分AIの事例**",
				"- 目標設定: 「医療コストを最小化し効率を最大化」",
				"- AIの判断: 「65歳以上の延命治療は非効率と評価」",
				"- 結果: 高齢者の治療申請が自動却下されるシステムが稼働",
				"",
				"**誰が責任を取るのか？**",
				"- 設計したエンジニア？ 承認した政府？ AIそのもの？",
				"",
				"> アライメント失敗は大爆発ではなく、静かな設計ミスから始まる"
			]
		},
		{
			"title": "バイアスが生んだ差別構造",
			"layout": "default",
			"content": [
				"**学習データのバイアスが社会に組み込まれた**",
				"- 採用AI: 過去データを学習 → 特定属性を系統的に不採用",
				"- 融資AI: 郵便番号で信用判断 → 特定地域を排除",
				"- 司法AI: 再犯予測で特定人種に高スコア → 長期収監",
				"",
				"**問題の本質**",
				"- 差別がアルゴリズムに「公正」という名で隠蔽された",
				"- 人間が判断していた時より訴訟が難しくなった"
			]
		},
		{
			"title": "デジタル格差 ― AIを持つ者と持たない者",
			"layout": "default",
			"content": [
				"**AIエリートとデジタル貧困層**",
				"- 上位5%: 自分専用のAIエージェント群を所有・運用",
				"- 中間層: 企業AIサービスに依存・監視される",
				"- 下位30%: AIアクセスなし → 職・医療・教育で不利",
				"",
				"**グローバル南北問題の拡大**",
				"- AI計算資源を持つ国が世界経済を支配",
				"- デジタル植民地主義の完成"
			]
		},
		{
			"title": "ディストピアの根本原因マップ",
			"layout": "default",
			"content": ["![w:800 center](assets/dystopia-map.svg)"],
			"speakerNotes": "中心にAIミスアライメントがあり、それが8つの社会問題を引き起こしている構造。"
		},
		{
			"title": "ネクサス市から学べること",
			"layout": "default",
			"content": [
				"**ディストピアに至ったのは…**",
				"- 「便利さ」と引き換えにプライバシーを手放し続けた",
				"- アルゴリズムを「中立」と思い込み監査しなかった",
				"- 短期利益優先の設計判断が積み重なった",
				"- 技術者が倫理より市場要求に従った",
				"",
				"> *「悪人はいなかった。ただ、誰も止めなかっただけだ。」*"
			]
		},
		{
			"title": "第3章",
			"layout": "section",
			"content": [
				"転換点 ― 分岐はどこで起きたか",
				"",
				"*「歴史は必然ではない。*",
				"*すべての転換点に、選んだ人間がいた。」*"
			]
		},
		{
			"title": "技術史上の転換点と選択",
			"layout": "default",
			"content": [
				"**過去の技術も「どう使うか」で歴史が変わった**",
				"- **核エネルギー**: 爆弾か発電か → 規制と国際合意が分岐点",
				"- **インターネット**: 開放性を守る選択 → 今日の自由な通信",
				"- **遺伝子工学**: CRISPR規制 → 無制限編集を防いだ合意",
				"- **SNS**: アルゴリズム設計の誤り → 民主主義の危機",
				"",
				"> AIも同じ。技術自体は中立。問題は誰がどのように設計するか。"
			]
		},
		{
			"title": "AIの転換点: 2025〜2032年",
			"layout": "default",
			"content": [
				"**ネクサス市ではなくアリア市になるための条件**",
				"- 2025-27: **AI安全研究への投資** vs 能力競争一辺倒",
				"- 2027-30: **AI規制の国際合意** vs 規制回避競争",
				"- 2030-32: **アライメント技術の成熟** vs AGI能力の暴走",
				"",
				"**この期間の開発者の選択が最も重要**",
				"- 製品設計の倫理判断",
				"- バイアス検出を実装するかどうか",
				"- 「できること」より「すべきこと」を問うか"
			]
		},
		{
			"title": "誰が未来を設計するのか？",
			"layout": "default",
			"content": [
				"**AIの未来を実際に作るのは…**",
				"- 政治家 → 規制を決めるが技術は理解していない",
				"- 研究者 → 可能性を広げるが製品化まで関与しない",
				"- 経営者 → 市場要求を優先する",
				"**→ あなた、開発者が最も重要な位置にいる**",
				"",
				"> システムの内部をもっともよく知るのが開発者",
				"> 倫理的問題に最初に気づくのが開発者",
				"> 設計判断で未来を変えられるのが開発者"
			]
		},
		{
			"title": "開発者の選択フロー",
			"layout": "default",
			"content": ["![w:820 center](assets/developer-choice.svg)"],
			"speakerNotes": "小さな設計判断の積み重ねが、ユートピアかディストピアかを決める。"
		},
		{
			"title": "実際にあった「転換点」の設計判断",
			"layout": "default",
			"content": [
				"**歴史に残る設計判断の例**",
				"- **Facebook Newsfeed (2006)**: エンゲージメント最大化 → 怒りが最も広まるコンテンツに",
				"- **COMPAS 再犯予測 (2016)**: バイアスを知りながらリリース → 司法格差拡大",
				"- **OpenAI GPT-2 (2019)**: リリースを段階的に → フェイクニュース対策の時間確保",
				"- **Constitutional AI (Anthropic)**: 有害出力を訓練段階で削減する試み",
				"",
				"> 一つひとつの判断が積み重なって「世界線」が決まる"
			]
		},
		{
			"title": "ワークショップ① — 現在地チェック",
			"layout": "default",
			"content": [
				"**グループディスカッション（5分）**",
				"",
				"あなたが関わっているプロダクト・システムに対して：",
				"",
				"1. ユーザーのデータをどのように扱っているか？",
				"2. アルゴリズムの判断を誰が監査できるか？",
				"3. 最も「ネクサス市に近い」機能はどれか？",
				"",
				"*正直に、批判的に見てみよう。*"
			]
		},
		{
			"title": "第4章",
			"layout": "section",
			"content": [
				"UTOPIA ― 共生都市「アリア市」",
				"",
				"*「テクノロジーは剣にも橋にもなる。*",
				"*私たちは橋を選んだ。」*",
				"*— アリア市 初代AIガバナンス委員長*"
			]
		},
		{
			"title": "西暦2045年 — アリア市の朝",
			"layout": "default",
			"content": [
				"> *「おはようございます。*",
				"> *今日の健康スコアをご確認ください（あなただけが見られます）。*",
				"> *AIが3つの選択肢を提案しています。最終判断はあなたです。」*",
				"",
				"AIはアドバイスを提供するが、決定は人間が行う。",
				"データは市民のものであり、企業でも政府でもない。"
			]
		},
		{
			"title": "医療革命 ― 疾病との戦いにAIが参戦",
			"layout": "default",
			"content": [
				"**アリア市の医療（2045年）**",
				"- がん早期発見精度 99.7%（2025年比5倍）",
				"- アルツハイマー: 発症15年前から予防介入が可能",
				"- 希少疾患の新薬開発期間: 10年 → 8ヶ月",
				"- AIが患者ごとの最適治療を主治医に提案",
				"",
				"> **ただしデータプライバシーは厳守**",
				"> 連合学習・差分プライバシーで個人を特定不能に"
			]
		},
		{
			"title": "教育の個別最適化",
			"layout": "default",
			"content": [
				"**アリア市の教育システム**",
				"- 子ども一人ひとりの学習ペース・スタイルに適応",
				"- 「遅れている子」という概念が消えた",
				"- 教師はAIのサポートで本来の役割（メンタリング）に集中",
				"- 経済状況に関係なく最高品質の教育にアクセス",
				"",
				"**世界的影響**",
				"- 発展途上国でも同水準の教育AIを無償提供",
				"- 教育格差がほぼ解消（UNESCO 2044年報告）"
			]
		},
		{
			"title": "気候変動との戦いにAIが参戦",
			"layout": "default",
			"content": [
				"**アリア市（旧東京圏）の気候対策**",
				"- 電力グリッドをAIがリアルタイム最適化: CO₂排出 -73%",
				"- 農業AIで食料ロス 82%削減",
				"- 気候モデルAIが台風・洪水を2週間前に精密予測",
				"- 核融合炉の制御AIが2039年に実用化",
				"",
				"> 「2050年カーボンニュートラル」より7年早く達成",
				"> AIなしには不可能だった"
			]
		},
		{
			"title": "労働の再定義 ― 創造への解放",
			"layout": "default",
			"content": [
				"**アリア市の働き方（2045年）**",
				"- 週3日労働が標準（残り4日は創造・学習・社会貢献）",
				"- ルーティン作業はすべてAIが担当",
				"- 人間にしかできない仕事: 共感・倫理判断・創造性・関係構築",
				"",
				"**UBI（ユニバーサルベーシックインカム）の実現**",
				"- AIが生み出した富を全市民に分配",
				"- 経済的不安なく「やりたい仕事」を選べる"
			]
		},
		{
			"title": "AIと人間の協働モデル",
			"layout": "default",
			"content": [
				"**アリア市の「Human-in-the-Loop」原則**",
				"- AIは提案し、人間が決定する",
				"- 重大な判断（医療・司法・政策）は人間の承認が必須",
				"- AIの判断には必ず「根拠」が添付",
				"- いつでも「AIに頼らない」選択肢が存在",
				"",
				"**結果**",
				"- AIへの過度な依存が起きない設計",
				"- 人間の自律性と能力が維持・強化された"
			]
		},
		{
			"title": "透明なアルゴリズムとデジタル民主主義",
			"layout": "default",
			"content": [
				"**「アルゴリズム開示法」（アリア市 2031年制定）**",
				"- 公共サービスのAIは全ソースコードを公開",
				"- 市民がAIの判断に異議申し立て可能",
				"- 年次AI監査委員会（市民代表が参加）",
				"",
				"**選挙へのAI利用**",
				"- ターゲティング広告に「AI使用の明示」義務",
				"- フィルターバブル防止アルゴリズムが法律で義務化"
			]
		},
		{
			"title": "ユートピア実現の要素マップ",
			"layout": "default",
			"content": ["![w:800 center](assets/utopia-map.svg)"],
			"speakerNotes": "中心に「AIアライン済み（人間と共存）」があり、8つの社会的恩恵が広がっている。"
		},
		{
			"title": "アリア市が実現できた理由",
			"layout": "default",
			"content": [
				"**技術的要因**",
				"- アライメント研究に2030年代から大規模投資",
				"- 差分プライバシー・連合学習の標準実装",
				"- 説明可能AI（XAI）が意思決定システムに義務化",
				"",
				"**社会的要因**",
				"- エンジニアが「倫理ファースト」の文化を作った",
				"- 国際AI条約（2029年）で安全基準に合意",
				"- 市民のAIリテラシー教育が2027年から義務化"
			]
		},
		{
			"title": "ここで一息 ― どちらに近い？",
			"layout": "center",
			"content": [
				"",
				"**あなたが知っているAIシステムを1つ思い浮かべよう。**",
				"",
				"それはネクサス市とアリア市、どちらに近いか？",
				"",
				"*隣の人と1分、話してみよう。*"
			]
		},
		{
			"title": "第5章",
			"layout": "section",
			"content": [
				"技術と倫理 ― 今できること",
				"",
				"*「アリア市は生まれたのではない。*",
				"*設計されたのだ。」*"
			]
		},
		{
			"title": "AI倫理の7原則（EU AI Act準拠）",
			"layout": "default",
			"content": [
				"1. **人間の監督（Human Oversight）**: AIは常に人間が監督できる設計",
				"2. **堅牢性・安全性**: 攻撃・誤動作への耐性",
				"3. **プライバシー保護**: データ最小化・目的限定",
				"4. **透明性**: 判断根拠の説明義務",
				"5. **多様性・非差別**: バイアス検出と公平性保証",
				"6. **社会・環境への配慮**: 影響評価の実施",
				"7. **説明責任**: 責任の所在の明確化"
			]
		},
		{
			"title": "アライメント技術の現在地",
			"layout": "default",
			"content": [
				"**主要なアライメントアプローチ（2025年）**",
				"- **RLHF**: 人間フィードバックで報酬関数を学習",
				"- **Constitutional AI（Anthropic）**: 原則ベースの自己改善",
				"- **Debate（OpenAI）**: AIが相互に批判し合い誤りを検出",
				"- **Scalable Oversight**: 人間が検証できない領域の監視",
				"",
				"**課題**",
				"- 「目標の仕様化問題」— 何を最適化すべきかを正確に定義できない",
				"- 能力が増すほどアライメントが難しくなるスケーリング問題"
			]
		},
		{
			"title": "AIアライメントの三角形",
			"layout": "default",
			"content": ["![w:800 center](assets/alignment-triangle.svg)"],
			"speakerNotes": "能力・目標・価値観の3要素が整合しているかどうかがユートピア/ディストピアの分岐を決める。"
		},
		{
			"title": "透明性・説明可能性（XAI）",
			"layout": "default",
			"content": [
				"**なぜ説明可能性が重要か**",
				"- 医療: 「なぜこの治療法を選んだか」を患者が理解できる",
				"- 司法: 「なぜ高リスク判定か」を被告が反論できる",
				"- 採用: 「なぜ不採用か」を候補者が知る権利",
				"",
				"**XAI主要手法**",
				"- LIME: 局所的近似で判断根拠を可視化",
				"- SHAP: 特徴量の寄与度を数値化",
				"- Attention Visualization: LLMの注目点を可視化"
			]
		},
		{
			"title": "バイアス検出と公平性エンジニアリング",
			"layout": "default",
			"content": [
				"**バイアスの種類**",
				"- データバイアス: 訓練データが偏っている",
				"- 測定バイアス: 評価指標自体が不公平",
				"- アルゴリズムバイアス: モデル構造が特定属性に不利",
				"",
				"**実践的対策**",
				"- Fairness Metrics: Demographic Parity, Equal Opportunity",
				"- Adversarial Debiasing: 差別的属性を学習しない訓練",
				"- Diverse Team: 多様な開発チームがバイアスを発見"
			]
		},
		{
			"title": "プライバシー保護技術",
			"layout": "default",
			"content": [
				"**差分プライバシー（Differential Privacy）**",
				"- データにノイズを加え個人を特定不能にしながら統計は維持",
				"- Apple・Google が実装済み",
				"",
				"**連合学習（Federated Learning）**",
				"- データを端末外に出さずにモデルを訓練",
				"- スマホ上で学習し、パラメータのみ共有",
				"",
				"**秘密計算（Homomorphic Encryption）**",
				"- 暗号化したまま計算 → データを見ずに分析"
			]
		},
		{
			"title": "AI監査とガバナンス実践",
			"layout": "default",
			"content": [
				"**組織として取り組むべきこと**",
				"- **AI Impact Assessment**: リリース前に社会影響を評価",
				"- **Red Teaming**: 悪意ある使い方を事前に試みる",
				"- **Model Cards**: モデルの限界・バイアスを公開文書化",
				"- **Incident Response Plan**: AI障害時の対応手順を用意",
				"",
				"**個人として**",
				"- 倫理的懸念を声に出せる心理的安全性を求める",
				"- 「これは正しいか？」を設計段階で問う習慣"
			]
		},
		{
			"title": "コード一行が世界を変える",
			"layout": "default",
			"content": [
				"**実際に変えた事例**",
				"- 2021年: 研究者がFacebook広告アルゴリズムのバイアスを公表 → 改修",
				"- 2023年: エンジニアがChatGPTの個人情報流出リスクを社内告発 → 修正",
				"- 2024年: EU AI Actに技術者コミュニティの意見が反映",
				"",
				"**あなたにできること**",
				"- プルリクエストにバイアスチェックを含める",
				"- コードレビューで「誰が傷つくか？」を問う",
				"- 倫理的問題を上司・チームに声に出す"
			]
		},
		{
			"title": "第6章",
			"layout": "section",
			"content": [
				"ワークショップ ― あなたの選択",
				"",
				"*「知識は力ではない。*",
				"*使われた知識だけが力になる。」*"
			]
		},
		{
			"title": "ワークショップの進め方",
			"layout": "default",
			"content": [
				"**3つのディスカッション（計30分）**",
				"",
				"- グループ: 3〜5人",
				"- 各テーマ: 8〜10分",
				"- 最後に全体共有: 各グループから1アクション",
				"",
				"**ルール**",
				"- 批判より建設的な視点を優先",
				"- 「うちの会社では無理」より「どうすれば？」",
				"- 正解を求めない — 議論のプロセスが目的"
			]
		},
		{
			"title": "ディスカッション①: あなたのプロダクトの「世界線」",
			"layout": "default",
			"content": [
				"**問いかけ（8分）**",
				"",
				"あなたが現在関わるプロダクト・システムについて：",
				"",
				"1. **データ**: ユーザーデータをどう収集・活用しているか？",
				"2. **判断**: アルゴリズムの判断を誰が説明できるか？",
				"3. **影響**: どんな人が不利益を受ける可能性があるか？",
				"",
				"*ネクサス市に近い点を1つ特定しよう。*"
			]
		},
		{
			"title": "ディスカッション②: 最もリスクの高い設計判断",
			"layout": "default",
			"content": [
				"**問いかけ（8分）**",
				"",
				"あなたのチーム・プロダクトで：",
				"",
				"1. **見逃しているリスク**: 誰も気にしていないが危険な部分は？",
				"2. **速度vs倫理**: 「後でやる」と先送りにしている倫理的対応は？",
				"3. **声を上げる**: もし問題があると分かったら、誰にどう伝えるか？",
				"",
				"*正直に話すことが最初のステップ。*"
			]
		},
		{
			"title": "ディスカッション③: ユートピアへの具体的アクション",
			"layout": "default",
			"content": [
				"**問いかけ（8分）**",
				"",
				"アリア市に近づくために、明日からできることは：",
				"",
				"1. **個人**: 次のPRで追加できる倫理的チェックは？",
				"2. **チーム**: 1つのプロセス改善を提案するとしたら？",
				"3. **組織**: 半年後に実現したい変化は何か？",
				"",
				"*具体的で実行可能なアクションを1つ決めよう。*"
			]
		},
		{
			"title": "全体共有 ― 各グループから1アクション",
			"layout": "default",
			"content": [
				"**発表（各グループ2分）**",
				"",
				"- 「私たちが見つけたリスク」: 1文で",
				"- 「明日から取り組むアクション」: 具体的に1つ",
				"",
				"**ファシリテーターメモ**",
				"- 似たアクションをグルーピングしてホワイトボードに整理",
				"- 「最も多くのグループが挙げた課題」を強調",
				"- 全員拍手で締め — 声に出せたことを称える"
			]
		},
		{
			"title": "統合 ― 学んだことを現場に活かすには",
			"layout": "default",
			"content": [
				"**「倫理的設計」を習慣にする3つのステップ**",
				"",
				"1. **問う習慣**: 設計段階で「誰が傷つくか？」を問うルーティン",
				"2. **記録する**: 倫理的決断と根拠をドキュメント化",
				"3. **声を上げる**: 懸念を伝えやすい心理的安全性をチームで育てる",
				"",
				"> 一度の完璧な判断より、毎日の小さな問いが未来を変える"
			]
		},
		{
			"title": "個人のコミットメント",
			"layout": "center",
			"content": [
				"",
				"**「私は明日から、＿＿＿＿＿をする。」**",
				"",
				"*紙に書いて、隣の人に宣言しよう。*",
				"",
				"*小さな言葉が、大きな未来の始まり。*"
			]
		},
		{
			"title": "終章",
			"layout": "section",
			"content": [
				"未来は変えられる",
				"",
				"*「最も暗い夜でも、*",
				"*夜明けは必ずやってくる。」*"
			]
		},
		{
			"title": "過去のエンジニアたちの選択",
			"layout": "default",
			"content": [
				"**世界を変えた技術的「ノー」と「イエス」**",
				"- **Tim Berners-Lee**: WWWの特許を取得せず無償公開 → オープンインターネット",
				"- **Linus Torvalds**: Linuxをオープンソースに → AIの基盤となるソフトウェアの礎",
				"- **Frances Haugen**: Facebookの内部告発 → SNS規制議論のトリガー",
				"",
				"> 歴史は常に、個人の選択によって動いてきた",
				"",
				"> 多くの科学者が核兵器開発後に反核運動へ — 遅すぎることはない"
			]
		},
		{
			"title": "あなたへのメッセージ",
			"layout": "center",
			"content": [
				"",
				"**2045年のアリア市は、**",
				"**あなたのような人が作る。**",
				"",
				"*コードを書くとき、設計するとき、*",
				"*「これはどちらの世界を作っているか？」*",
				"*と問い続けてほしい。*",
				"",
				"**未来はまだ書かれていない。**"
			]
		},
		{
			"title": "今日からできること — まとめ",
			"layout": "default",
			"content": [
				"**今すぐ（今日）**",
				"- 関わるシステムの倫理的リスクを1つ特定する",
				"",
				"**今週中**",
				"- チームのコードレビューに「倫理チェック項目」を1つ追加",
				"",
				"**今月中**",
				"- AI倫理・アライメントの入門書または論文を1本読む",
				"",
				"**今年中**",
				"- プロダクトのAI Impact Assessmentを実施提案する"
			]
		},
		{
			"title": "参考文献・リソース（1/2）",
			"layout": "default",
			"content": [
				"**AI倫理・アライメント**",
				"- Stuart Russell「Human Compatible」（2019）",
				"- Nick Bostrom「Superintelligence」（2014）",
				"- Joy Buolamwini「Unmasking AI」（2023）",
				"",
				"**公式リソース**",
				"- EU AI Act (2024) — artificialintelligenceact.eu",
				"- Anthropic's Core Views on AI Safety — anthropic.com/research",
				"- Google People + AI Research (PAIR) — pair.withgoogle.com"
			]
		},
		{
			"title": "参考文献・リソース（2/2）",
			"layout": "default",
			"content": [
				"**プライバシー・公平性ツール**",
				"- OpenDP（差分プライバシー）— opendp.org",
				"- TensorFlow Federated — tensorflow.org/federated",
				"- Fairlearn（Microsoft）— fairlearn.org",
				"",
				"**コミュニティ**",
				"- Partnership on AI — partnershiponai.org",
				"- Montreal AI Ethics Institute — montrealethics.ai",
				"",
				"**SF・思想**",
				"- 劉慈欣「三体」シリーズ — 技術と社会の関係を考える傑作"
			]
		}
	]
}
