---
marp: true
theme: gaia
class: invert
size: 16:9
paginate: true
header: "最適化と倫理"
footer: "© 2026"
style: |
  section pre code { font-size: 0.58em; line-height: 1.4; }
  
---

<!-- _class: lead -->
# 最適化の倫理問題
— アルゴリズムが「最善」を決めるとき

- 何を最適化するかで、誰が得をして誰が損をするか変わる
- クリック率・利益・公平性は両立しない
- トロッコ問題が現実のAI設計に現れている


---

# アジェンダ

- 1. 最適化とは何を最適化しているのか
- 2. 目的関数の選び方が倫理を決める
- 3. 事例：採用AI・推薦アルゴリズム
- 4. 公平性の数学的定義と不可能定理
- 5. 誰が決めるべきか


---

<!-- _class: lead -->
# 最適化とは何を最適化しているのか


---

# 目的関数が世界観を決める

- **最適化の構造：**
- 目的関数（何を最大化・最小化するか）を設定 → 数学が解を出す
- ---
- **問題：同じ状況でも目的関数が違えば解が全く変わる**
- 配車アプリの「最適なルート」= 最短時間？最安値？最低CO₂？
- ---
- **SNSの推薦アルゴリズム：**
- - クリック率最大化 → 怒り・恐怖コンテンツが増幅される
- - 滞在時間最大化 → 依存性が高いコンテンツを優先
- - 社会的幸福最大化 → どう定義する？
- ---
- 「目的関数を書いた人が世界の形を決めている」— Stuart Russell


---

<!-- _class: lead -->
# 採用AIと推薦システムの実例


---

# Amazonの採用AIと公平性の失敗

- **Amazon採用AI（2018年廃止）：**
- 過去10年の採用データで学習 → 女性候補者を低く評価
- 理由：過去の採用が男性に偏っていたため
- = 歴史的差別を「最適化」して再生産
- ---
- **YouTubeの過激化パイプライン（2019年研究）：**
- 視聴継続時間最適化 → より過激な動画を次々と推薦
- 穏健な政治チャンネル → 陰謀論へと誘導
- ---
- **Facebookの感情伝染実験（2014年）：**
- 68万人のフィードを無断で操作（ネガティブ・ポジティブ）
- 感情が伝染することを確認 → 倫理委員会を通さずに実施


---

# 公平性の不可能定理

- **Chouldechova & Roth（2018年）：**
- 複数の「公平性」の定義は数学的に同時に満たせない
- ---
- **3種類の公平性（同時達成不可能）：**
- 1. **個人的公平性**：似た人には似た結果を
- 2. **グループ公平性**：グループ間で同じ正解率を
- 3. **反事実的公平性**：属性を変えても結果が変わらない
- ---
- **COMPAS（再犯予測AI）の事例：**
- 黒人被告を白人より2倍高く「再犯リスク高」と評価
- → グループ正解率は同じでも、誤りの種類が人種で偏っていた
- ---
- 公平性の定義を選ぶこと自体が価値判断


---

# まとめ：アルゴリズムは中立でない

- ✅ **最適化は常に「何を」最適化するかの価値判断を含む**
- ✅ **クリック最大化はフィルターバブルと過激化を生む**
- ✅ **公平性の数学的定義は同時に満たせない（不可能定理）**
- ✅ **歴史的バイアスのあるデータで学習すると差別を再生産**
- 
- 「誰のための最適化か」を問うことが、AI倫理の核心

