<!DOCTYPE html><html lang="en-US"><head><title>AI Transformer 完全ガイド 2026</title><meta property="og:title" content="AI Transformer 完全ガイド 2026"><meta charset="UTF-8"><meta name="viewport" content="width=device-width,height=device-height,initial-scale=1.0"><meta name="apple-mobile-web-app-capable" content="yes"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta property="og:type" content="website"><meta name="twitter:card" content="summary"><style>@media screen{body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button{appearance:none;background-color:initial;border:0;color:inherit;cursor:pointer;font-size:inherit;opacity:.8;outline:none;padding:0;transition:opacity .2s linear;-webkit-tap-highlight-color:transparent}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button:disabled,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button:disabled,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button:disabled,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button:disabled{cursor:not-allowed;opacity:.15!important}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button:hover,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button:hover,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button:hover,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button:hover{opacity:1}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button:hover:active,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button:hover:active,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button:hover:active,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button:hover:active{opacity:.6}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button:hover:not(:disabled),body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button:hover:not(:disabled),body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button:hover:not(:disabled),body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button:hover:not(:disabled){transition:none}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=prev],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=prev],body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button.bespoke-marp-presenter-info-page-prev{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48cGF0aCBmaWxsPSJub25lIiBzdHJva2U9IiNmZmYiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSI1IiBkPSJNNjggOTAgMjggNTBsNDAtNDAiLz48L3N2Zz4=") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=next],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=next],body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button.bespoke-marp-presenter-info-page-next{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48cGF0aCBmaWxsPSJub25lIiBzdHJva2U9IiNmZmYiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSI1IiBkPSJtMzIgOTAgNDAtNDAtNDAtNDAiLz48L3N2Zz4=") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=fullscreen],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=fullscreen]{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48ZGVmcz48c3R5bGU+LmF7ZmlsbDpub25lO3N0cm9rZTojZmZmO3N0cm9rZS1saW5lY2FwOnJvdW5kO3N0cm9rZS1saW5lam9pbjpyb3VuZDtzdHJva2Utd2lkdGg6NXB4fTwvc3R5bGU+PC9kZWZzPjxyZWN0IHdpZHRoPSI4MCIgaGVpZ2h0PSI2MCIgeD0iMTAiIHk9IjIwIiBjbGFzcz0iYSIgcng9IjUuNjciLz48cGF0aCBkPSJNNDAgNzBIMjBWNTBtMjAgMEwyMCA3MG00MC00MGgyMHYyMG0tMjAgMCAyMC0yMCIgY2xhc3M9ImEiLz48L3N2Zz4=") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button.exit[data-bespoke-marp-osc=fullscreen],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button.exit[data-bespoke-marp-osc=fullscreen]{background-image:url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48ZGVmcz48c3R5bGU+LmF7ZmlsbDpub25lO3N0cm9rZTojZmZmO3N0cm9rZS1saW5lY2FwOnJvdW5kO3N0cm9rZS1saW5lam9pbjpyb3VuZDtzdHJva2Utd2lkdGg6NXB4fTwvc3R5bGU+PC9kZWZzPjxyZWN0IHdpZHRoPSI4MCIgaGVpZ2h0PSI2MCIgeD0iMTAiIHk9IjIwIiBjbGFzcz0iYSIgcng9IjUuNjciLz48cGF0aCBkPSJNMjAgNTBoMjB2MjBtLTIwIDAgMjAtMjBtNDAgMEg2MFYzMG0yMCAwTDYwIDUwIiBjbGFzcz0iYSIvPjwvc3ZnPg==")}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=presenter],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=presenter]{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48cGF0aCBmaWxsPSJub25lIiBzdHJva2U9IiNmZmYiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSI1IiBkPSJNODcuOCA0Ny41Qzg5IDUwIDg3LjcgNTIgODUgNTJIMzVhOC43IDguNyAwIDAgMS03LjItNC41bC0xNS42LTMxQzExIDE0IDEyLjIgMTIgMTUgMTJoNTBhOC44IDguOCAwIDAgMSA3LjIgNC41ek02MCA1MnYzNm0tMTAgMGgyME00NSA0MmgyMCIvPjwvc3ZnPg==") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button.bespoke-marp-presenter-note-bigger{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48cGF0aCBzdHJva2U9IiNmZmYiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSI1IiBkPSJNMTIgNTBoODBNNTIgOTBWMTAiLz48L3N2Zz4=") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button.bespoke-marp-presenter-note-smaller{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48cGF0aCBmaWxsPSJub25lIiBzdHJva2U9IiNmZmYiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSI1IiBkPSJNMTIgNTBoODAiLz48L3N2Zz4=") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}}@keyframes __bespoke_marp_transition_reduced_outgoing__{0%{opacity:1}to{opacity:0}}@keyframes __bespoke_marp_transition_reduced_incoming__{0%{mix-blend-mode:plus-lighter;opacity:0}to{mix-blend-mode:plus-lighter;opacity:1}}.bespoke-marp-note,.bespoke-marp-osc,.bespoke-progress-parent{display:none;transition:none}@media screen{::view-transition-group(*){animation-duration:var(--marp-bespoke-transition-animation-duration,.5s);animation-timing-function:ease}::view-transition-new(*),::view-transition-old(*){animation-delay:0s;animation-direction:var(--marp-bespoke-transition-animation-direction,normal);animation-duration:var(--marp-bespoke-transition-animation-duration,.5s);animation-fill-mode:both;animation-name:var(--marp-bespoke-transition-animation-name,var(--marp-bespoke-transition-animation-name-fallback,__bespoke_marp_transition_no_animation__));mix-blend-mode:normal}::view-transition-old(*){--marp-bespoke-transition-animation-name-fallback:__bespoke_marp_transition_reduced_outgoing__;animation-timing-function:ease}::view-transition-new(*){--marp-bespoke-transition-animation-name-fallback:__bespoke_marp_transition_reduced_incoming__;animation-timing-function:ease}::view-transition-new(root),::view-transition-old(root){animation-timing-function:linear}::view-transition-new(__bespoke_marp_transition_osc__),::view-transition-old(__bespoke_marp_transition_osc__){animation-duration:0s!important;animation-name:__bespoke_marp_transition_osc__!important}::view-transition-new(__bespoke_marp_transition_osc__){opacity:0!important}.bespoke-marp-transition-warming-up::view-transition-group(*),.bespoke-marp-transition-warming-up::view-transition-new(*),.bespoke-marp-transition-warming-up::view-transition-old(*){animation-play-state:paused!important}body,html{height:100%;margin:0}body{background:#000;overflow:hidden}svg.bespoke-marp-slide{content-visibility:hidden;opacity:0;pointer-events:none;z-index:-1}svg.bespoke-marp-slide:not(.bespoke-marp-active) *{view-transition-name:none!important}svg.bespoke-marp-slide.bespoke-marp-active{content-visibility:visible;opacity:1;pointer-events:auto;z-index:0}svg.bespoke-marp-slide.bespoke-marp-active.bespoke-marp-active-ready *{animation-name:__bespoke_marp__!important}@supports not (content-visibility:hidden){svg.bespoke-marp-slide[data-bespoke-marp-load=hideable]{display:none}svg.bespoke-marp-slide[data-bespoke-marp-load=hideable].bespoke-marp-active{display:block}}}@media screen and (prefers-reduced-motion:reduce){svg.bespoke-marp-slide *{view-transition-name:none!important}}@media screen{[data-bespoke-marp-fragment=inactive]{visibility:hidden}body[data-bespoke-view=""] .bespoke-marp-parent,body[data-bespoke-view=next] .bespoke-marp-parent{inset:0;position:absolute}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc{background:#000000a6;border-radius:7px;bottom:50px;color:#fff;contain:paint;display:block;font-family:Helvetica,Arial,sans-serif;font-size:16px;left:50%;line-height:0;opacity:1;padding:12px;position:absolute;touch-action:manipulation;transform:translateX(-50%);transition:opacity .2s linear;-webkit-user-select:none;user-select:none;white-space:nowrap;will-change:transform;z-index:1;view-transition-name:__bespoke_marp_transition_osc__}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>*,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>*{margin-left:6px}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>:first-child,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>:first-child{margin-left:0}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>span,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>span{opacity:.8}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>span[data-bespoke-marp-osc=page],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>span[data-bespoke-marp-osc=page]{display:inline-block;min-width:140px;text-align:center}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=fullscreen],body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=next],body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=presenter],body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=prev],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=fullscreen],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=next],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=presenter],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=prev]{height:32px;line-height:32px;width:32px}body[data-bespoke-view=""] .bespoke-marp-parent.bespoke-marp-inactive,body[data-bespoke-view=next] .bespoke-marp-parent.bespoke-marp-inactive{cursor:none}body[data-bespoke-view=""] .bespoke-marp-parent.bespoke-marp-inactive>.bespoke-marp-osc,body[data-bespoke-view=next] .bespoke-marp-parent.bespoke-marp-inactive>.bespoke-marp-osc{opacity:0;pointer-events:none}body[data-bespoke-view=""] svg.bespoke-marp-slide,body[data-bespoke-view=next] svg.bespoke-marp-slide{height:100%;left:0;position:absolute;top:0;width:100%}body[data-bespoke-view=""] .bespoke-progress-parent{background:#222;display:flex;height:5px;width:100%}body[data-bespoke-view=""] .bespoke-progress-parent+.bespoke-marp-parent{top:5px}body[data-bespoke-view=""] .bespoke-progress-parent .bespoke-progress-bar{background:#0288d1;flex:0 0 0;transition:flex-basis .2s cubic-bezier(0,1,1,1)}body[data-bespoke-view=next]{background:#0000}body[data-bespoke-view=presenter]{background:#161616}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container{display:grid;font-family:Helvetica,Arial,sans-serif;grid-template:"current dragbar next" minmax(140px,1fr) "current dragbar note" 2fr "info    dragbar note" 3em;grid-template-columns:minmax(3px,var(--bespoke-marp-presenter-split-ratio,66%)) 0 minmax(3px,1fr);height:100%;left:0;position:absolute;top:0;width:100%}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-parent{grid-area:current;overflow:hidden;position:relative}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-parent svg.bespoke-marp-slide{height:calc(100% - 40px);left:20px;pointer-events:none;position:absolute;top:20px;-webkit-user-select:none;user-select:none;width:calc(100% - 40px)}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-parent svg.bespoke-marp-slide.bespoke-marp-active{filter:drop-shadow(0 3px 10px rgba(0,0,0,.5))}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-dragbar-container{background:#0288d1;cursor:col-resize;grid-area:dragbar;margin-left:-3px;opacity:0;position:relative;transition:opacity .4s linear .1s;width:6px;z-index:10}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-dragbar-container:hover{opacity:1}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-dragbar-container.active{opacity:1;transition-delay:0s}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-next-container{background:#222;cursor:pointer;display:none;grid-area:next;overflow:hidden;position:relative}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-next-container.active{display:block}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-next-container iframe.bespoke-marp-presenter-next{background:#0000;border:0;display:block;filter:drop-shadow(0 3px 10px rgba(0,0,0,.5));height:calc(100% - 40px);left:20px;pointer-events:none;position:absolute;top:20px;-webkit-user-select:none;user-select:none;width:calc(100% - 40px)}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container{background:#222;color:#eee;grid-area:note;position:relative;z-index:1}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button{height:1.5em;line-height:1.5em;width:1.5em}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-presenter-note-wrapper{display:block;inset:0;position:absolute}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-presenter-note-buttons{background:#000000a6;border-radius:4px;bottom:0;display:flex;gap:4px;margin:12px;opacity:0;padding:6px;pointer-events:none;position:absolute;right:0;transition:opacity .2s linear}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-presenter-note-buttons:focus-within,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-presenter-note-wrapper:focus-within+.bespoke-marp-presenter-note-buttons,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container:hover .bespoke-marp-presenter-note-buttons{opacity:1;pointer-events:auto}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note{box-sizing:border-box;font-size:calc(1.1em*var(--bespoke-marp-note-font-scale, 1));height:calc(100% - 40px);margin:20px;overflow:auto;padding-right:3px;white-space:pre-wrap;width:calc(100% - 40px);word-wrap:break-word;scrollbar-color:#eeeeee80 #0000;scrollbar-width:thin}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note::-webkit-scrollbar{width:6px}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note::-webkit-scrollbar-track{background:#0000}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note::-webkit-scrollbar-thumb{background:#eeeeee80;border-radius:6px}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note:empty{pointer-events:none}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note.active{display:block}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note p:first-child{margin-top:0}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note p:last-child{margin-bottom:0}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container{align-items:center;box-sizing:border-box;color:#eee;display:flex;flex-wrap:nowrap;grid-area:info;justify-content:center;overflow:hidden;padding:0 10px}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-page,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-time,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-timer{box-sizing:border-box;display:block;padding:0 10px;white-space:nowrap;width:100%}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button{height:1.5em;line-height:1.5em;width:1.5em}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-page{order:2;text-align:center}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-page .bespoke-marp-presenter-info-page-text{display:inline-block;min-width:120px;text-align:center}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-time{color:#999;order:1;text-align:left}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-timer{color:#999;order:3;text-align:right}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-timer:hover{cursor:pointer}}@media print{.bespoke-marp-presenter-info-container,.bespoke-marp-presenter-next-container,.bespoke-marp-presenter-note-container{display:none}}</style><style>@charset "UTF-8";@import "https://fonts.bunny.net/css?family=Lato:400,900|Roboto+Mono:400,700&display=swap";div#\:\$p > svg > foreignObject > section{width:1280px;height:720px;box-sizing:border-box;overflow:hidden;position:relative;scroll-snap-align:center center;-webkit-text-size-adjust:100%;text-size-adjust:100%}div#\:\$p > svg > foreignObject > section::after{bottom:0;content:attr(data-marpit-pagination);padding:inherit;pointer-events:none;position:absolute;right:0}div#\:\$p > svg > foreignObject > section:not([data-marpit-pagination])::after{display:none}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1){font-size:2em;margin-block:0.67em}div#\:\$p > svg > foreignObject > section video::-webkit-media-controls{will-change:transform}@page {size:1280px 720px;margin:0}@media print{html, body{background-color:#fff;margin:0;page-break-inside:avoid;break-inside:avoid-page}div#\:\$p > svg > foreignObject > section{page-break-before:always;break-before:page}div#\:\$p > svg > foreignObject > section, div#\:\$p > svg > foreignObject > section *{-webkit-print-color-adjust:exact!important;animation-delay:0s!important;animation-duration:0s!important;color-adjust:exact!important;print-color-adjust:exact!important;transition:none!important}div#\:\$p > svg[data-marpit-svg]{display:block;height:100vh;width:100vw}}div#\:\$p > svg > foreignObject > :where(section){container-type:size}div#\:\$p > svg > foreignObject > section mjx-container[jax="SVG"]{direction:ltr}div#\:\$p > svg > foreignObject > section mjx-container[jax="SVG"] > svg{overflow:visible;min-height:1px;min-width:1px}div#\:\$p > svg > foreignObject > section mjx-container[jax="SVG"] > svg a{fill:blue;stroke:blue}div#\:\$p > svg > foreignObject > section mjx-container[jax="SVG"][display="true"]{display:block;text-align:center;margin:1em 0}div#\:\$p > svg > foreignObject > section mjx-container[jax="SVG"][display="true"][width="full"]{display:flex}div#\:\$p > svg > foreignObject > section mjx-container[jax="SVG"][justify="left"]{text-align:left}div#\:\$p > svg > foreignObject > section mjx-container[jax="SVG"][justify="right"]{text-align:right}div#\:\$p > svg > foreignObject > section g[data-mml-node="merror"] > g{fill:red;stroke:red}div#\:\$p > svg > foreignObject > section g[data-mml-node="merror"] > rect[data-background]{fill:yellow;stroke:none}div#\:\$p > svg > foreignObject > section g[data-mml-node="mtable"] > line[data-line], div#\:\$p > svg > foreignObject > section svg[data-table] > g > line[data-line]{stroke-width:70px;fill:none}div#\:\$p > svg > foreignObject > section g[data-mml-node="mtable"] > rect[data-frame], div#\:\$p > svg > foreignObject > section svg[data-table] > g > rect[data-frame]{stroke-width:70px;fill:none}div#\:\$p > svg > foreignObject > section g[data-mml-node="mtable"] > .mjx-dashed, div#\:\$p > svg > foreignObject > section svg[data-table] > g > .mjx-dashed{stroke-dasharray:140}div#\:\$p > svg > foreignObject > section g[data-mml-node="mtable"] > .mjx-dotted, div#\:\$p > svg > foreignObject > section svg[data-table] > g > .mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}div#\:\$p > svg > foreignObject > section g[data-mml-node="mtable"] > g > svg{overflow:visible}div#\:\$p > svg > foreignObject > section [jax="SVG"] mjx-tool{display:inline-block;position:relative;width:0;height:0}div#\:\$p > svg > foreignObject > section [jax="SVG"] mjx-tool > mjx-tip{position:absolute;top:0;left:0}div#\:\$p > svg > foreignObject > section mjx-tool > mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#F8F8F8;color:black;box-shadow:2px 2px 5px #AAAAAA}div#\:\$p > svg > foreignObject > section g[data-mml-node="maction"][data-toggle]{cursor:pointer}div#\:\$p > svg > foreignObject > section mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#F8F8F8;color:black}div#\:\$p > svg > foreignObject > section foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}div#\:\$p > svg > foreignObject > section mjx-container[jax="SVG"] path[data-c], div#\:\$p > svg > foreignObject > section mjx-container[jax="SVG"] use[data-c]{stroke-width:3}@media print{div#\:\$p > svg > foreignObject > section mjx-container[jax=SVG] path[data-c],div#\:\$p > svg > foreignObject > section mjx-container[jax=SVG] use[data-c]{stroke-width:0}}div#\:\$p > svg > foreignObject > section img[data-marp-twemoji]{background:transparent;height:1em;margin:0 .05em 0 .1em;vertical-align:-.1em;width:1em}/*!
 * Marp / Marpit Gaia theme.
 *
 * @theme gaia
 * @author Yuki Hattori
 *
 * @auto-scaling true
 * @size 16:9 1280px 720px
 * @size 4:3 960px 720px
 */div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) code.hljs{display:block;overflow-x:auto;padding:1em}div#\:\$p > svg > foreignObject > section code.hljs{padding:3px 5px}div#\:\$p > svg > foreignObject > section .hljs{background:#000;color:#f8f8f8}div#\:\$p > svg > foreignObject > section .hljs-comment,div#\:\$p > svg > foreignObject > section .hljs-quote{color:#aeaeae;font-style:italic}div#\:\$p > svg > foreignObject > section .hljs-keyword,div#\:\$p > svg > foreignObject > section .hljs-selector-tag,div#\:\$p > svg > foreignObject > section .hljs-type{color:#e28964}div#\:\$p > svg > foreignObject > section .hljs-string{color:#65b042}div#\:\$p > svg > foreignObject > section .hljs-subst{color:#daefa3}div#\:\$p > svg > foreignObject > section .hljs-link,div#\:\$p > svg > foreignObject > section .hljs-regexp{color:#e9c062}div#\:\$p > svg > foreignObject > section .hljs-name,div#\:\$p > svg > foreignObject > section .hljs-section,div#\:\$p > svg > foreignObject > section .hljs-tag,div#\:\$p > svg > foreignObject > section .hljs-title{color:#89bdff}div#\:\$p > svg > foreignObject > section .hljs-class .hljs-title,div#\:\$p > svg > foreignObject > section .hljs-doctag,div#\:\$p > svg > foreignObject > section .hljs-title.class_{text-decoration:underline}div#\:\$p > svg > foreignObject > section .hljs-bullet,div#\:\$p > svg > foreignObject > section .hljs-number,div#\:\$p > svg > foreignObject > section .hljs-symbol{color:#3387cc}div#\:\$p > svg > foreignObject > section .hljs-params,div#\:\$p > svg > foreignObject > section .hljs-template-variable,div#\:\$p > svg > foreignObject > section .hljs-variable{color:#3e87e3}div#\:\$p > svg > foreignObject > section .hljs-attribute{color:#cda869}div#\:\$p > svg > foreignObject > section .hljs-meta{color:#8996a8}div#\:\$p > svg > foreignObject > section .hljs-formula{background-color:#0e2231;color:#f8f8f8;font-style:italic}div#\:\$p > svg > foreignObject > section .hljs-addition{background-color:#253b22;color:#f8f8f8}div#\:\$p > svg > foreignObject > section .hljs-deletion{background-color:#420e09;color:#f8f8f8}div#\:\$p > svg > foreignObject > section .hljs-selector-class{color:#9b703f}div#\:\$p > svg > foreignObject > section .hljs-selector-id{color:#8b98ab}div#\:\$p > svg > foreignObject > section .hljs-emphasis{font-style:italic}div#\:\$p > svg > foreignObject > section .hljs-strong{font-weight:700}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1),div#\:\$p > svg > foreignObject > section :is(h2, marp-h2),div#\:\$p > svg > foreignObject > section :is(h3, marp-h3),div#\:\$p > svg > foreignObject > section :is(h4, marp-h4),div#\:\$p > svg > foreignObject > section :is(h5, marp-h5),div#\:\$p > svg > foreignObject > section :is(h6, marp-h6){margin:.5em 0 0}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1) strong,div#\:\$p > svg > foreignObject > section :is(h2, marp-h2) strong,div#\:\$p > svg > foreignObject > section :is(h3, marp-h3) strong,div#\:\$p > svg > foreignObject > section :is(h4, marp-h4) strong,div#\:\$p > svg > foreignObject > section :is(h5, marp-h5) strong,div#\:\$p > svg > foreignObject > section :is(h6, marp-h6) strong{font-weight:inherit}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1)::part(auto-scaling),div#\:\$p > svg > foreignObject > section :is(h2, marp-h2)::part(auto-scaling),div#\:\$p > svg > foreignObject > section :is(h3, marp-h3)::part(auto-scaling),div#\:\$p > svg > foreignObject > section :is(h4, marp-h4)::part(auto-scaling),div#\:\$p > svg > foreignObject > section :is(h5, marp-h5)::part(auto-scaling),div#\:\$p > svg > foreignObject > section :is(h6, marp-h6)::part(auto-scaling){max-height:580px}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1){font-size:1.8em}div#\:\$p > svg > foreignObject > section :is(h2, marp-h2){font-size:1.5em}div#\:\$p > svg > foreignObject > section :is(h3, marp-h3){font-size:1.3em}div#\:\$p > svg > foreignObject > section :is(h4, marp-h4){font-size:1.1em}div#\:\$p > svg > foreignObject > section :is(h5, marp-h5){font-size:1em}div#\:\$p > svg > foreignObject > section :is(h6, marp-h6){font-size:.9em}div#\:\$p > svg > foreignObject > section blockquote,div#\:\$p > svg > foreignObject > section p{margin:1em 0 0}div#\:\$p > svg > foreignObject > section ol>li,div#\:\$p > svg > foreignObject > section ul>li{margin:.3em 0 0}div#\:\$p > svg > foreignObject > section ol>li>p,div#\:\$p > svg > foreignObject > section ul>li>p{margin:.6em 0 0}div#\:\$p > svg > foreignObject > section code{display:inline-block;font-family:Roboto Mono,monospace;font-size:.8em;letter-spacing:0;margin:-.1em .15em;padding:.1em .2em;vertical-align:baseline}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre){display:block;margin:1em 0 0;overflow:visible}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) code{box-sizing:border-box;font-size:.7em;margin:0;min-width:100%;padding:.5em}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre)::part(auto-scaling){max-height:calc(580px - 1em)}div#\:\$p > svg > foreignObject > section blockquote{margin:1em 0 0;padding:0 1em;position:relative}div#\:\$p > svg > foreignObject > section blockquote:after,div#\:\$p > svg > foreignObject > section blockquote:before{content:"“";display:block;font-family:Times New Roman,serif;font-weight:700;position:absolute}div#\:\$p > svg > foreignObject > section blockquote:before{left:0;top:0}div#\:\$p > svg > foreignObject > section blockquote:after{bottom:0;right:0;transform:rotate(180deg)}div#\:\$p > svg > foreignObject > section blockquote>:first-child{margin-top:0}div#\:\$p > svg > foreignObject > section mark{background:transparent}div#\:\$p > svg > foreignObject > section table{border-collapse:collapse;border-spacing:0;margin:1em 0 0}div#\:\$p > svg > foreignObject > section table td,div#\:\$p > svg > foreignObject > section table th{border-style:solid;border-width:1px;padding:.2em .4em}div#\:\$p > svg > foreignObject > section footer,div#\:\$p > svg > foreignObject > section header,div#\:\$p > svg > foreignObject > section:after{box-sizing:border-box;font-size:66%;height:70px;line-height:50px;overflow:hidden;padding:10px 25px;position:absolute}div#\:\$p > svg > foreignObject > section:after{--marpit-root-font-size:66%}div#\:\$p > svg > foreignObject > section header{top:0}div#\:\$p > svg > foreignObject > section footer,div#\:\$p > svg > foreignObject > section header{left:0;right:0}div#\:\$p > svg > foreignObject > section footer{bottom:0}div#\:\$p > svg > foreignObject > section{--color-background:light-dark(#fff8e1, #455a64);--color-background-stripe:light-dark(
    rgba(69,90,100,.1),
    rgba(255,248,225,.1)
  );--color-foreground:light-dark(#455a64, #fff8e1);--color-dimmed:light-dark(
    #6a7a7d,
    #dad8c8
  );--color-highlight:light-dark(#0288d1, #81d4fa);background-color:var(--color-background);background-image:linear-gradient(135deg, hsla(0,0%,53%,0), hsla(0,0%,53%,.02) 50%, hsla(0,0%,100%,0) 0, hsla(0,0%,100%,.05));color:var(--color-foreground);color-scheme:light;font-family:Lato,Avenir Next,Avenir,Trebuchet MS,Segoe UI,sans-serif;font-size:35px;height:720px;letter-spacing:1.25px;line-height:1.35;overflow-wrap:break-word;padding:70px;width:1280px}div#\:\$p > svg > foreignObject > section{--marpit-root-font-size:35px}div#\:\$p > svg > foreignObject > section:after{bottom:0;font-size:80%;right:0}div#\:\$p > svg > foreignObject > section:after{--marpit-root-font-size:80%}div#\:\$p > svg > foreignObject > section a,div#\:\$p > svg > foreignObject > section mark{color:var(--color-highlight)}div#\:\$p > svg > foreignObject > section code{background:var(--color-dimmed);color:var(--color-background)}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1) strong,div#\:\$p > svg > foreignObject > section :is(h2, marp-h2) strong,div#\:\$p > svg > foreignObject > section :is(h3, marp-h3) strong,div#\:\$p > svg > foreignObject > section :is(h4, marp-h4) strong,div#\:\$p > svg > foreignObject > section :is(h5, marp-h5) strong,div#\:\$p > svg > foreignObject > section :is(h6, marp-h6) strong{color:var(--color-highlight)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre){background:var(--color-foreground)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre)>code{background:transparent}div#\:\$p > svg > foreignObject > section blockquote:after,div#\:\$p > svg > foreignObject > section blockquote:before,div#\:\$p > svg > foreignObject > section footer,div#\:\$p > svg > foreignObject > section header,div#\:\$p > svg > foreignObject > section section:after{color:var(--color-dimmed)}div#\:\$p > svg > foreignObject > section table td,div#\:\$p > svg > foreignObject > section table th{border-color:var(--color-foreground)}div#\:\$p > svg > foreignObject > section table thead th{background:var(--color-foreground);color:var(--color-background)}div#\:\$p > svg > foreignObject > section table tbody>tr:nth-child(odd) td,div#\:\$p > svg > foreignObject > section table tbody>tr:nth-child(odd) th{background:var(--color-background-stripe, transparent)}div#\:\$p > svg > foreignObject > section>:first-child,div#\:\$p > svg > foreignObject > section>header:first-child+*{margin-top:0}div#\:\$p > svg > foreignObject > section:where(.invert){color-scheme:dark}div#\:\$p > svg > foreignObject > section:where(.gaia){--color-background:#0288d1;--color-background-stripe:rgba(255,248,225,.1);--color-foreground:#fff8e1;--color-dimmed:#cce2de;--color-highlight:#81d4fa;}div#\:\$p > svg > foreignObject > section:where(.lead){align-items:stretch;flex-flow:column nowrap;place-content:safe center center}div#\:\$p > svg > foreignObject > section:where(.lead) :is(h1, marp-h1),div#\:\$p > svg > foreignObject > section:where(.lead) :is(h2, marp-h2),div#\:\$p > svg > foreignObject > section:where(.lead) :is(h3, marp-h3),div#\:\$p > svg > foreignObject > section:where(.lead) :is(h4, marp-h4),div#\:\$p > svg > foreignObject > section:where(.lead) :is(h5, marp-h5),div#\:\$p > svg > foreignObject > section:where(.lead) :is(h6, marp-h6){text-align:center}div#\:\$p > svg > foreignObject > section:where(.lead) p{text-align:center}div#\:\$p > svg > foreignObject > section:where(.lead) blockquote>:is(h1, marp-h1),div#\:\$p > svg > foreignObject > section:where(.lead) blockquote>:is(h2, marp-h2),div#\:\$p > svg > foreignObject > section:where(.lead) blockquote>:is(h3, marp-h3),div#\:\$p > svg > foreignObject > section:where(.lead) blockquote>:is(h4, marp-h4),div#\:\$p > svg > foreignObject > section:where(.lead) blockquote>:is(h5, marp-h5),div#\:\$p > svg > foreignObject > section:where(.lead) blockquote>:is(h6, marp-h6),div#\:\$p > svg > foreignObject > section:where(.lead) blockquote>p{text-align:left}div#\:\$p > svg > foreignObject > section:where(.lead) ol>li>p,div#\:\$p > svg > foreignObject > section:where(.lead) ul>li>p{text-align:left}div#\:\$p > svg > foreignObject > section:where(.lead) table{margin-left:auto;margin-right:auto}div#\:\$p > svg > foreignObject > section{width:1280px;height:720px}div#\:\$p > svg > foreignObject > section{font-size:1.05em}div#\:\$p > svg > foreignObject > section{--marpit-root-font-size: 1.05em}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) code{font-size:0.56em;line-height:1.35}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1){font-size:1.6em}div#\:\$p > svg > foreignObject > section :is(h2, marp-h2){font-size:1.3em}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"]{columns:initial!important;display:block!important;padding:0!important}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"]::before, div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"]::after, div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="content"]::before, div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="content"]::after{display:none!important}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"] > div[data-marpit-advanced-background-container]{all:initial;display:flex;flex-direction:row;height:100%;overflow:hidden;width:100%}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"] > div[data-marpit-advanced-background-container][data-marpit-advanced-background-direction="vertical"]{flex-direction:column}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"][data-marpit-advanced-background-split] > div[data-marpit-advanced-background-container]{width:var(--marpit-advanced-background-split, 50%)}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"][data-marpit-advanced-background-split="right"] > div[data-marpit-advanced-background-container]{margin-left:calc(100% - var(--marpit-advanced-background-split, 50%))}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"] > div[data-marpit-advanced-background-container] > figure{all:initial;background-position:center;background-repeat:no-repeat;background-size:cover;flex:auto;margin:0}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"] > div[data-marpit-advanced-background-container] > figure > figcaption{position:absolute;border:0;clip:rect(0, 0, 0, 0);height:1px;margin:-1px;overflow:hidden;padding:0;white-space:nowrap;width:1px}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="content"], div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="pseudo"]{background:transparent!important}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="pseudo"], div#\:\$p > svg[data-marpit-svg] > foreignObject[data-marpit-advanced-background="pseudo"]{pointer-events:none!important}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background-split]{width:100%;height:100%}
</style></head><body><div class="bespoke-marp-osc"><button data-bespoke-marp-osc="prev" tabindex="-1" title="Previous slide">Previous slide</button><span data-bespoke-marp-osc="page"></span><button data-bespoke-marp-osc="next" tabindex="-1" title="Next slide">Next slide</button><button data-bespoke-marp-osc="fullscreen" tabindex="-1" title="Toggle fullscreen (f)">Toggle fullscreen</button><button data-bespoke-marp-osc="presenter" tabindex="-1" title="Open presenter view (p)">Open presenter view</button></div><div id=":$p"><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="1" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="1" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="ai-transformer-%E5%AE%8C%E5%85%A8%E3%82%AC%E3%82%A4%E3%83%89-2026">AI Transformer 完全ガイド 2026</h1>
<ul>
<li>アーキテクチャから最新研究まで</li>
<li></li>
<li><strong>対象</strong>: テックリード・アーキテクト</li>
<li><strong>レベル</strong>: 数式・実装・応用を網羅</li>
<li></li>
<li><em>Vaswani et al. 2017 「Attention Is All You Need」から現在まで</em></li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="2" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="2" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E3%81%93%E3%81%AE%E3%83%97%E3%83%AC%E3%82%BC%E3%83%B3%E3%83%86%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6">このプレゼンテーションについて</h1>
<ul>
<li><strong>目標</strong>: Transformerを「なぜそうなっているか」まで理解する</li>
<li><strong>前提知識</strong>: 行列演算・微分の基礎、深層学習の概念</li>
<li></li>
<li><strong>カバー範囲</strong>:</li>
<li>
<ul>
<li>アーキテクチャの数学的基盤（Q/K/V、Attention、LayerNorm）</li>
</ul>
</li>
<li>
<ul>
<li>主要変種（GPT / BERT / T5 / ViT / MoE）の設計思想</li>
</ul>
</li>
<li>
<ul>
<li>スケーリング則と効率化技術（Flash Attention / LoRA / QLoRA）</li>
</ul>
</li>
<li>
<ul>
<li>Fine-tuning・Alignmentパイプライン（RLHF / DPO）</li>
</ul>
</li>
<li>
<ul>
<li>最新研究動向（Mamba / o1 / Multimodal）</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="3" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="3" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E3%82%A2%E3%82%B8%E3%82%A7%E3%83%B3%E3%83%80">アジェンダ</h1>
<ul>
<li>
<ol>
<li><strong>Transformer以前の世界</strong> — RNN/LSTMの限界</li>
</ol>
</li>
<li>
<ol start="2">
<li><strong>アーキテクチャ全体像</strong> — Encoder-Decoder構造</li>
</ol>
</li>
<li>
<ol start="3">
<li><strong>Self-Attention深掘り</strong> — Q/K/V・数式・O(n²)</li>
</ol>
</li>
<li>
<ol start="4">
<li><strong>Positional Encoding</strong> — Sinusoidal・RoPE・ALiBi</li>
</ol>
</li>
<li>
<ol start="5">
<li><strong>FFN・正規化・残差接続</strong> — 数式詳解</li>
</ol>
</li>
<li>
<ol start="6">
<li><strong>主要変種</strong> — GPT/BERT/T5/ViT/MoE系譜</li>
</ol>
</li>
<li>
<ol start="7">
<li><strong>スケーリング則</strong> — Kaplan・Chinchilla法則</li>
</ol>
</li>
<li>
<ol start="8">
<li><strong>効率化技術</strong> — Flash Attention・KV Cache・LoRA</li>
</ol>
</li>
<li>
<ol start="9">
<li><strong>Fine-tuning &amp; Alignment</strong> — RLHF・DPO</li>
</ol>
</li>
<li>
<ol start="10">
<li><strong>実装・デプロイ</strong> — HuggingFace・vLLM</li>
</ol>
</li>
<li>
<ol start="11">
<li><strong>最新研究動向</strong> — Mamba・Reasoning・Multimodal</li>
</ol>
</li>
<li>
<ol start="12">
<li><strong>まとめ &amp; 参考文献</strong></li>
</ol>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="4" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="4" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-2-transformer%E4%BB%A5%E5%89%8D%E3%81%AE%E4%B8%96%E7%95%8C">Chapter 2: Transformer以前の世界</h1>
<ul>
<li>RNN / LSTM / Seq2Seq の限界と</li>
<li>「Attention Is All You Need」誕生の背景</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="5" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="5" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="rnn--%E9%80%90%E6%AC%A1%E5%87%A6%E7%90%86%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BB%95%E7%B5%84%E3%81%BF">RNN — 逐次処理モデルの仕組み</h1>
<ul>
<li><strong>基本式</strong>: <code>h_t = tanh(W_h × h_{t-1} + W_x × x_t + b)</code></li>
<li></li>
<li>
<ul>
<li>隠れ状態 h_t が「記憶」を担う</li>
</ul>
</li>
<li>
<ul>
<li>時刻 t の出力は t-1 の状態に依存（逐次依存）</li>
</ul>
</li>
<li>
<ul>
<li><strong>LSTM</strong>: ゲート機構（input/forget/output gate）で長期記憶を改善</li>
</ul>
</li>
<li>
<ul>
<li><code>f_t = σ(W_f [h_{t-1}, x_t] + b_f)</code> (forget gate)</li>
</ul>
</li>
<li>
<ul>
<li><code>c_t = f_t × c_{t-1} + i_t × g_t</code> (cell state)</li>
</ul>
</li>
<li>
<ul>
<li><strong>GRU</strong>: LSTMの簡略版（reset/update gate）</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="6" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="6" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="seq2seq--bahdanau-attention-2015">Seq2Seq + Bahdanau Attention (2015)</h1>
<ul>
<li><strong>Seq2Seq</strong>: Encoder RNN → context vector → Decoder RNN</li>
<li></li>
<li><strong>問題</strong>: ボトルネック — 全情報を1つのベクトルに圧縮</li>
<li></li>
<li><strong>Bahdanau Attention（2015年）の革新</strong>:</li>
<li>
<ul>
<li>Decoderの各ステップでEncoderの全隠れ状態を参照</li>
</ul>
</li>
<li>
<ul>
<li>アライメントスコア: <code>e_{ij} = a(s_{i-1}, h_j)</code></li>
</ul>
</li>
<li>
<ul>
<li>Attention重み: <code>α_{ij} = exp(e_{ij}) / Σ exp(e_{ik})</code></li>
</ul>
</li>
<li>
<ul>
<li>Context vector: <code>c_i = Σ α_{ij} × h_j</code></li>
</ul>
</li>
<li>
<ul>
<li>これが「Attention」の起源 — Transformerに結実</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="7" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="7" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="rnn%E3%81%AE3%E3%81%A4%E3%81%AE%E6%A0%B9%E6%9C%AC%E7%9A%84%E9%99%90%E7%95%8C">RNNの3つの根本的限界</h1>
<ul>
<li><strong>① 逐次処理 → GPUの並列化を活かせない</strong></li>
<li>
<ul>
<li>時刻 t は t-1 が終わらないと計算できない</li>
</ul>
</li>
<li>
<ul>
<li>系列長N で O(N) の直列ステップ</li>
</ul>
</li>
<li></li>
<li><strong>② 長距離依存の困難</strong></li>
<li>
<ul>
<li>離れた位置の情報が隠れ状態を「伝搬」する間に希薄化</li>
</ul>
</li>
<li>
<ul>
<li>LSTMで緩和されるが根本解決にはならない</li>
</ul>
</li>
<li></li>
<li><strong>③ 勾配消失 / 爆発</strong></li>
<li>
<ul>
<li>Back-Propagation Through Time (BPTT) で:</li>
</ul>
</li>
<li>
<ul>
<li><code>∂L/∂h_1 = ∏_{t=1}^{T} ∂h_t/∂h_{t-1}</code></li>
</ul>
</li>
<li>
<ul>
<li><code>|λ| &lt; 1</code> → 指数的消失 / <code>|λ| &gt; 1</code> → 指数的爆発</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="8" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="8" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E5%8B%BE%E9%85%8D%E6%B6%88%E5%A4%B1%E3%81%AE%E6%95%B0%E5%AD%A6%E7%9A%84%E8%A9%B3%E7%B4%B0">勾配消失の数学的詳細</h1>
<ul>
<li><strong>BPTT における勾配の積</strong>:</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li>∂L/∂W = Σ_t ∂L_t/∂W</li>
<li>∂L_t/∂h_1 = (∂h_t/∂h_{t-1}) × ... × (∂h_2/∂h_1)</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>       = ∏_{k=2}^{t} diag(σ'(W_h h_{k-1} + ...)) × W_h
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li><strong>問題</strong>: <code>W_h</code> の最大固有値 <code>|λ_max|</code></li>
<li>
<ul>
<li><code>|λ_max| &lt; 1</code>: 勾配 → 0（長期依存が学習できない）</li>
</ul>
</li>
<li>
<ul>
<li><code>|λ_max| &gt; 1</code>: 勾配 → ∞（学習が発散）</li>
</ul>
</li>
<li></li>
<li><strong>LSTM の部分的解決</strong>:</li>
<li>
<ul>
<li>Cell state <code>c_t</code> に加算ベースの更新（乗算でなく）</li>
</ul>
</li>
<li>
<ul>
<li><code>c_t = f_t ⊙ c_{t-1} + i_t ⊙ g_t</code> → 勾配ハイウェイ</li>
</ul>
</li>
<li>
<ul>
<li><strong>それでも</strong> 長い系列では勾配が薄れる</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="9" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="9" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="attention-is-all-you-need-2017-%E3%81%AE%E8%A1%9D%E6%92%83">&quot;Attention Is All You Need&quot; (2017) の衝撃</h1>
<ul>
<li><strong>Vaswani et al., Google Brain / Google Research (2017)</strong></li>
<li></li>
<li><strong>革命的提案</strong>: RNNを完全に排除し、Attentionのみで構成</li>
<li></li>
<li><strong>解決した問題</strong>:</li>
<li>
<ul>
<li>全トークン間の直接接続 → 長距離依存をO(1)で学習</li>
</ul>
</li>
<li>
<ul>
<li>完全並列計算 → 学習時間の劇的短縮</li>
</ul>
</li>
<li>
<ul>
<li>訓練可能なAttention重み → 柔軟な「参照」パターン</li>
</ul>
</li>
<li></li>
<li><strong>インパクト</strong>:</li>
<li>
<ul>
<li>2018: BERT（Google） / GPT（OpenAI）</li>
</ul>
</li>
<li>
<ul>
<li>2019: T5 / GPT-2 / RoBERTa</li>
</ul>
</li>
<li>
<ul>
<li>2020: GPT-3（175B）/ ViT</li>
</ul>
</li>
<li>
<ul>
<li>2022〜: ChatGPT / GPT-4 / LLaMA / Claude</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="10" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="10" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="rnn-vs-transformer--%E4%B8%A6%E5%88%97%E5%87%A6%E7%90%86%E3%81%AE%E6%9C%AC%E8%B3%AA%E7%9A%84%E9%81%95%E3%81%84">RNN vs Transformer — 並列処理の本質的違い</h1>
<p><img src="../assets/rnn-vs-transformer.svg" alt="center" style="width:900px;" /></p>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="11" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="11" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-3-transformer%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3%E5%85%A8%E4%BD%93%E5%83%8F">Chapter 3: Transformerアーキテクチャ全体像</h1>
<ul>
<li>Encoder-Decoder構造・Tokenization・Embedding・</li>
<li>行列次元の追跡</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="12" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="12" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="transformer%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3%E5%85%A8%E4%BD%93%E5%9B%B3">Transformerアーキテクチャ全体図</h1>
<p><img src="../assets/transformer-architecture.svg" alt="center" style="width:900px;" /></p>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="13" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="13" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="tokenization--%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%82%92%E6%95%B0%E5%80%A4%E5%88%97%E3%81%AB%E5%A4%89%E6%8F%9B">Tokenization — テキストを数値列に変換</h1>
<ul>
<li><strong>Tokenizationの種類</strong>:</li>
<li>
<ul>
<li><strong>Word-level</strong>: 単語単位（OOV問題あり）</li>
</ul>
</li>
<li>
<ul>
<li><strong>Character-level</strong>: 文字単位（系列が長くなりすぎる）</li>
</ul>
</li>
<li>
<ul>
<li><strong>BPE (Byte-Pair Encoding)</strong>: GPT系が採用</li>
</ul>
</li>
<li>
<ul>
<li>頻出ペアを繰り返しマージして語彙を構築</li>
</ul>
</li>
<li>
<ul>
<li><strong>WordPiece</strong>: BERT系が採用（BPEの変種）</li>
</ul>
</li>
<li>
<ul>
<li><strong>SentencePiece</strong>: 言語非依存（LLaMA等）</li>
</ul>
</li>
<li></li>
<li><strong>語彙サイズ (|V|)</strong>:</li>
<li>
<ul>
<li>GPT-2/3: 50,257 / GPT-4: ~100K / LLaMA: 32K</li>
</ul>
</li>
<li>
<ul>
<li>Token ID → 整数インデックスでEmbedding lookupに利用</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="14" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="14" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="input-embedding--%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E3%82%92%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E7%A9%BA%E9%96%93%E3%81%B8">Input Embedding — トークンをベクトル空間へ</h1>
<ul>
<li><strong>Embedding行列</strong>: <code>E ∈ ℝ^{|V| × d_model}</code></li>
<li></li>
<li>
<ul>
<li>Token ID <code>i</code> → <code>E[i, :] ∈ ℝ^{d_model}</code> (d_model次元ベクトル)</li>
</ul>
</li>
<li>
<ul>
<li>典型的な値: <code>d_model = 512</code> (原論文) / <code>768</code> (BERT-base) / <code>4096</code> (LLaMA-7B)</li>
</ul>
</li>
<li></li>
<li><strong>スケーリング</strong>:</li>
<li>
<ul>
<li>原論文: <code>Embedding × √d_model</code> でスケール</li>
</ul>
</li>
<li>
<ul>
<li>理由: Positional EncodingとEmbeddingのスケールを揃える</li>
</ul>
</li>
<li></li>
<li><strong>行列次元追跡 (sequence length = n, batch = B)</strong>:</li>
<li>
<ul>
<li>Input: <code>[B, n]</code> (token IDs)</li>
</ul>
</li>
<li>
<ul>
<li>After Embedding: <code>[B, n, d_model]</code></li>
</ul>
</li>
<li>
<ul>
<li>After PE addition: <code>[B, n, d_model]</code></li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="15" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="15" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="encoder%E3%83%96%E3%83%AD%E3%83%83%E3%82%AF%E3%81%AE%E8%A9%B3%E7%B4%B0">Encoderブロックの詳細</h1>
<ul>
<li><strong>Encoderブロック（×N層）の構造</strong>:</li>
<li></li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li>Input x ∈ ℝ^{n × d_model}</li>
<li>↓</li>
<li>x' = LayerNorm(x)            # Pre-Norm (modern)</li>
<li>↓</li>
<li>attn = MultiHeadAttention(x', x', x')  # self-attention</li>
<li>↓</li>
<li>x = x + attn                 # Residual connection</li>
<li>↓</li>
<li>x' = LayerNorm(x)            # Pre-Norm</li>
<li>↓</li>
<li>ffn = FFN(x')                # Feed-Forward</li>
<li>↓</li>
<li>x = x + ffn                  # Residual connection</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li></li>
<li>原論文(Post-Norm): LayerNormが残差加算の後</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="16" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="16" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="decoder%E3%83%96%E3%83%AD%E3%83%83%E3%82%AF%E3%81%AE%E8%A9%B3%E7%B4%B0">Decoderブロックの詳細</h1>
<ul>
<li><strong>Decoderブロック（×N層）の追加機構</strong>:</li>
<li></li>
<li>
<ul>
<li><strong>Masked Self-Attention</strong>: 未来のトークンを参照禁止</li>
</ul>
</li>
<li>
<ul>
<li>Attention maskで上三角を <code>-∞</code> に設定</li>
</ul>
</li>
<li>
<ul>
<li><code>softmax</code> 後に 0 になり影響ゼロ</li>
</ul>
</li>
<li></li>
<li>
<ul>
<li><strong>Cross-Attention (Encoder-Decoder Attention)</strong>:</li>
</ul>
</li>
<li>
<ul>
<li><code>Q</code>: Decoderの前層出力</li>
</ul>
</li>
<li>
<ul>
<li><code>K, V</code>: <strong>Encoderの最終出力</strong>（固定）</li>
</ul>
</li>
<li>
<ul>
<li>DecoderがEncoderの情報を参照するゲートウェイ</li>
</ul>
</li>
<li></li>
<li>
<ul>
<li><strong>Causal Language Modeling (GPT)</strong>:</li>
</ul>
</li>
<li>
<ul>
<li>Decoder-onlyではCross-Attentionを省略</li>
</ul>
</li>
<li>
<ul>
<li>Masked Self-Attentionのみで自回帰生成</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="17" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="17" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E8%A1%8C%E5%88%97%E6%AC%A1%E5%85%83%E3%81%AE%E5%AE%8C%E5%85%A8%E8%BF%BD%E8%B7%A1-d_model512-h8-n10">行列次元の完全追跡 (d_model=512, h=8, n=10)</h1>
<ul>
<li><strong>入力から出力まで次元を追う</strong>:</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li>Input tokens:        [B, n=10]</li>
<li>After Embedding:     [B, 10, d_model=512]</li>
<li>After PE:            [B, 10, 512]</li>
<li></li>
<li>
<h1 id="multi-head-attention-h8-d_k--d_v--d_modelh--64">Multi-Head Attention (h=8, d_k = d_v = d_model/h = 64)</h1>
</li>
<li>WQ, WK, WV:          [512, 64] × 8 heads</li>
<li>Q, K, V per head:    [B, 10, 64]</li>
<li>Attention scores:    [B, 10, 10]  ← n×n matrix</li>
<li>Attention output:    [B, 10, 64] × 8 heads</li>
<li>After Concat:        [B, 10, 512]</li>
<li>After W^O:           [B, 10, 512]</li>
<li></li>
<li>
<h1 id="feed-forward-d_ff--2048--4-%C3%97-d_model">Feed-Forward (d_ff = 2048 = 4 × d_model)</h1>
</li>
<li>After FFN:           [B, 10, 512]</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>

</code></pre>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="18" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="18" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-4-self-attention-%E6%B7%B1%E6%8E%98%E3%82%8A">Chapter 4: Self-Attention 深掘り</h1>
<ul>
<li>Q/K/V 行列演算 · Scaled Dot-Product · Multi-Head ·</li>
<li>√d_k スケーリング · Attention Entropy · O(n²)問題</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="19" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="19" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="attention%E3%81%AE%E7%9B%B4%E6%84%9F--%E3%81%A9%E3%81%93%E3%81%AB%E6%B3%A8%E7%9B%AE%E3%81%99%E3%82%8B%E3%81%8B">Attentionの直感 — 「どこに注目するか」</h1>
<ul>
<li><strong>日本語の例</strong>: 「銀行に行った。そこで手続きした。」</li>
<li>
<ul>
<li>「そこ」を処理するとき → 「銀行」に高いAttention</li>
</ul>
</li>
<li></li>
<li><strong>Transformerでの実現</strong>:</li>
<li>
<ul>
<li>各トークンが「何を探しているか」= <strong>Query (Q)</strong></li>
</ul>
</li>
<li>
<ul>
<li>各トークンが「何を持っているか」= <strong>Key (K)</strong></li>
</ul>
</li>
<li>
<ul>
<li>各トークンが「実際の情報」= <strong>Value (V)</strong></li>
</ul>
</li>
<li></li>
<li><strong>例えると</strong>:</li>
<li>
<ul>
<li>Query = 「図書館で科学書を探している」</li>
</ul>
</li>
<li>
<ul>
<li>Key = 書籍の背表紙タイトル</li>
</ul>
</li>
<li>
<ul>
<li>Value = 書籍の実際の内容</li>
</ul>
</li>
<li>
<ul>
<li>マッチ度（Q·K）が高い本の内容（V）を多く取り込む</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="20" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="20" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="qkv-%E8%A1%8C%E5%88%97%E6%BC%94%E7%AE%97%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96">Q/K/V 行列演算の可視化</h1>
<p><img src="../assets/self-attention-qkv.svg" alt="center" style="width:900px;" /></p>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="21" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="21" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="qkv-%E6%8A%95%E5%BD%B1--%E7%B7%9A%E5%BD%A2%E5%A4%89%E6%8F%9B%E3%81%AE%E6%95%B0%E5%BC%8F">Q/K/V 投影 — 線形変換の数式</h1>
<ul>
<li><strong>入力 X ∈ ℝ^{n × d_model} から Q, K, V を生成</strong>:</li>
<li></li>
<li><code>Q = X W^Q</code>  ← <code>W^Q ∈ ℝ^{d_model × d_k}</code></li>
<li><code>K = X W^K</code>  ← <code>W^K ∈ ℝ^{d_model × d_k}</code></li>
<li><code>V = X W^V</code>  ← <code>W^V ∈ ℝ^{d_model × d_v}</code></li>
<li></li>
<li><strong>結果の次元</strong>:</li>
<li>
<ul>
<li><code>Q, K ∈ ℝ^{n × d_k}</code></li>
</ul>
</li>
<li>
<ul>
<li><code>V ∈ ℝ^{n × d_v}</code></li>
</ul>
</li>
<li>
<ul>
<li>通常: <code>d_k = d_v = d_model / h</code> (h = head数)</li>
</ul>
</li>
<li></li>
<li><strong>重要</strong>: Q/K/Vは「同じ入力Xから異なる投影」</li>
<li>→ Self-Attentionと呼ばれる所以（自分自身への注意）</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="22" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="22" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="scaled-dot-product-attention--%E5%AE%8C%E5%85%A8%E3%81%AA%E6%95%B0%E5%BC%8F">Scaled Dot-Product Attention — 完全な数式</h1>
<ul>
<li><strong>定義</strong> (Vaswani et al. 2017, Eq. 1):</li>
<li></li>
<li>
<h2 id="attentionq-k-v--softmaxqkt--%E2%88%9Ad_k-v"><code>Attention(Q, K, V) = softmax(QK^T / √d_k) V</code></h2>
</li>
<li></li>
<li><strong>各ステップの意味</strong>:</li>
<li>
<ol>
<li><code>QK^T ∈ ℝ^{n × n}</code>: 全トークン対のスコア（類似度行列）</li>
</ol>
</li>
<li>
<ol start="2">
<li><code>/ √d_k</code>: スケーリング（後述）</li>
</ol>
</li>
<li>
<ol start="3">
<li><code>softmax(...)</code>: 各行を確率分布に正規化</li>
</ol>
</li>
<li>
<ul>
<li><code>softmax(x_i) = exp(x_i) / Σ_j exp(x_j)</code></li>
</ul>
</li>
<li>
<ol start="4">
<li><code>× V</code>: 価値ベクトルの加重平均</li>
</ol>
</li>
<li></li>
<li><strong>出力</strong>: <code>∈ ℝ^{n × d_v}</code> — 各位置の「文脈を考慮した表現」</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="23" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="23" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E2%88%9Ad_k-%E3%82%B9%E3%82%B1%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%AE%E6%95%B0%E5%AD%A6%E7%9A%84%E6%A0%B9%E6%8B%A0">√d_k スケーリングの数学的根拠</h1>
<ul>
<li><strong>なぜ √d_k で割るのか？</strong></li>
<li></li>
<li><strong>仮定</strong>: <code>q, k</code> の各要素が平均0、分散1の独立な確率変数</li>
<li></li>
<li><strong>内積 q·k の分散</strong>:</li>
<li>
<ul>
<li><code>Var(q·k) = Var(Σ_{i=1}^{d_k} q_i k_i) = d_k</code></li>
</ul>
</li>
<li>
<ul>
<li>→ 標準偏差 = <strong>√d_k</strong></li>
</ul>
</li>
<li></li>
<li><strong>問題</strong>: <code>d_k</code> が大きいほどdot productの値が大きくなる</li>
<li>→ <code>softmax</code> に大きな値が入ると勾配が消失</li>
<li></li>
<li><strong>softmax飽和</strong>: <code>softmax([10, -10, ...]) ≈ [1, 0, ...]</code></li>
<li>→ 勾配がほぼゼロ → 学習が止まる</li>
<li></li>
<li><strong>解決</strong>: <code>/ √d_k</code> で分散を1に正規化 → 安定した勾配</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="24" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="24" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="softmax%E6%B8%A9%E5%BA%A6%E3%81%A8%E6%B3%A8%E6%84%8F%E5%88%86%E5%B8%83">Softmax温度と注意分布</h1>
<ul>
<li><strong>Softmax温度パラメータ τ の影響</strong>:</li>
<li></li>
<li><code>softmax(QK^T / τ)</code> — τ = √d_k が標準</li>
<li></li>
<li>
<ul>
<li><strong>τ → 0</strong> (温度低): 分布がシャープ → 少数のトークンに集中</li>
</ul>
</li>
<li>
<ul>
<li><code>softmax([10, 9, 8] / 0.1) ≈ [0.997, 0.003, 0.000]</code></li>
</ul>
</li>
<li></li>
<li>
<ul>
<li><strong>τ → ∞</strong> (温度高): 分布が均一 → 全トークンに等分</li>
</ul>
</li>
<li>
<ul>
<li><code>softmax([10, 9, 8] / 100) ≈ [0.34, 0.33, 0.33]</code></li>
</ul>
</li>
<li></li>
<li><strong>Attention Entropy</strong>:</li>
<li>
<ul>
<li><code>H = -Σ_i α_i log(α_i)</code></li>
</ul>
</li>
<li>
<ul>
<li>高エントロピー: 広い文脈参照（一般的な単語）</li>
</ul>
</li>
<li>
<ul>
<li>低エントロピー: 特定の単語に集中（固有名詞・代名詞解決）</li>
</ul>
</li>
<li>
<ul>
<li>ヘッドによって異なるパターン → Multi-Headの意義</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="25" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="25" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="multi-head-attention-%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96">Multi-Head Attention の可視化</h1>
<p><img src="../assets/multi-head-attention.svg" alt="center" style="width:900px;" /></p>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="26" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="26" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="multi-head-attention--%E6%95%B0%E5%BC%8F%E3%81%A8%E8%A8%AD%E8%A8%88%E6%80%9D%E6%83%B3">Multi-Head Attention — 数式と設計思想</h1>
<ul>
<li><strong>定義</strong>:</li>
<li><code>MultiHead(Q,K,V) = Concat(head_1, ..., head_h) W^O</code></li>
<li></li>
<li><code>head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)</code></li>
<li></li>
<li><strong>パラメータ数 (d_model=512, h=8)</strong>:</li>
<li>
<ul>
<li>各ヘッド: <code>W_i^Q, W_i^K, W_i^V ∈ ℝ^{512×64}</code></li>
</ul>
</li>
<li>
<ul>
<li><code>W^O ∈ ℝ^{512×512}</code> (Concat後の投影)</li>
</ul>
</li>
<li>
<ul>
<li>計算コスト: Single-head と同等（実装上の工夫）</li>
</ul>
</li>
<li></li>
<li><strong>なぜ複数ヘッドか？</strong></li>
<li>
<ul>
<li>各ヘッドが異なる「注目パターン」を学習</li>
</ul>
</li>
<li>
<ul>
<li>Head 1: 構文依存関係 / Head 2: 照応解決</li>
</ul>
</li>
<li>
<ul>
<li>Head 3: 近傍文脈 / Head 8: 意味的類似性</li>
</ul>
</li>
<li>
<ul>
<li>単一ヘッドでは表現力が限られる</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="27" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="27" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="masked-self-attention--%E5%9B%A0%E6%9E%9C%E3%83%9E%E3%82%B9%E3%82%AF">Masked Self-Attention — 因果マスク</h1>
<ul>
<li><strong>Decoder（GPT系）での必須制約</strong>: 未来は見えない</li>
<li></li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"></code></pre>
</li>
<li>
<h1 id="causal-mask-n%C3%97n-%E3%81%AE%E4%B8%8A%E4%B8%89%E8%A7%92%E3%82%92--inf-%E3%81%AB">Causal mask: n×n の上三角を -inf に</h1>
</li>
<li>def causal_mask(n):</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>mask = torch.triu(torch.ones(n, n), diagonal=1)
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>return mask.masked_fill(mask == 1, float('-inf'))
</code></pre>
</li>
<li></li>
<li>
<h1 id="attention-score-%E3%81%AB%E5%8A%A0%E7%AE%97">Attention score に加算</h1>
</li>
<li>scores = (Q @ K.transpose(-2,-1)) / math.sqrt(d_k)</li>
<li>scores = scores + causal_mask(n)  # 上三角が -inf</li>
<li>attn = F.softmax(scores, dim=-1)  # -inf → 0</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li></li>
<li><strong>直感</strong>: トークン <code>t</code> は <code>t'≤t</code> のトークンのみ参照可能</li>
<li>→ 自回帰生成（1トークンずつ順番に生成）が成立</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="28" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="28" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="on%C2%B2-%E3%81%AE%E5%95%8F%E9%A1%8C--%E3%82%B3%E3%83%B3%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E9%95%B7%E3%81%AE%E5%A3%81">O(n²) の問題 — コンテキスト長の壁</h1>
<ul>
<li><strong>Self-Attentionの計算量・メモリ量</strong>:</li>
<li></li>
<li>
<ul>
<li><strong>計算量</strong>: <code>O(n² × d_model)</code> — QK^T は n×n 行列</li>
</ul>
</li>
<li>
<ul>
<li><strong>メモリ</strong>: <code>O(n²)</code> — Attention行列の保存</li>
</ul>
</li>
<li></li>
<li><strong>具体的な限界</strong>:</li>
<li>
<ul>
<li>n=1K: Attention行列 = 1M要素 → 問題なし</li>
</ul>
</li>
<li>
<ul>
<li>n=8K: 64M要素 → GPU VRAMがボトルネック</li>
</ul>
</li>
<li>
<ul>
<li>n=128K: 16B要素 → A100 (80GB) でも直接計算不可</li>
</ul>
</li>
<li></li>
<li><strong>2024-2025年の主な解決策</strong>:</li>
<li>
<ul>
<li><strong>Flash Attention</strong>: IO-Aware算法でメモリO(n)に</li>
</ul>
</li>
<li>
<ul>
<li><strong>Sliding Window (Mistral)</strong>: 局所窓で O(n×w)</li>
</ul>
</li>
<li>
<ul>
<li><strong>Sparse Attention</strong>: 選択的注意でO(n√n)</li>
</ul>
</li>
<li>
<ul>
<li><strong>Mamba/SSM</strong>: O(n)の代替アーキテクチャ</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="29" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="29" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-5-positional-encoding-%E8%A9%B3%E8%A7%A3">Chapter 5: Positional Encoding 詳解</h1>
<ul>
<li>Sinusoidal PE · Learned PE · RoPE · ALiBi ·</li>
<li>位置情報の数学的表現</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="30" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="30" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E3%81%AA%E3%81%9C-positional-encoding-%E3%81%8C%E5%BF%85%E8%A6%81%E3%81%8B">なぜ Positional Encoding が必要か</h1>
<ul>
<li><strong>Self-Attentionは位置不変 (Permutation Invariant)</strong></li>
<li></li>
<li>入力系列をシャッフルしても Attention値は変わらない:</li>
<li><code>softmax(QK^T / √d_k)V</code> に順序情報なし</li>
<li></li>
<li><strong>問題</strong>: 「猫が犬を追いかけた」と「犬が猫を追いかけた」が同じ表現になる</li>
<li></li>
<li><strong>解決策</strong>: 位置情報を埋め込む</li>
<li>
<ul>
<li><code>x_pos = x_embed + PE(pos)</code></li>
</ul>
</li>
<li>
<ul>
<li><code>PE(pos) ∈ ℝ^{d_model}</code> — 位置固有のベクトル</li>
</ul>
</li>
<li></li>
<li><strong>要件</strong>:</li>
<li>
<ul>
<li>各位置が一意に表現される</li>
</ul>
</li>
<li>
<ul>
<li>相対距離が計算可能</li>
</ul>
</li>
<li>
<ul>
<li>学習データにない長さの系列にも汎化</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="31" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="31" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="sinusoidal-pe--%E5%8E%9F%E8%AB%96%E6%96%87%E3%81%AE%E5%AE%9A%E5%BC%8F%E5%8C%96">Sinusoidal PE — 原論文の定式化</h1>
<ul>
<li><strong>Vaswani et al. (2017) の定義</strong>:</li>
<li></li>
<li><code>PE(pos, 2i) = sin(pos / 10000^{2i/d_model})</code></li>
<li><code>PE(pos, 2i+1) = cos(pos / 10000^{2i/d_model})</code></li>
<li></li>
<li>
<ul>
<li><code>pos</code>: トークンの位置 (0, 1, 2, ...)</li>
</ul>
</li>
<li>
<ul>
<li><code>i</code>: 次元のインデックス (0 ≤ i &lt; d_model/2)</li>
</ul>
</li>
<li></li>
<li><strong>設計の意図</strong>:</li>
<li>
<ul>
<li>異なる周波数の sin/cos → 各次元で異なるスケールの位置情報</li>
</ul>
</li>
<li>
<ul>
<li>低インデックス次元: 高周波（細かい位置変化）</li>
</ul>
</li>
<li>
<ul>
<li>高インデックス次元: 低周波（大域的な位置構造）</li>
</ul>
</li>
<li></li>
<li><strong>相対位置の表現</strong>:</li>
<li><code>PE(pos+k)</code> は <code>PE(pos)</code> の線形変換 → 相対距離が内積で計算可能</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="32" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="32" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="positional-encoding-%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96">Positional Encoding の可視化</h1>
<p><img src="../assets/positional-encoding.svg" alt="center" style="width:900px;" /></p>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="33" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="33" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="learned-pe-%E3%81%A8-rope">Learned PE と RoPE</h1>
<ul>
<li><strong>Learned Positional Embedding (GPT/BERT)</strong>:</li>
<li>
<ul>
<li>Sinusoidalの代わりに学習可能な行列 <code>E_pos ∈ ℝ^{max_len × d_model}</code></li>
</ul>
</li>
<li>
<ul>
<li>学習データの位置分布に適応できる</li>
</ul>
</li>
<li>
<ul>
<li>欠点: 学習時より長い系列に汎化しにくい</li>
</ul>
</li>
<li></li>
<li><strong>RoPE — Rotary Position Embedding (Su et al. 2021)</strong>:</li>
<li>
<ul>
<li>LLaMA / GPT-NeoX / Falcon などが採用</li>
</ul>
</li>
<li>
<ul>
<li>位置情報を<strong>回転行列</strong>として埋め込む</li>
</ul>
</li>
<li>
<ul>
<li><code>f_q(x_m, m) = R_Θ,m^d W_q x_m</code></li>
</ul>
</li>
<li>
<ul>
<li><code>R_Θ,m</code> は位置 m に応じた 2D回転行列のブロック対角</li>
</ul>
</li>
<li>
<ul>
<li><strong>内積が相対位置のみに依存</strong>: <code>q_m^T k_n ∝ g(x_m, x_n, m-n)</code></li>
</ul>
</li>
<li>
<ul>
<li>長い系列への外挿性が良い（YaRN等でさらに改善）</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="34" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="34" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="alibi-%E3%81%A8%E7%9B%B8%E5%AF%BE%E4%BD%8D%E7%BD%AE%E3%82%A8%E3%83%B3%E3%82%B3%E3%83%BC%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0">ALiBi と相対位置エンコーディング</h1>
<ul>
<li><strong>ALiBi — Attention with Linear Biases (Press et al. 2022)</strong>:</li>
<li>
<ul>
<li>Attentionスコアに線形バイアスを加算（PE不要）</li>
</ul>
</li>
<li>
<ul>
<li><code>softmax(QK^T / √d_k + m × M)</code></li>
</ul>
</li>
<li>
<ul>
<li><code>M_{ij} = -|i - j|</code> （相対距離）</li>
</ul>
</li>
<li>
<ul>
<li><code>m</code>: ヘッドごとの異なる傾き（固定超パラメータ）</li>
</ul>
</li>
<li>
<ul>
<li><strong>長文脈への強力な外挿性</strong>: 学習長を超えた推論でも性能維持</li>
</ul>
</li>
<li></li>
<li><strong>比較まとめ</strong>:<br />
| 手法 | 学習パラメータ | 外挿性 | 採用例 |<br />
|------|------------|--------|-------|<br />
| Sinusoidal | なし | 中 | 原Transformer |<br />
| Learned PE | あり | 低 | BERT, GPT-2 |<br />
| RoPE | なし | 高 | LLaMA, GPT-NeoX |<br />
| ALiBi | なし | 最高 | BLOOM, MPT |</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="35" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="35" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-6-ffn%E3%83%BB%E6%AD%A3%E8%A6%8F%E5%8C%96%E3%83%BB%E6%AE%8B%E5%B7%AE%E6%8E%A5%E7%B6%9A">Chapter 6: FFN・正規化・残差接続</h1>
<ul>
<li>Feed-Forward Network · GELU/SiLU · LayerNorm ·</li>
<li>Residual Connection · Pre-Norm vs Post-Norm</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="36" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="36" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="feed-forward-network--%E6%95%B0%E5%BC%8F%E3%81%A8%E5%BD%B9%E5%89%B2">Feed-Forward Network — 数式と役割</h1>
<ul>
<li><strong>FFNの数式</strong> (2層の全結合層):</li>
<li></li>
<li><code>FFN(x) = max(0, xW_1 + b_1) W_2 + b_2</code>  (原論文: ReLU)</li>
<li></li>
<li><strong>現代の変種</strong>:</li>
<li>
<ul>
<li>GELU版: <code>FFN(x) = GELU(xW_1 + b_1) W_2 + b_2</code></li>
</ul>
</li>
<li>
<ul>
<li>SwiGLU版: <code>FFN(x) = (xW_1 ⊙ σ(xW_3)) W_2</code> (LLaMA採用)</li>
</ul>
</li>
<li></li>
<li><strong>次元</strong>: <code>d_ff = 4 × d_model</code> (典型的)</li>
<li>
<ul>
<li>d_model=512 → d_ff=2048 / d_model=4096 → d_ff=11008 (LLaMA)</li>
</ul>
</li>
<li></li>
<li><strong>役割</strong>: 各トークンの表現を非線形に変換</li>
<li>
<ul>
<li>Self-Attentionが「どの情報を集めるか」</li>
</ul>
</li>
<li>
<ul>
<li>FFNが「集めた情報をどう処理するか」</li>
</ul>
</li>
<li>
<ul>
<li>FFNがモデルの「知識」を格納する場所とも言われる</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="37" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="37" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="gelu--silu-%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0">GELU / SiLU 活性化関数</h1>
<ul>
<li><strong>GELU (Gaussian Error Linear Unit)</strong>:</li>
<li><code>GELU(x) = x × Φ(x)</code> — Φ は標準正規分布のCDF</li>
<li>近似: <code>GELU(x) ≈ 0.5x(1 + tanh[√(2/π)(x + 0.044715x³)])</code></li>
<li>
<ul>
<li>ReLUと異なり x&lt;0 でも小さい勾配が流れる</li>
</ul>
</li>
<li>
<ul>
<li>BERT, GPT-2 などが採用</li>
</ul>
</li>
<li></li>
<li><strong>SiLU / Swish</strong>:</li>
<li><code>SiLU(x) = x × σ(x)</code> — σ はシグモイド関数</li>
<li>
<ul>
<li>自己ゲートつき活性化（gating mechanism）</li>
</ul>
</li>
<li></li>
<li><strong>SwiGLU (Shazeer 2020)</strong>:</li>
<li><code>SwiGLU(x, W, V) = SiLU(xW) ⊙ (xV)</code></li>
<li>
<ul>
<li>ゲート付きFFN — PaLM, LLaMA, GPT-4 (推定) が採用</li>
</ul>
</li>
<li>
<ul>
<li>ReLU/GELUより一貫して高い性能</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="38" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="38" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="layer-normalization--%E5%AE%89%E5%AE%9A%E5%AD%A6%E7%BF%92%E3%81%AE%E9%8D%B5">Layer Normalization — 安定学習の鍵</h1>
<ul>
<li><strong>定義</strong>:</li>
<li><code>LayerNorm(x) = γ × (x - μ) / (σ + ε) + β</code></li>
<li></li>
<li>
<ul>
<li><code>μ = (1/d) Σ_i x_i</code> — 特徴次元の平均</li>
</ul>
</li>
<li>
<ul>
<li><code>σ² = (1/d) Σ_i (x_i - μ)²</code> — 分散</li>
</ul>
</li>
<li>
<ul>
<li><code>γ, β ∈ ℝ^d</code> — 学習可能なスケール/シフト</li>
</ul>
</li>
<li>
<ul>
<li><code>ε</code>: 数値安定のための小定数（典型: 1e-5）</li>
</ul>
</li>
<li></li>
<li><strong>BatchNorm との違い</strong>:</li>
<li>
<ul>
<li>BatchNorm: バッチ方向で正規化 → バッチサイズ依存</li>
</ul>
</li>
<li>
<ul>
<li>LayerNorm: 特徴次元で正規化 → バッチサイズ非依存</li>
</ul>
</li>
<li>
<ul>
<li>可変長系列・小バッチ → <strong>LayerNormが優れる</strong></li>
</ul>
</li>
<li></li>
<li><strong>RMSNorm (Llama採用)</strong>:</li>
<li><code>RMSNorm(x) = x / RMS(x) × γ</code></li>
<li><code>RMS(x) = √(1/d Σ x_i²)</code> — μ計算を省略、高速</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="39" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="39" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="residual-connection-%E3%81%A8-prepost-norm">Residual Connection と Pre/Post-Norm</h1>
<ul>
<li><strong>Residual Connection (He et al. 2016)</strong>:</li>
<li><code>output = x + SubLayer(x)</code></li>
<li>
<ul>
<li>勾配が「飛び越え」て伝播 → 深いネットワークで安定学習</li>
</ul>
</li>
<li></li>
<li><strong>Post-Norm (原論文)</strong>:</li>
<li><code>output = LayerNorm(x + SubLayer(x))</code></li>
<li>
<ul>
<li>各サブレイヤーの後に正規化</li>
</ul>
</li>
<li>
<ul>
<li>深いモデルでは学習が不安定になりやすい</li>
</ul>
</li>
<li></li>
<li><strong>Pre-Norm (現代の標準)</strong>:</li>
<li><code>output = x + SubLayer(LayerNorm(x))</code></li>
<li>
<ul>
<li>サブレイヤーへの入力を正規化</li>
</ul>
</li>
<li>
<ul>
<li>安定した勾配流 → 大規模モデルで主流</li>
</ul>
</li>
<li>
<ul>
<li>GPT-3, LLaMA, PaLM など大規模LLMが採用</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="40" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="40" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="dropout-%E3%81%A8%E6%AD%A3%E5%89%87%E5%8C%96">Dropout と正則化</h1>
<ul>
<li><strong>Dropout</strong> (Srivastava et al. 2014):</li>
<li>
<ul>
<li>訓練時にニューロンをランダムに無効化（確率 p）</li>
</ul>
</li>
<li>
<ul>
<li><code>Dropout(x)_i = x_i / (1-p)</code> (活性時のスケール補正)</li>
</ul>
</li>
<li>
<ul>
<li>過学習防止・アンサンブル効果</li>
</ul>
</li>
<li></li>
<li><strong>Transformer でのDropout適用箇所</strong>:</li>
<li>
<ul>
<li>Attention重み後: <code>Dropout(softmax(...))</code></li>
</ul>
</li>
<li>
<ul>
<li>各サブレイヤーの出力後</li>
</ul>
</li>
<li>
<ul>
<li>Embedding後</li>
</ul>
</li>
<li></li>
<li><strong>大規模LLMでの傾向</strong>:</li>
<li>
<ul>
<li>データが大量にある場合、Dropout不要か効果小</li>
</ul>
</li>
<li>
<ul>
<li>GPT-3: <code>p=0.1</code> / LLaMA: Dropout なし</li>
</ul>
</li>
<li></li>
<li><strong>Weight Decay</strong> (L2正則化) は引き続き有効:</li>
<li><code>L_reg = L + λ Σ ||W||²_F</code></li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="41" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="41" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-7-transformer%E3%81%AE%E4%B8%BB%E8%A6%81%E5%A4%89%E7%A8%AE">Chapter 7: Transformerの主要変種</h1>
<ul>
<li>GPT / BERT / T5 / ViT / CLIP / MoE —</li>
<li>設計思想と使い分け</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="42" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="42" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="decoder-only-gpt%E7%B3%BB%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3">Decoder-only: GPT系アーキテクチャ</h1>
<ul>
<li><strong>GPT (Generative Pre-trained Transformer, OpenAI 2018)</strong></li>
<li></li>
<li><strong>アーキテクチャ</strong>: Decoder-only (Encoder/Cross-Attentionなし)</li>
<li><strong>学習目標</strong>: 因果言語モデリング (CLM)</li>
<li><code>L_CLM = -Σ_t log P(x_t | x_{&lt;t}; θ)</code></li>
<li></li>
<li><strong>訓練</strong>: 次のトークンを予測し続けるだけ</li>
<li>
<ul>
<li>BPEトークン化 → Masked Self-Attention × N層</li>
</ul>
</li>
<li></li>
<li><strong>スケール進化</strong>:<br />
| モデル | パラメータ | 訓練データ | 年 |<br />
|-------|-----------|-----------|---|<br />
| GPT-1 | 117M | BooksCorpus (4.5GB) | 2018 |<br />
| GPT-2 | 1.5B | WebText (40GB) | 2019 |<br />
| GPT-3 | 175B | 570GB | 2020 |<br />
| GPT-4 | 不明 (推定 ~1T) | 不明 | 2023 |</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="43" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="43" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="gpt%E3%81%AE%E5%AD%A6%E7%BF%92-%E5%9B%A0%E6%9E%9C%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%A8%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E7%94%9F%E6%88%90">GPTの学習: 因果言語モデリングとテキスト生成</h1>
<ul>
<li><strong>Pre-training</strong>: 次トークン予測</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"></code></pre>
</li>
<li>
<h1 id="gpt-2-style-inference-huggingface">GPT-2 style inference (HuggingFace)</h1>
</li>
<li>from transformers import GPT2LMHeadModel, GPT2Tokenizer</li>
<li>model = GPT2LMHeadModel.from_pretrained('gpt2')</li>
<li>tokenizer = GPT2Tokenizer.from_pretrained('gpt2')</li>
<li>inputs = tokenizer('Attention is all you need', return_tensors='pt')</li>
<li>output = model.generate(**inputs, max_new_tokens=50,</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>                    do_sample=True, temperature=0.8)
</code></pre>
</li>
<li>print(tokenizer.decode(output[0]))</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li></li>
<li><strong>Sampling 戦略</strong>:</li>
<li>
<ul>
<li>Greedy: <code>argmax P(x_t | x_{&lt;t})</code> — 決定論的だが単調</li>
</ul>
</li>
<li>
<ul>
<li>Temperature: <code>P_τ ∝ P^{1/τ}</code> — 多様性を調整</li>
</ul>
</li>
<li>
<ul>
<li>Top-k / Top-p (Nucleus) sampling: 確率上位のみからサンプル</li>
</ul>
</li>
<li>
<ul>
<li>Beam Search: 複数候補を同時追跡</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="44" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="44" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="encoder-only-bert--%E3%83%9E%E3%82%B9%E3%82%AF%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AA%E3%83%B3%E3%82%B0">Encoder-only: BERT / マスク言語モデリング</h1>
<ul>
<li><strong>BERT (Bidirectional Encoder Representations, Google 2018)</strong></li>
<li></li>
<li><strong>設計</strong>: Encoder-only — 両方向のContextを利用</li>
<li><strong>学習目標 1 — MLM (Masked Language Model)</strong>:</li>
<li>
<ul>
<li>入力の15%をランダムにマスク: <code>[MASK]</code></li>
</ul>
</li>
<li>
<ul>
<li><code>L_MLM = -Σ_{i∈masked} log P(x_i | x_{\i}; θ)</code></li>
</ul>
</li>
<li></li>
<li><strong>学習目標 2 — NSP (Next Sentence Prediction)</strong>:</li>
<li>
<ul>
<li>2文が隣接するか否かを分類 (後に批判・廃止傾向)</li>
</ul>
</li>
<li></li>
<li><strong>GPTとの根本的違い</strong>:<br />
| | BERT | GPT |<br />
|---|------|-----|<br />
| 方向性 | 双方向 | 単方向 |<br />
| 用途 | 分類・抽出 | 生成 |<br />
| Attention | Full (Mask不要) | Causal Mask |<br />
| 代表 | RoBERTa, ELECTRA | GPT-4, LLaMA |</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="45" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="45" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="encoder-decoder-t5--bart-%E3%81%AE%E8%A8%AD%E8%A8%88">Encoder-Decoder: T5 / BART の設計</h1>
<ul>
<li><strong>T5 (Text-to-Text Transfer Transformer, Google 2019)</strong></li>
<li></li>
<li><strong>統一フォーマット</strong>: あらゆるNLPタスクをtext→textに変換</li>
<li>
<ul>
<li>翻訳: <code>&quot;translate English to French: I love you&quot;</code> → <code>&quot;Je t'aime&quot;</code></li>
</ul>
</li>
<li>
<ul>
<li>要約: <code>&quot;summarize: [長文]&quot;</code> → <code>&quot;[要約]&quot;</code></li>
</ul>
</li>
<li>
<ul>
<li>質問応答: <code>&quot;question: What is? context: ...&quot;</code> → <code>&quot;answer&quot;</code></li>
</ul>
</li>
<li></li>
<li><strong>BART (Facebook 2019)</strong>:</li>
<li>
<ul>
<li>Denoising Autoencoder: 破損テキスト → 元テキスト</li>
</ul>
</li>
<li>
<ul>
<li>Token masking, deletion, infilling, permutation</li>
</ul>
</li>
<li></li>
<li><strong>使い分けガイドライン</strong>:</li>
<li>
<ul>
<li>分類・固有表現認識: BERT/RoBERTa (Encoder-only)</li>
</ul>
</li>
<li>
<ul>
<li>テキスト生成: GPT/LLaMA (Decoder-only)</li>
</ul>
</li>
<li>
<ul>
<li>翻訳・要約・QA: T5/BART (Encoder-Decoder)</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="46" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="46" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="vision-transformer-vit--%E7%94%BB%E5%83%8F%E3%81%B8%E3%81%AEtransformer%E9%81%A9%E7%94%A8">Vision Transformer (ViT) — 画像へのTransformer適用</h1>
<ul>
<li><strong>ViT (Dosovitskiy et al. 2020, Google Brain)</strong></li>
<li></li>
<li><strong>アイデア</strong>: 画像をパッチ列として扱いTransformerに入力</li>
<li></li>
<li><strong>処理ステップ</strong>:</li>
<li>
<ol>
<li>画像を P×P のパッチに分割 (例: 16×16)</li>
</ol>
</li>
<li>
<ol start="2">
<li>各パッチを線形投影でd_model次元に変換</li>
</ol>
</li>
<li>
<ol start="3">
<li>[CLS]トークンをシーケンス先頭に追加</li>
</ol>
</li>
<li>
<ol start="4">
<li>Learned PE を加算</li>
</ol>
</li>
<li>
<ol start="5">
<li>標準的なTransformer Encoderに入力</li>
</ol>
</li>
<li>
<ol start="6">
<li>[CLS]トークンの出力で分類</li>
</ol>
</li>
<li></li>
<li><strong>スケーリング効果</strong>:</li>
<li>
<ul>
<li>CNNより大規模データで優れる (ImageNet-21K以上)</li>
</ul>
</li>
<li>
<ul>
<li>ViT-G/14: 190億パラメータ, ImageNet精度90.45%</li>
</ul>
</li>
<li></li>
<li><strong>発展</strong>: DeiT, Swin Transformer, DINOv2</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="47" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="47" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="clip--%E5%AF%BE%E7%85%A7%E5%AD%A6%E7%BF%92%E3%81%A8%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB">CLIP — 対照学習とマルチモーダル</h1>
<ul>
<li><strong>CLIP (Contrastive Language-Image Pre-training, OpenAI 2021)</strong></li>
<li></li>
<li><strong>訓練目標</strong>: (画像, テキスト)ペアの対照学習</li>
<li>
<ul>
<li>対応するペアの類似度を最大化</li>
</ul>
</li>
<li>
<ul>
<li>非対応ペアの類似度を最小化</li>
</ul>
</li>
<li>
<ul>
<li><strong>InfoNCE Loss</strong>: <code>L = -log exp(s_ii/τ) / Σ_j exp(s_ij/τ)</code></li>
</ul>
</li>
<li></li>
<li><strong>2つのエンコーダ</strong>:</li>
<li>
<ul>
<li>Image Encoder: ViT または ResNet</li>
</ul>
</li>
<li>
<ul>
<li>Text Encoder: Transformer</li>
</ul>
</li>
<li>
<ul>
<li>両者を共通埋め込み空間に写像</li>
</ul>
</li>
<li></li>
<li><strong>ゼロショット能力</strong>:</li>
<li>
<ul>
<li>「犬の写真」→ テキスト埋め込みに最も近い画像を取得</li>
</ul>
</li>
<li>
<ul>
<li>学習していないクラスでも分類可能</li>
</ul>
</li>
<li></li>
<li><strong>応用</strong>: DALL·E 2, Stable Diffusion, GPT-4V</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="48" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="48" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="mixture-of-experts-moe">Mixture of Experts (MoE)</h1>
<ul>
<li><strong>MoE — 条件付きスパース活性化</strong></li>
<li></li>
<li><strong>アイデア</strong>: FFN層を複数の「Expert」に置き換え</li>
<li>各トークンは少数のExpertのみ経由する</li>
<li></li>
<li><strong>Router / Gating Network</strong>:</li>
<li><code>G(x) = softmax(Top-k(xW_g))</code></li>
<li>
<ul>
<li>各トークンの入力 x → Router → Top-k Expert選択</li>
</ul>
</li>
<li>
<ul>
<li>典型: k=2, Expert数=8〜64</li>
</ul>
</li>
<li></li>
<li><strong>メリット</strong>:</li>
<li>
<ul>
<li>総パラメータ数 ↑ → 表現力 ↑</li>
</ul>
</li>
<li>
<ul>
<li>活性化パラメータは同程度 → 計算コスト維持</li>
</ul>
</li>
<li></li>
<li><strong>採用例</strong>:</li>
<li>
<ul>
<li>Switch Transformer (Google, 2021): 1.6T params</li>
</ul>
</li>
<li>
<ul>
<li>Mixtral 8×7B (Mistral, 2023): 8 experts, 2 active</li>
</ul>
</li>
<li>
<ul>
<li>GPT-4 (推定): MoE構成と噂</li>
</ul>
</li>
<li>
<ul>
<li>Gemini 1.5 / DeepSeek-V2</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="49" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="49" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="transformer%E5%A4%89%E7%A8%AE%E3%81%AE%E7%B3%BB%E8%AD%9C%E3%83%9E%E3%83%83%E3%83%97">Transformer変種の系譜マップ</h1>
<p><img src="../assets/model-variants.svg" alt="center" style="width:900px;" /></p>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="50" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="50" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E4%B8%BB%E8%A6%81%E3%83%A2%E3%83%87%E3%83%AB%E6%AF%94%E8%BC%83%E8%A1%A8-2024-2025">主要モデル比較表 (2024-2025)</h1>
<table>
<thead>
<tr>
<th>モデル</th>
<th>種類</th>
<th>パラメータ</th>
<th>Context</th>
<th>特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA-3 8B</td>
<td>Dec-only</td>
<td>8B</td>
<td>128K</td>
<td>Meta OSS</td>
</tr>
<tr>
<td>LLaMA-3 70B</td>
<td>Dec-only</td>
<td>70B</td>
<td>128K</td>
<td>高性能OSS</td>
</tr>
<tr>
<td>Mistral 7B</td>
<td>Dec-only</td>
<td>7B</td>
<td>32K</td>
<td>Sliding Window</td>
</tr>
<tr>
<td>Mixtral 8×7B</td>
<td>MoE</td>
<td>45B (12B active)</td>
<td>32K</td>
<td>MoE効率</td>
</tr>
<tr>
<td>Gemma 2 9B</td>
<td>Dec-only</td>
<td>9B</td>
<td>8K</td>
<td>Google軽量</td>
</tr>
<tr>
<td>DeepSeek-V3</td>
<td>MoE</td>
<td>671B</td>
<td>128K</td>
<td>高コスパ</td>
</tr>
<tr>
<td>Claude 3.5 Sonnet</td>
<td>不明</td>
<td>不明</td>
<td>200K</td>
<td>Anthropic</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>不明</td>
<td>不明</td>
<td>128K</td>
<td>マルチモーダル</td>
</tr>
<tr>
<td>Qwen2.5 72B</td>
<td>Dec-only</td>
<td>72B</td>
<td>128K</td>
<td>Alibaba</td>
</tr>
</tbody>
</table>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="51" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="51" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-8-%E3%82%B9%E3%82%B1%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E5%89%87">Chapter 8: スケーリング則</h1>
<ul>
<li>Kaplan Scaling Laws · Chinchilla最適化 ·</li>
<li>Emergent Abilities · データとパラメータの最適比率</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="52" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="52" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E3%82%B9%E3%82%B1%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E5%89%87%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96">スケーリング則の可視化</h1>
<p><img src="../assets/scaling-laws.svg" alt="center" style="width:900px;" /></p>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="53" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="53" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="kaplan-scaling-laws-2020--%E5%86%AA%E4%B9%97%E5%89%87">Kaplan Scaling Laws (2020) — 冪乗則</h1>
<ul>
<li><strong>Kaplan et al., OpenAI (2020)</strong> — Neural Language Scaling Laws</li>
<li></li>
<li><strong>Loss は N, D, C の冪乗則に従う</strong>:</li>
<li>
<ul>
<li><code>L(N) ≈ (N_c / N)^{α_N}</code>, <code>α_N ≈ 0.076</code></li>
</ul>
</li>
<li>
<ul>
<li><code>L(D) ≈ (D_c / D)^{α_D}</code>, <code>α_D ≈ 0.095</code></li>
</ul>
</li>
<li>
<ul>
<li><code>L(C) ≈ (C_c / C)^{α_C}</code>, <code>α_C ≈ 0.050</code></li>
</ul>
</li>
<li></li>
<li><strong>N</strong>: パラメータ数 / <strong>D</strong>: 訓練データ量 (tokens)</li>
<li><strong>C</strong>: 総計算量 (FLOPs) ≈ 6ND</li>
<li></li>
<li><strong>重要な示唆</strong>:</li>
<li>
<ul>
<li>計算量を10倍にするとLossは<strong>冪乗的</strong>に改善</li>
</ul>
</li>
<li>
<ul>
<li>データが固定ならパラメータを増やすより計算増が効果的</li>
</ul>
</li>
<li>→ 「大きなモデルを少ないデータで訓練」という当時の戦略</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="54" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="54" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="chinchilla%E6%9C%80%E9%81%A9%E5%8C%96-2022--%E3%83%87%E3%83%BC%E3%82%BF%E3%81%A8%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E9%BB%84%E9%87%91%E6%AF%94">Chinchilla最適化 (2022) — データとパラメータの黄金比</h1>
<ul>
<li><strong>Hoffmann et al., DeepMind (2022)</strong> — Training Compute-Optimal LLMs</li>
<li></li>
<li><strong>Kaplan則の修正</strong>:</li>
<li>
<ul>
<li>Kaplan: 同じ計算量でパラメータ数を最大化すべき</li>
</ul>
</li>
<li>
<ul>
<li>Chinchilla: <strong>パラメータとデータを同程度増やすべき</strong></li>
</ul>
</li>
<li></li>
<li><strong>Chinchilla最適則</strong>:</li>
<li><code>N_opt ∝ C^{0.49}</code>, <code>D_opt ∝ C^{0.51}</code></li>
<li>→ <strong>N : D ≈ 1 : 20</strong> が最適 (tokens)</li>
<li></li>
<li><strong>実証</strong>: Chinchilla (70B, 1.4T tokens) が</li>
<li>Gopher (280B, 300B tokens) を大幅に上回る</li>
<li></li>
<li><strong>業界への影響</strong>:</li>
<li>
<ul>
<li>LLaMA-1 (2023): 7B model × 1T tokens</li>
</ul>
</li>
<li>
<ul>
<li>LLaMA-3 (2024): 8B model × 15T tokens (Chinchilla超過)</li>
</ul>
</li>
<li>→ 推論コストを下げるため<strong>過学習気味に訓練</strong>するのが現在の標準</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="55" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="55" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="emergent-abilities--%E9%87%8F%E7%9A%84%E5%A4%89%E5%8C%96%E3%81%8C%E8%B3%AA%E7%9A%84%E5%A4%89%E5%8C%96%E3%82%92%E7%94%9F%E3%82%80">Emergent Abilities — 量的変化が質的変化を生む</h1>
<ul>
<li><strong>Wei et al. (2022)</strong> — Emergent Abilities of Large Language Models</li>
<li></li>
<li><strong>定義</strong>: 小さいモデルでは見られず、大きいモデルで突然出現する能力</li>
<li>閾値的な非線形性（冪乗則とは異なる）</li>
<li></li>
<li><strong>観察された創発例</strong>:</li>
<li>
<ul>
<li>Few-shot learning (GPT-3, ~13B以上で顕在化)</li>
</ul>
</li>
<li>
<ul>
<li>Chain-of-Thought Reasoning (PaLM, ~62B以上)</li>
</ul>
</li>
<li>
<ul>
<li>多桁算術 (~100B以上)</li>
</ul>
</li>
<li>
<ul>
<li>外国語翻訳（データがほぼない言語でも）</li>
</ul>
</li>
<li></li>
<li><strong>論争点</strong>:</li>
<li>
<ul>
<li>評価指標の閾値性による人工的アーティファクトか？</li>
</ul>
</li>
<li>
<ul>
<li>(Schaeffer et al. 2023): 連続指標では創発は見えにくい</li>
</ul>
</li>
<li></li>
<li><strong>実践上の示唆</strong>: スケールアップは予測困難な能力向上をもたらす</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="56" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="56" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E5%8A%B9%E7%8E%87%E5%8C%96%E3%81%AE%E9%87%8D%E8%A6%81%E6%80%A7">パラメータ効率化の重要性</h1>
<ul>
<li><strong>なぜパラメータ効率化が必要か</strong>:</li>
<li></li>
<li>
<ul>
<li>GPT-3 175B: 学習コスト ~$4.6M (2020年時点)</li>
</ul>
</li>
<li>
<ul>
<li>GPT-4 (推定1T): 学習コスト ~$100M以上</li>
</ul>
</li>
<li>
<ul>
<li>推論: A100 1枚でGPT-3推論は約$0.006/1K tokens</li>
</ul>
</li>
<li></li>
<li><strong>効率化の2軸</strong>:</li>
<li>
<ol>
<li><strong>学習効率化</strong>: LoRA, QLoRA, Adapter, Prefix Tuning</li>
</ol>
</li>
<li>→ 全パラメータの0.1-1%のみ更新</li>
<li>
<ol start="2">
<li><strong>推論効率化</strong>: Quantization, Flash Attention, vLLM</li>
</ol>
</li>
<li>→ スループット10x, コスト1/10</li>
<li></li>
<li><strong>モデルサイズと実用コストのトレードオフ</strong>:</li>
<li>
<ul>
<li>7B量子化 (INT4): 一般GPU (RTX 3090, 24GB) で動作</li>
</ul>
</li>
<li>
<ul>
<li>70B量子化 (INT4): A100 80GB 1枚で動作</li>
</ul>
</li>
<li>
<ul>
<li>700B+: マルチGPU / 分散推論が必要</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="57" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="57" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E3%83%A2%E3%83%87%E3%83%AB%E8%A6%8F%E6%A8%A1%E3%81%AE%E9%80%B2%E5%8C%96%E3%82%BF%E3%82%A4%E3%83%A0%E3%83%A9%E3%82%A4%E3%83%B3">モデル規模の進化タイムライン</h1>
<ul>
<li><strong>パラメータ数の指数的増加</strong> (2017〜2025):</li>
<li></li>
<li>
<ul>
<li>2017: Transformer (原論文) — 65M</li>
</ul>
</li>
<li>
<ul>
<li>2018: BERT-Large — 340M / GPT-1 — 117M</li>
</ul>
</li>
<li>
<ul>
<li>2019: GPT-2 — 1.5B / XLNet — 340M</li>
</ul>
</li>
<li>
<ul>
<li>2020: GPT-3 — <strong>175B</strong> / T5-11B — 11B</li>
</ul>
</li>
<li>
<ul>
<li>2021: Switch-C — 1.6T (MoE) / PaLM — 540B</li>
</ul>
</li>
<li>
<ul>
<li>2022: Chinchilla — 70B / BLOOM — 176B</li>
</ul>
</li>
<li>
<ul>
<li>2023: LLaMA-1 — 65B / GPT-4 — 不明 / LLaMA-2 — 70B</li>
</ul>
</li>
<li>
<ul>
<li>2024: LLaMA-3 — 405B / Gemini 1.5 Ultra / DeepSeek-V3 — 671B</li>
</ul>
</li>
<li>
<ul>
<li>2025: Gemini 2.0 Ultra / Claude 3.5 / GPT-5 (予定)</li>
</ul>
</li>
<li></li>
<li><strong>18ヶ月で約10倍</strong> (Moore's Law超え)</li>
<li><strong>ただし</strong>: Chinchilla以降は「小モデル × 大データ」がトレンド</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="58" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="58" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-9-%E5%8A%B9%E7%8E%87%E5%8C%96%E6%8A%80%E8%A1%93">Chapter 9: 効率化技術</h1>
<ul>
<li>Flash Attention · KV Cache · Quantization ·</li>
<li>LoRA / QLoRA · Speculative Decoding · vLLM</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="59" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="59" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="flash-attention--io-aware%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0">Flash Attention — IO-Awareアルゴリズム</h1>
<p><img src="../assets/flash-attention.svg" alt="center" style="width:900px;" /></p>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="60" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="60" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="flash-attention-%E3%81%AE%E6%95%B0%E5%BC%8F--online-softmax">Flash Attention の数式 — Online Softmax</h1>
<ul>
<li><strong>問題</strong>: 標準的Attentionは O(n²) メモリが必要</li>
<li>→ n=4096で 16GB VRAM (FP16)</li>
<li></li>
<li><strong>Flash Attention (Dao et al. 2022) の鍵</strong>: タイル処理 + Online Softmax</li>
<li></li>
<li><strong>Online Softmax (数値安定版)</strong>:</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li>
<h1 id="%E3%83%96%E3%83%AD%E3%83%83%E3%82%AFi%E3%81%AE%E5%87%A6%E7%90%86">ブロックiの処理:</h1>
</li>
<li>m_new = max(m_old, rowmax(S_i))   # 最大値更新</li>
<li>l_new = e^(m_old-m_new) * l_old + rowsum(e^(S_i - m_new))</li>
<li>O_new = (l_old * e^(m_old-m_new) * O_old + e^(S_i-m_new) * V_i) / l_new</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li></li>
<li><strong>効果</strong>:</li>
<li>
<ul>
<li>HBMアクセス: O(n²) → O(n) (タイル処理で中間結果をSRAMに保持)</li>
</ul>
</li>
<li>
<ul>
<li>メモリ: O(n²) → O(n)</li>
</ul>
</li>
<li>
<ul>
<li>速度: A100で約3倍高速</li>
</ul>
</li>
<li>
<ul>
<li>Flash Attention 2 (2023): さらに2倍 / FA3 (2024): Hopper最適化</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="61" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="61" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="kv-cache--%E6%8E%A8%E8%AB%96%E9%AB%98%E9%80%9F%E5%8C%96%E3%81%AE%E5%9F%BA%E6%9C%AC">KV Cache — 推論高速化の基本</h1>
<ul>
<li><strong>問題</strong>: 自回帰生成で毎トークン全過去KVを再計算</li>
<li></li>
<li><strong>KV Cache</strong>: K, V を計算済みトークン分キャッシュ</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"></code></pre>
</li>
<li>
<h1 id="%E7%96%91%E4%BC%BC%E3%82%B3%E3%83%BC%E3%83%89-kv-cache%E4%BB%98%E3%81%8D%E6%8E%A8%E8%AB%96">疑似コード: KV Cache付き推論</h1>
</li>
<li>past_key_values = None</li>
<li>for step in range(max_new_tokens):</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>outputs = model(input_ids=new_token,
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>                past_key_values=past_key_values,
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>                use_cache=True)
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>new_token = outputs.logits.argmax(-1)
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>past_key_values = outputs.past_key_values
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li></li>
<li><strong>メモリコスト</strong>: K, V それぞれ <code>[B, h, n, d_k]</code></li>
<li>
<ul>
<li>LLaMA-7B (32層, h=32, d_k=128): 長さ2K で ~512MB</li>
</ul>
</li>
<li>
<ul>
<li><strong>Grouped Query Attention (GQA)</strong>: KVヘッド数を削減 (LLaMA-3採用)</li>
</ul>
</li>
<li>
<ul>
<li>Multi-Head: h KV ヘッド / GQA: h/g KV ヘッド</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="62" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="62" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="quantization--%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E5%B0%8F%E3%81%95%E3%81%8F%E3%83%BB%E9%80%9F%E3%81%8F">Quantization — モデルを小さく・速く</h1>
<ul>
<li><strong>量子化</strong>: 浮動小数点数を低精度整数に変換</li>
<li></li>
<li><strong>精度と表現範囲</strong>:<br />
| 形式 | ビット数 | メモリ (7B) | VRAM必要量 |<br />
|------|---------|-----------|----------|<br />
| FP32 | 32 bit | 28 GB | 56GB+ |<br />
| BF16/FP16 | 16 bit | 14 GB | 28GB+ |<br />
| INT8 | 8 bit | 7 GB | 14GB+ |<br />
| INT4/NF4 | 4 bit | 3.5 GB | 8GB+ |</li>
<li></li>
<li><strong>均一量子化の数式</strong>:</li>
<li><code>x_q = round(x / s) + z</code></li>
<li><code>x_dequant = (x_q - z) × s</code></li>
<li>
<ul>
<li><code>s</code> (scale): <code>(x_max - x_min) / (2^b - 1)</code></li>
</ul>
</li>
<li>
<ul>
<li><code>z</code> (zero-point): オフセット</li>
</ul>
</li>
<li></li>
<li><strong>精度損失</strong>: INT4は概ねBF16の性能の90-95%程度を維持</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="63" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="63" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="gptq--awq--gguf--%E9%87%8F%E5%AD%90%E5%8C%96%E3%81%AE%E5%AE%9F%E8%B7%B5">GPTQ / AWQ / GGUF — 量子化の実践</h1>
<ul>
<li><strong>GPTQ (2022)</strong> — Post-Training Quantization</li>
<li>
<ul>
<li>Hessian行列を使った最適な量子化ポイント探索</li>
</ul>
</li>
<li>
<ul>
<li>重みを層ごとに逐次量子化 (1ショット)</li>
</ul>
</li>
<li>
<ul>
<li><code>W_q = argmin ||WX - W_q X||²_F</code></li>
</ul>
</li>
<li></li>
<li><strong>AWQ (2023)</strong> — Activation-aware Weight Quantization</li>
<li>
<ul>
<li>重要な重み (活性化値の大きいチャンネル) を保護</li>
</ul>
</li>
<li>
<ul>
<li>スムージング変換: <code>W' = W / s</code>, <code>X' = X × s</code></li>
</ul>
</li>
<li>
<ul>
<li>GPTQより精度が高いケースが多い</li>
</ul>
</li>
<li></li>
<li><strong>GGUF / llama.cpp</strong> — CPU推論向け</li>
<li>
<ul>
<li>Q2_K, Q4_K_M, Q5_K_M など多彩な量子化レベル</li>
</ul>
</li>
<li>
<ul>
<li>Apple Silicon (Metal), CUDA, Vulkanに対応</li>
</ul>
</li>
<li>
<ul>
<li>Ollama / LM Studio が内部で使用</li>
</ul>
</li>
<li></li>
<li><strong>実用選択</strong>: 品質優先→AWQ / CPU推論→GGUF / GPU推論→GPTQ</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="64" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="64" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="lora-%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3">LoRA アーキテクチャ</h1>
<p><img src="../assets/lora-architecture.svg" alt="center" style="width:900px;" /></p>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="65" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="65" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="lora--%E4%BD%8E%E3%83%A9%E3%83%B3%E3%82%AF%E8%BF%91%E4%BC%BC%E3%81%AE%E6%95%B0%E5%AD%A6">LoRA — 低ランク近似の数学</h1>
<ul>
<li><strong>LoRA (Hu et al. 2021)</strong> — Low-Rank Adaptation</li>
<li></li>
<li><strong>直感</strong>: 事前訓練モデルの重み変化 ΔW は低ランク行列</li>
<li>→ <code>ΔW ≈ BA</code> (B: d×r, A: r×k, rank r &lt;&lt; min(d,k))</li>
<li></li>
<li><strong>順伝播</strong>:</li>
<li><code>h = W₀x + ΔWx = W₀x + BAx</code></li>
<li>
<ul>
<li><code>W₀</code>: 凍結 (gradient不要)</li>
</ul>
</li>
<li>
<ul>
<li><code>B, A</code>: 学習対象 (ランダム初期化: A~N(0,σ²), B=0)</li>
</ul>
</li>
<li></li>
<li><strong>スケーリング</strong>: <code>ΔW = (α/r) × BA</code></li>
<li>
<ul>
<li><code>α</code>: ハイパーパラメータ (典型: α=r または α=2r)</li>
</ul>
</li>
<li></li>
<li><strong>パラメータ削減比</strong> (d=k=4096, r=8):</li>
<li>
<ul>
<li>Full FT: 4096 × 4096 = 16.7M</li>
</ul>
</li>
<li>
<ul>
<li>LoRA: (4096+4096) × 8 = 65K → <strong>99.6%削減</strong></li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="66" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="66" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="qlora--4-bit%E9%87%8F%E5%AD%90%E5%8C%96--lora">QLoRA — 4-bit量子化 + LoRA</h1>
<ul>
<li><strong>QLoRA (Dettmers et al. 2023)</strong> — Efficient Fine-tuning</li>
<li></li>
<li><strong>3つの革新</strong>:</li>
<li>
<ol>
<li><strong>NF4 (NormalFloat4) 量子化</strong>:</li>
</ol>
</li>
<li>
<ul>
<li>正規分布に最適化された4-bit量子化</li>
</ul>
</li>
<li>
<ul>
<li><code>NF4 = {-1, -0.69, ..., 0, ..., 0.69, 1}</code> (16値)</li>
</ul>
</li>
<li>
<ol start="2">
<li><strong>Double Quantization (DQ)</strong>:</li>
</ol>
</li>
<li>
<ul>
<li>量子化定数自体をさらに量子化 → 0.37bit/param節約</li>
</ul>
</li>
<li>
<ol start="3">
<li><strong>Paged Optimizers</strong>:</li>
</ol>
</li>
<li>
<ul>
<li>CUDA Unified Memory でGPUからCPUへオフロード</li>
</ul>
</li>
<li></li>
<li><strong>実用例</strong>:</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-bash"></code></pre>
</li>
<li>
<h1 id="qlora-%E3%81%A7-llama-3-8b-%E3%82%92%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0">QLoRA で LLaMA-3 8B をファインチューニング</h1>
</li>
<li>
<h1 id="%E5%BF%85%E8%A6%81vram-8gb-rtx-3080--rtx-4070%E3%81%A7%E5%8B%95%E4%BD%9C">必要VRAM: ~8GB (RTX 3080 / RTX 4070で動作)</h1>
</li>
<li>python train.py --model meta-llama/Meta-Llama-3-8B \</li>
<li>--load_in_4bit True --use_lora True \</li>
<li>--lora_r 16 --lora_alpha 32</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>

</code></pre>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="67" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="67" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="speculative-decoding--%E6%8A%95%E6%A9%9F%E7%9A%84%E3%83%87%E3%82%B3%E3%83%BC%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0">Speculative Decoding — 投機的デコーディング</h1>
<ul>
<li><strong>問題</strong>: LLMの自回帰生成はメモリバウンド (1トークン/フォワードパス)</li>
<li></li>
<li><strong>Speculative Decoding (Leviathan et al. 2022)</strong>:</li>
<li>
<ul>
<li>小さいドラフトモデル (target の1/10程度) で先読み</li>
</ul>
</li>
<li>
<ul>
<li>大きいモデルで並列検証・修正</li>
</ul>
</li>
<li></li>
<li><strong>アルゴリズム</strong>:</li>
<li>
<ol>
<li>ドラフトモデルで <code>γ</code> 個のトークンを高速生成</li>
</ol>
</li>
<li>
<ol start="2">
<li>ターゲットモデルで <code>γ</code> トークンを<strong>一括</strong>フォワードパス</li>
</ol>
</li>
<li>
<ol start="3">
<li>各トークンを確率比で受容/拒絶</li>
</ol>
</li>
<li>
<ul>
<li><code>accept if q(x)/p(x) ≥ U(0,1)</code> — 分布は同一を保証</li>
</ul>
</li>
<li>
<ol start="4">
<li>受容された分だけ進み繰り返し</li>
</ol>
</li>
<li></li>
<li><strong>効果</strong>:</li>
<li>
<ul>
<li>出力分布は元モデルと同一 (理論保証)</li>
</ul>
</li>
<li>
<ul>
<li>2〜3倍のスループット向上</li>
</ul>
</li>
<li>
<ul>
<li>Gemini, Claude, GPT-4 で採用 (推定)</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="68" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="68" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="continuous-batching-%E3%81%A8-pagedattention">Continuous Batching と PagedAttention</h1>
<ul>
<li><strong>Continuous Batching (Yu et al. 2022)</strong>:</li>
<li>
<ul>
<li>従来: リクエストを固定バッチで処理 → 早く終わったリクエストがGPUを無駄使い</li>
</ul>
</li>
<li>
<ul>
<li>Continuous: 完了したリクエストをすぐ新リクエストで置換</li>
</ul>
</li>
<li>
<ul>
<li>スループット: 最大8〜16倍向上</li>
</ul>
</li>
<li></li>
<li><strong>PagedAttention (vLLM, Kwon et al. 2023)</strong>:</li>
<li>
<ul>
<li>KV Cacheをページ (小固定ブロック) に分割</li>
</ul>
</li>
<li>
<ul>
<li>OSのページングメモリ管理と同じ概念</li>
</ul>
</li>
<li>
<ul>
<li>非連続なGPUメモリを効率利用</li>
</ul>
</li>
<li>
<ul>
<li>メモリ断片化を1%未満に</li>
</ul>
</li>
<li></li>
<li><strong>vLLM の効果</strong>:</li>
<li>
<ul>
<li>HuggingFace単純実装比: スループット <strong>3〜24倍</strong></li>
</ul>
</li>
<li>
<ul>
<li>企業向けLLM推論サービングのデファクトスタンダード</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="69" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="69" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E5%8A%B9%E7%8E%87%E5%8C%96%E6%8A%80%E8%A1%93-%E3%82%B5%E3%83%9E%E3%83%AA%E3%83%BC%E3%83%9E%E3%83%83%E3%83%97">効率化技術 サマリーマップ</h1>
<table>
<thead>
<tr>
<th>技術</th>
<th>適用フェーズ</th>
<th>効果</th>
<th>コスト削減</th>
</tr>
</thead>
<tbody>
<tr>
<td>Flash Attention</td>
<td>訓練+推論</td>
<td>速度3倍</td>
<td>メモリO(n²)→O(n)</td>
</tr>
<tr>
<td>KV Cache</td>
<td>推論</td>
<td>レイテンシ大幅減</td>
<td>計算量O(n)→O(1)/step</td>
</tr>
<tr>
<td>GQA</td>
<td>訓練+推論</td>
<td>KVメモリ1/4</td>
<td>スループット向上</td>
</tr>
<tr>
<td>INT4 Quant</td>
<td>推論</td>
<td>VRAM 1/4</td>
<td>コスト75%削減</td>
</tr>
<tr>
<td>LoRA</td>
<td>訓練</td>
<td>学習パラメータ99%減</td>
<td>訓練コスト10x削減</td>
</tr>
<tr>
<td>QLoRA</td>
<td>訓練</td>
<td>VRAM 1/4 + LoRA</td>
<td>個人GPUでFT可能</td>
</tr>
<tr>
<td>Speculative</td>
<td>推論</td>
<td>スループット2-3倍</td>
<td>レイテンシ改善</td>
</tr>
<tr>
<td>PagedAttention</td>
<td>推論</td>
<td>スループット24x</td>
<td>サーバー台数削減</td>
</tr>
</tbody>
</table>
<ul>
<li></li>
<li><strong>組み合わせ例 (推論)</strong>:</li>
<li>INT4 + Flash Attention 2 + PagedAttention = vLLM本番環境</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="70" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="70" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-10-fine-tuning--alignment">Chapter 10: Fine-tuning &amp; Alignment</h1>
<ul>
<li>Instruction Tuning · RLHF · DPO ·</li>
<li>LoRA実践 · アライメント技術の比較</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="71" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="71" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="pre-training-%E2%86%92-fine-tuning-%E2%86%92-rlhf-%E3%81%AE%E6%B5%81%E3%82%8C">Pre-training → Fine-tuning → RLHF の流れ</h1>
<ul>
<li><strong>Stage 1: Pre-training</strong></li>
<li>
<ul>
<li>数兆トークンの生テキストで次トークン予測</li>
</ul>
</li>
<li>
<ul>
<li>世界知識・言語パターン・推論能力を獲得</li>
</ul>
</li>
<li>
<ul>
<li>コスト: $数百万〜数千万</li>
</ul>
</li>
<li></li>
<li><strong>Stage 2: Supervised Fine-Tuning (SFT)</strong></li>
<li>
<ul>
<li>指示-応答ペアで微調整 (数十万〜数百万サンプル)</li>
</ul>
</li>
<li>
<ul>
<li><code>L_SFT = -Σ_t log P(y_t | x, y_{&lt;t})</code></li>
</ul>
</li>
<li>
<ul>
<li>コスト: $数千〜数万</li>
</ul>
</li>
<li></li>
<li><strong>Stage 3: RLHF (任意)</strong></li>
<li>
<ul>
<li>人間のフィードバックで Helpful/Harmless/Honest に</li>
</ul>
</li>
<li>
<ul>
<li>コスト: $数万〜数百万 (人間アノテーション高い)</li>
</ul>
</li>
<li></li>
<li><strong>現実的な使い方 (LoRA)</strong>:</li>
<li>
<ul>
<li>SFTをLoRAで実行 → コスト100x削減 → 実験迅速化</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="72" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="72" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="instruction-tuning--%E6%8C%87%E7%A4%BA%E3%81%AB%E5%BE%93%E3%81%86%E3%83%A2%E3%83%87%E3%83%AB%E3%81%B8">Instruction Tuning — 指示に従うモデルへ</h1>
<ul>
<li><strong>Instruction Tuning</strong>: タスク指示に従う能力を獲得</li>
<li></li>
<li><strong>データ形式例</strong>:</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li>{</li>
<li>&quot;instruction&quot;: &quot;以下の文章を英語に翻訳してください&quot;,</li>
<li>&quot;input&quot;: &quot;今日は晴れです&quot;,</li>
<li>&quot;output&quot;: &quot;It is sunny today&quot;</li>
<li>}</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li></li>
<li><strong>主要データセット</strong>:</li>
<li>
<ul>
<li>FLAN (Google, 2021): 62タスク, 1.8K templates</li>
</ul>
</li>
<li>
<ul>
<li>Alpaca (Stanford, 2023): GPT-3.5で生成した52K指示ペア</li>
</ul>
</li>
<li>
<ul>
<li>OpenOrca (2023): ChatGPT/GPT-4で生成した100万+</li>
</ul>
</li>
<li></li>
<li><strong>Self-Instruct (Wang et al. 2022)</strong>:</li>
<li>
<ul>
<li>LLM自身で指示データを生成 → コスト大幅削減</li>
</ul>
</li>
<li>
<ul>
<li>品質は人手データより低いが量で補う</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="73" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="73" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="rlhf-%E3%81%A8-dpo-%E3%81%AE%E6%AF%94%E8%BC%83">RLHF と DPO の比較</h1>
<p><img src="../assets/rlhf-dpo.svg" alt="center" style="width:900px;" /></p>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="74" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="74" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="rlhf--%E6%95%B0%E5%BC%8F%E3%81%A8%E8%AA%B2%E9%A1%8C">RLHF — 数式と課題</h1>
<ul>
<li><strong>RLHF (Christiano et al. 2017, OpenAI InstructGPT 2022)</strong></li>
<li></li>
<li><strong>Step 1 - Reward Model訓練</strong>:</li>
<li><code>L_RM = -E[(r_θ(x, y_w) - r_θ(x, y_l))]</code></li>
<li>= Bradley-Terry model (y_w: 好まれる, y_l: 好まれない)</li>
<li></li>
<li><strong>Step 2 - PPO (Proximal Policy Optimization)</strong>:</li>
<li><code>L_PPO = E[r_θ(x,y)] - β × KL(π_θ || π_ref)</code></li>
<li>
<ul>
<li>KL項: 元モデルから離れすぎないよう制約</li>
</ul>
</li>
<li>
<ul>
<li>β: KLペナルティの強さ</li>
</ul>
</li>
<li></li>
<li><strong>RLHFの課題</strong>:</li>
<li>
<ul>
<li>報酬モデルのオーバーフィッティング (reward hacking)</li>
</ul>
</li>
<li>
<ul>
<li>PPOの不安定性・ハイパーパラメータ敏感性</li>
</ul>
</li>
<li>
<ul>
<li>3段階のパイプラインが複雑</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="75" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="75" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="dpo--%E7%9B%B4%E6%8E%A5%E9%81%B8%E5%A5%BD%E6%9C%80%E9%81%A9%E5%8C%96%E3%81%AE%E6%95%B0%E5%BC%8F">DPO — 直接選好最適化の数式</h1>
<ul>
<li><strong>DPO (Rafailov et al. 2023)</strong> — Direct Preference Optimization</li>
<li></li>
<li><strong>鍵となる洞察</strong>: RLHFの最適解は閉形式で書ける:</li>
<li><code>π* = π_ref × exp(r(x,y)/β) / Z(x)</code></li>
<li></li>
<li>→ 報酬関数を方策で表現:</li>
<li><code>r*(x,y) = β log(π*(y|x)/π_ref(y|x)) + β log Z(x)</code></li>
<li></li>
<li><strong>DPO損失関数</strong>:</li>
<li><code>L_DPO = -E[log σ(β log(π_θ(y_w|x)/π_ref(y_w|x)) - β log(π_θ(y_l|x)/π_ref(y_l|x)))]</code></li>
<li></li>
<li><strong>利点</strong>:</li>
<li>
<ul>
<li>報酬モデル不要 → 2段階 (SFT + DPO)</li>
</ul>
</li>
<li>
<ul>
<li>PPOより安定 / 実装シンプル</li>
</ul>
</li>
<li>
<ul>
<li>速度: PPO比 5〜10倍高速</li>
</ul>
</li>
<li></li>
<li><strong>採用</strong>: Zephyr, Llama-3-Instruct, Mistral-Instruct</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="76" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="76" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="lora-fine-tuning-%E5%AE%9F%E8%B7%B5%E3%82%B3%E3%83%BC%E3%83%89">LoRA Fine-tuning 実践コード</h1>
<ul>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"></code></pre>
</li>
<li>from transformers import AutoModelForCausalLM, BitsAndBytesConfig</li>
<li>from peft import LoraConfig, get_peft_model</li>
<li></li>
<li>
<h1 id="4-bit%E9%87%8F%E5%AD%90%E5%8C%96%E3%81%A7%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E3%83%AD%E3%83%BC%E3%83%89">4-bit量子化でモデルをロード</h1>
</li>
<li>bnb_config = BitsAndBytesConfig(load_in_4bit=True,</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.bfloat16)
</code></pre>
</li>
<li>model = AutoModelForCausalLM.from_pretrained(</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>'meta-llama/Meta-Llama-3-8B', quantization_config=bnb_config)
</code></pre>
</li>
<li></li>
<li>
<h1 id="lora%E8%A8%AD%E5%AE%9A">LoRA設定</h1>
</li>
<li>lora_config = LoraConfig(r=16, lora_alpha=32,</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>target_modules=['q_proj','v_proj'],  # 注意層のQ/Vのみ
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>lora_dropout=0.05, bias='none', task_type='CAUSAL_LM')
</code></pre>
</li>
<li>model = get_peft_model(model, lora_config)</li>
<li>model.print_trainable_parameters()</li>
<li>
<h1 id="trainable-params-3407872--total-8033669120-0042">trainable params: 3,407,872 || total: 8,033,669,120 (0.042%)</h1>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>

</code></pre>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="77" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="77" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E3%83%87%E3%83%BC%E3%82%BF%E5%93%81%E8%B3%AA-vs-%E3%83%87%E3%83%BC%E3%82%BF%E9%87%8F">データ品質 vs データ量</h1>
<ul>
<li><strong>LIMA (Zhou et al. 2023)</strong>: &quot;Less Is More for Alignment&quot;</li>
<li>
<ul>
<li>1,000件の高品質指示データのみでSFT</li>
</ul>
</li>
<li>
<ul>
<li>ChatGPT / DaVinci-003 と競争的な性能</li>
</ul>
</li>
<li>→ <strong>「データ品質 &gt; データ量」</strong> という示唆</li>
<li></li>
<li><strong>ファインチューニングのデータガイドライン</strong>:<br />
| タスク | 最低データ量 | 推奨 | 注意 |<br />
|-------|------------|------|------|<br />
| 分類 | 100 / クラス | 1K+ | バランスを保つ |<br />
| 指示チューニング | 1,000 | 10K-100K | 多様性が重要 |<br />
| コード生成 | 5,000 | 50K+ | エラーなしが必須 |<br />
| RLHF/DPO | 10,000ペア | 100K+ | 比較の一貫性 |</li>
<li></li>
<li><strong>データクリーニングの重要性</strong>:</li>
<li>
<ul>
<li>重複除去 (MinHash等) で訓練の無駄を削減</li>
</ul>
</li>
<li>
<ul>
<li>有害コンテンツのフィルタリング</li>
</ul>
</li>
<li>
<ul>
<li>フォーマット統一 (指示/応答テンプレート)</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="78" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="78" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="sft--rlhf--dpo-%E6%AF%94%E8%BC%83%E3%82%B5%E3%83%9E%E3%83%AA%E3%83%BC">SFT / RLHF / DPO 比較サマリー</h1>
<table>
<thead>
<tr>
<th></th>
<th>SFT</th>
<th>RLHF</th>
<th>DPO</th>
</tr>
</thead>
<tbody>
<tr>
<td>学習ステージ</td>
<td>1段階</td>
<td>3段階</td>
<td>2段階</td>
</tr>
<tr>
<td>必要データ</td>
<td>指示-応答ペア</td>
<td>ランキングペア</td>
<td>選好ペア (y_w, y_l)</td>
</tr>
<tr>
<td>報酬モデル</td>
<td>不要</td>
<td>必要</td>
<td>不要</td>
</tr>
<tr>
<td>安定性</td>
<td>高</td>
<td>低 (PPO)</td>
<td>高</td>
</tr>
<tr>
<td>計算コスト</td>
<td>低</td>
<td>高</td>
<td>中</td>
</tr>
<tr>
<td>アライメント強度</td>
<td>低</td>
<td>高</td>
<td>中〜高</td>
</tr>
<tr>
<td>採用例</td>
<td>LLaMA-1</td>
<td>InstructGPT</td>
<td>Llama-3-Instruct</td>
</tr>
</tbody>
</table>
<ul>
<li></li>
<li><strong>実践的な推奨</strong>:</li>
<li>
<ul>
<li>ドメイン適応: SFT + LoRA</li>
</ul>
</li>
<li>
<ul>
<li>安全性・有用性向上: DPO (RLHFの実用的代替)</li>
</ul>
</li>
<li>
<ul>
<li>最高性能: RLHF (コストを許容できる場合)</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="79" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="79" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-11-%E5%AE%9F%E8%A3%85%E3%83%BB%E3%83%87%E3%83%97%E3%83%AD%E3%82%A4%E5%AE%9F%E8%B7%B5">Chapter 11: 実装・デプロイ実践</h1>
<ul>
<li>HuggingFace Ecosystem · vLLM · GPU選定 ·</li>
<li>コスト最適化 · Serving Architecture</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="80" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="80" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="huggingface-%E3%82%A8%E3%82%B3%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0">HuggingFace エコシステム</h1>
<ul>
<li><strong>HuggingFace</strong> — LLMエコシステムの中心</li>
<li></li>
<li><strong>主要ライブラリ</strong>:</li>
<li>
<ul>
<li><code>transformers</code>: モデルロード・推論・Fine-tuning</li>
</ul>
</li>
<li>
<ul>
<li><code>peft</code>: LoRA/Adapter等のPEFT手法</li>
</ul>
</li>
<li>
<ul>
<li><code>accelerate</code>: マルチGPU / 分散訓練の抽象化</li>
</ul>
</li>
<li>
<ul>
<li><code>datasets</code>: データセットの効率的ロード</li>
</ul>
</li>
<li>
<ul>
<li><code>trl</code>: RLHF / DPO訓練フレームワーク</li>
</ul>
</li>
<li>
<ul>
<li><code>optimum</code>: ONNX / TensorRT最適化</li>
</ul>
</li>
<li></li>
<li><strong>Hub</strong>: 500,000+のモデル / 80,000+のデータセット</li>
<li></li>
<li><strong>基本的なPipeline API</strong>:</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"></code></pre>
</li>
<li>from transformers import pipeline</li>
<li>generator = pipeline('text-generation',</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>model='meta-llama/Meta-Llama-3-8B-Instruct',
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>device_map='auto', torch_dtype=torch.bfloat16)
</code></pre>
</li>
<li>result = generator('日本の首都は？', max_new_tokens=100)</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>

</code></pre>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="81" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="81" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="vllm--%E6%9C%AC%E7%95%AA%E6%8E%A8%E8%AB%96%E3%82%B5%E3%83%BC%E3%83%93%E3%83%B3%E3%82%B0">vLLM — 本番推論サービング</h1>
<ul>
<li><strong>vLLM</strong> (UC Berkeley, 2023) — LLM Serving最速フレームワーク</li>
<li></li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"></code></pre>
</li>
<li>from vllm import LLM, SamplingParams</li>
<li></li>
<li>
<h1 id="%E3%83%A2%E3%83%87%E3%83%AB%E3%83%AD%E3%83%BC%E3%83%89-%E8%87%AA%E5%8B%95%E7%9A%84%E3%81%ABpagedattention%E9%81%A9%E7%94%A8">モデルロード (自動的にPagedAttention適用)</h1>
</li>
<li>llm = LLM(model='meta-llama/Meta-Llama-3-8B-Instruct',</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>      tensor_parallel_size=1,   # GPU数
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>      gpu_memory_utilization=0.90)
</code></pre>
</li>
<li></li>
<li>params = SamplingParams(temperature=0.8, top_p=0.95,</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>                    max_tokens=512)
</code></pre>
</li>
<li>prompts = ['Transformerとは何ですか？', '量子化の利点は？']</li>
<li></li>
<li>
<h1 id="%E3%83%90%E3%83%83%E3%83%81%E6%8E%A8%E8%AB%96-continuous-batching%E3%81%8C%E8%87%AA%E5%8B%95%E9%81%A9%E7%94%A8">バッチ推論 (Continuous Batchingが自動適用)</h1>
</li>
<li>outputs = llm.generate(prompts, params)</li>
<li>for output in outputs:</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>print(output.outputs[0].text)
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li></li>
<li><strong>OpenAI互換API</strong>: <code>python -m vllm.entrypoints.openai.api_server</code></li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="82" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="82" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="gpu--hardware-%E9%81%B8%E5%AE%9A%E3%82%AC%E3%82%A4%E3%83%89">GPU / Hardware 選定ガイド</h1>
<table>
<thead>
<tr>
<th>GPU</th>
<th>VRAM</th>
<th>実行可能モデル</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>RTX 4090</td>
<td>24GB</td>
<td>7B (BF16) / 30B (INT4)</td>
<td>個人・研究</td>
</tr>
<tr>
<td>A100 40GB</td>
<td>40GB</td>
<td>30B (BF16) / 70B (INT4)</td>
<td>中規模推論</td>
</tr>
<tr>
<td>A100 80GB</td>
<td>80GB</td>
<td>70B (BF16) / 180B (INT4)</td>
<td>本番推論</td>
</tr>
<tr>
<td>H100 80GB</td>
<td>80GB (HBM3)</td>
<td>70B (BF16) 高速</td>
<td>訓練・本番</td>
</tr>
<tr>
<td>H200 141GB</td>
<td>141GB</td>
<td>405B (BF16) 単体</td>
<td>大規模推論</td>
</tr>
</tbody>
</table>
<ul>
<li></li>
<li><strong>テンソル並列化</strong> (Tensor Parallelism):</li>
<li>
<ul>
<li>Attention headをGPU間で分割</li>
</ul>
</li>
<li>
<ul>
<li>4×A100でLLaMA-3 70Bをリアルタイム推論</li>
</ul>
</li>
<li></li>
<li><strong>クラウドコスト概算 (2025年)</strong>:</li>
<li>
<ul>
<li>A100 80GB: $2-3/時 (スポット ~$1)</li>
</ul>
</li>
<li>
<ul>
<li>H100: $5-8/時</li>
</ul>
</li>
<li>
<ul>
<li>個人: RunPod, Lambda Labs, Vast.aiが安価</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="83" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="83" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E3%82%B3%E3%82%B9%E3%83%88%E6%9C%80%E9%81%A9%E5%8C%96%E6%88%A6%E7%95%A5">コスト最適化戦略</h1>
<ul>
<li><strong>推論コストの計算式</strong>:</li>
<li>コスト ≈ (GPU時間) × (GPU単価) / (スループット)</li>
<li></li>
<li><strong>最適化の優先順位</strong>:</li>
<li>
<ol>
<li><strong>量子化</strong> (INT4): VRAM 1/4 → 同一GPUで4倍のバッチ</li>
</ol>
</li>
<li>
<ol start="2">
<li><strong>Flash Attention 2</strong>: スループット +50-100%</li>
</ol>
</li>
<li>
<ol start="3">
<li><strong>Continuous Batching</strong>: GPUアイドル削減</li>
</ol>
</li>
<li>
<ol start="4">
<li><strong>Speculative Decoding</strong>: レイテンシ改善</li>
</ol>
</li>
<li>
<ol start="5">
<li><strong>KV Cache Reuse (Prefix Caching)</strong>: 同一プレフィックス再利用</li>
</ol>
</li>
<li></li>
<li><strong>API vs セルフホスト損益分岐点</strong>:</li>
<li>
<ul>
<li>GPT-4o: $5/1M input, $15/1M output tokens</li>
</ul>
</li>
<li>
<ul>
<li>LLaMA-3 70B (A100): ~$0.5/1M tokens</li>
</ul>
</li>
<li>→ 月10M tokens以上でセルフホストが有利</li>
<li></li>
<li><strong>ハイブリッド戦略</strong>: 複雑タスク→GPT-4o / 単純タスク→7B OSS</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="84" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="84" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="serving-architecture--%E6%9C%AC%E7%95%AA%E8%A8%AD%E8%A8%88">Serving Architecture — 本番設計</h1>
<ul>
<li><strong>本番LLMサービングの標準構成</strong>:</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li>Client → Load Balancer</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>   → Rate Limiter (Upstash Redis)
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>   → Prompt Guardrails (入力検証)
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>   → [vLLM Cluster]
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>        GPU Pod 1: LLaMA-3 70B (INT4)
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>        GPU Pod 2: LLaMA-3 70B (INT4)
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>   → Response Caching (Semantic Cache)
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>   → Output Filter (安全性フィルタ)
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code>   → Client
</code></pre>
</li>
<li>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code></code></pre>
</li>
<li></li>
<li><strong>キーポイント</strong>:</li>
<li>
<ul>
<li>Streaming (SSE) でTTFTを最小化</li>
</ul>
</li>
<li>
<ul>
<li>Prefix Caching でシステムプロンプトを再利用</li>
</ul>
</li>
<li>
<ul>
<li>Model Routing: タスクに応じてモデル切り替え</li>
</ul>
</li>
<li>
<ul>
<li>Autoscaling: GPU Pod数を需要に応じてスケール</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="85" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="85" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-12-%E6%9C%80%E6%96%B0%E7%A0%94%E7%A9%B6%E5%8B%95%E5%90%91">Chapter 12: 最新研究動向</h1>
<ul>
<li>Long Context · Mamba/SSM · Reasoning ·</li>
<li>Multimodal · Hallucination · Safety</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="86" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="86" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="long-context--100k-%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E3%81%B8%E3%81%AE%E6%8C%91%E6%88%A6">Long Context — 100K+ トークンへの挑戦</h1>
<ul>
<li><strong>現状 (2025年)</strong>:</li>
<li>
<ul>
<li>GPT-4 Turbo: 128K / Claude 3: 200K / Gemini 1.5: <strong>1M</strong></li>
</ul>
</li>
<li></li>
<li><strong>技術的アプローチ</strong>:</li>
<li>
<ul>
<li><strong>RoPE外挿</strong>: YaRN, LongRoPE (学習なしで8Kモデルを128Kへ)</li>
</ul>
</li>
<li>
<ul>
<li><strong>Sliding Window Attention</strong>: ウィンドウ外は参照しない (Mistral)</li>
</ul>
</li>
<li>
<ul>
<li><strong>Flash Attention 2/3</strong>: O(n²)を実用的な範囲で</li>
</ul>
</li>
<li>
<ul>
<li><strong>Ring Attention</strong>: 超長系列をGPU間で分散 (1Mトークン+)</li>
</ul>
</li>
<li></li>
<li><strong>Long Context の「迷子問題」</strong>:</li>
<li>
<ul>
<li>Needle-in-a-Haystack Test: 長い文書中の事実を正確に引ける?</li>
</ul>
</li>
<li>
<ul>
<li>中間部の情報は先頭・末尾より想起されにくい傾向</li>
</ul>
</li>
<li>
<ul>
<li>実用的には: RAGと組み合わせるのが現実的</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="87" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="87" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="mamba--ssm--on%E3%81%AE%E4%BB%A3%E6%9B%BF%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3">Mamba / SSM — O(n)の代替アーキテクチャ</h1>
<ul>
<li><strong>State Space Models (SSM)</strong> — Transformerの O(n²) を克服</li>
<li></li>
<li><strong>線形状態空間の数式</strong>:</li>
<li><code>h'(t) = Ah(t) + Bx(t)</code> (連続時間)</li>
<li><code>y(t) = Ch(t) + Dx(t)</code></li>
<li></li>
<li><strong>S4 (2021)</strong>: 長い系列依存を O(n log n) で</li>
<li></li>
<li><strong>Mamba (Gu &amp; Dao, 2023)</strong>:</li>
<li>
<ul>
<li>「選択的」SSM: A, B, C が入力に依存</li>
</ul>
</li>
<li>
<ul>
<li><code>Δ, B, C = Linear(x)</code> — 入力に応じてゲーティング</li>
</ul>
</li>
<li>
<ul>
<li>計算量: <strong>O(n)</strong> — Transformerのスケーリング則も維持</li>
</ul>
</li>
<li>
<ul>
<li>推論: 状態を保持して定数時間でステップ実行</li>
</ul>
</li>
<li></li>
<li><strong>Mamba-2 / Jamba</strong>: MambaとTransformerのハイブリッド</li>
<li></li>
<li><strong>現状</strong>: 長系列では競争的だが一般タスクでは差あり</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="88" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="88" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="rwkv--rnn%E3%81%A8transformer%E3%81%AE%E8%9E%8D%E5%90%88">RWKV — RNNとTransformerの融合</h1>
<ul>
<li><strong>RWKV (Peng et al. 2023)</strong> — Receptance Weighted Key Value</li>
<li></li>
<li><strong>特徴</strong>:</li>
<li>
<ul>
<li>訓練時: Transformer的に並列計算</li>
</ul>
</li>
<li>
<ul>
<li>推論時: RNN的に O(1) 状態更新</li>
</ul>
</li>
<li></li>
<li><strong>Time-mixing層の数式</strong>:</li>
<li><code>wkv_t = (Σ_{i≤t} e^{-(t-i)w+k_i} v_i) / (Σ_{i≤t} e^{-(t-i)w+k_i})</code></li>
<li>
<ul>
<li><code>w</code>: 減衰率 (位置依存)</li>
</ul>
</li>
<li>
<ul>
<li><code>k, v</code>: Key/Value</li>
</ul>
</li>
<li></li>
<li><strong>利点</strong>:</li>
<li>
<ul>
<li>推論メモリ: 定数 (系列長によらず)</li>
</ul>
</li>
<li>
<ul>
<li>無限コンテキスト (理論上)</li>
</ul>
</li>
<li></li>
<li><strong>RWKV-6 (2024)</strong>: 14Bパラメータ / LLaMA相当の性能</li>
<li></li>
<li><strong>位置付け</strong>: エッジ推論・超長系列アプリケーションに有望</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="89" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="89" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="chain-of-thought-%E3%81%A8-reasoning%E5%BC%B7%E5%8C%96">Chain-of-Thought と Reasoning強化</h1>
<ul>
<li><strong>Chain-of-Thought (Wei et al. 2022)</strong>:</li>
<li>
<ul>
<li>「ステップバイステップで考えてください」</li>
</ul>
</li>
<li>
<ul>
<li>数学・推論タスクで性能が劇的向上</li>
</ul>
</li>
<li>
<ul>
<li>100B以上のモデルで効果 (小モデルでは逆効果も)</li>
</ul>
</li>
<li></li>
<li><strong>Self-Consistency (Wang et al. 2022)</strong>:</li>
<li>
<ul>
<li>複数の推論パスを生成 → 多数決で最終答</li>
</ul>
</li>
<li>
<ul>
<li>CoTより安定した性能向上</li>
</ul>
</li>
<li></li>
<li><strong>OpenAI o1 / o3 — Test-Time Compute</strong>:</li>
<li>
<ul>
<li>推論時により多くの計算を使う</li>
</ul>
</li>
<li>
<ul>
<li>内部的なCoTチェーンを学習 (RLHF的手法)</li>
</ul>
</li>
<li>
<ul>
<li>数学オリンピック・コーディング競技で人間超え</li>
</ul>
</li>
<li></li>
<li><strong>Scale of Thought</strong>: より長い推論チェーンほど精度向上</li>
<li>→ トークン数 ∝ 推論能力 という新しいスケーリング軸</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="90" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="90" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="multimodal%E9%80%B2%E5%8C%96--%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E8%B6%8A%E3%81%88%E3%82%8Bllm">Multimodal進化 — テキスト越えるLLM</h1>
<ul>
<li><strong>GPT-4V / GPT-4o (2023-2024)</strong>:</li>
<li>
<ul>
<li>画像→テキスト生成 / 音声リアルタイム対話</li>
</ul>
</li>
<li>
<ul>
<li>Video理解 (GPT-4o) / DALL·E 3統合</li>
</ul>
</li>
<li></li>
<li><strong>Gemini 1.5 Pro (Google, 2024)</strong>:</li>
<li>
<ul>
<li>1Mトークンコンテキスト + 動画・音声・画像</li>
</ul>
</li>
<li>
<ul>
<li>1時間の動画を直接処理</li>
</ul>
</li>
<li></li>
<li><strong>マルチモーダルアーキテクチャのパターン</strong>:</li>
<li>
<ol>
<li><strong>Early Fusion</strong>: 各モダリティをトークンに変換してConcatし統一LLMへ</li>
</ol>
</li>
<li>
<ul>
<li>LLaVA: ViT画像 → LLM</li>
</ul>
</li>
<li>
<ol start="2">
<li><strong>Cross-Modal Attention</strong>: テキストが画像に注意</li>
</ol>
</li>
<li>
<ul>
<li>Flamingo: Gated Cross-Attention</li>
</ul>
</li>
<li>
<ol start="3">
<li><strong>任意モダリティのトークン化</strong>:</li>
</ol>
</li>
<li>
<ul>
<li>音声: Whisper Encoder / 動画: Frame Sampling</li>
</ul>
</li>
<li></li>
<li><strong>方向性</strong>: テキスト・画像・音声・動画を統一空間で扱う</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="91" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="91" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="hallucination--%E5%B9%BB%E8%A6%9A%E5%95%8F%E9%A1%8C%E3%81%A8%E5%AF%BE%E7%AD%96">Hallucination — 幻覚問題と対策</h1>
<ul>
<li><strong>Hallucination</strong>: モデルが事実でない情報を自信を持って生成</li>
<li></li>
<li><strong>原因の分類</strong>:</li>
<li>
<ul>
<li><strong>知識の欠如</strong>: 訓練データに情報がない</li>
</ul>
</li>
<li>
<ul>
<li><strong>知識の古さ</strong>: カットオフ以降の情報</li>
</ul>
</li>
<li>
<ul>
<li><strong>注意の誤り</strong>: コンテキストを誤って参照</li>
</ul>
</li>
<li>
<ul>
<li><strong>学習バイアス</strong>: 尤もらしいが誤りのパターンを過学習</li>
</ul>
</li>
<li></li>
<li><strong>検出手法</strong>:</li>
<li>
<ul>
<li>SelfCheckGPT: 同プロンプトで複数回生成 → 不整合を検出</li>
</ul>
</li>
<li>
<ul>
<li>Factual ROUGE: 参照文書との一致度</li>
</ul>
</li>
<li></li>
<li><strong>軽減手法</strong>:</li>
<li>
<ul>
<li><strong>RAG</strong>: 最新・確実な情報をコンテキストとして注入</li>
</ul>
</li>
<li>
<ul>
<li><strong>Citation Generation</strong>: 根拠となる文書を引用させる</li>
</ul>
</li>
<li>
<ul>
<li><strong>RLHF with Factuality</strong>: 事実性を報酬に組み込む</li>
</ul>
</li>
<li>
<ul>
<li><strong>Uncertainty Estimation</strong>: 不確かなときは「わからない」と言わせる</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="92" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="92" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="constitutional-ai--safety">Constitutional AI / Safety</h1>
<ul>
<li><strong>Constitutional AI (Anthropic, 2022)</strong>:</li>
<li>
<ul>
<li>「憲法」(原則リスト) に基づく自己批評 + 改善</li>
</ul>
</li>
<li>
<ul>
<li>Critique: 「この応答は有害か？」→ LLM自身が評価</li>
</ul>
</li>
<li>
<ul>
<li>Revision: 批評に基づき応答を改善</li>
</ul>
</li>
<li>
<ul>
<li>RL-CAI: AI生成フィードバックでRLHF (RLAIF)</li>
</ul>
</li>
<li></li>
<li><strong>安全性確保の層</strong>:</li>
<li>
<ol>
<li><strong>Pre-training</strong>: 有害データのフィルタリング</li>
</ol>
</li>
<li>
<ol start="2">
<li><strong>SFT</strong>: 有害応答のデモンストレーションを排除</li>
</ol>
</li>
<li>
<ol start="3">
<li><strong>RLHF/CAI</strong>: 有害性を報酬で明示的にペナルティ</li>
</ol>
</li>
<li>
<ol start="4">
<li><strong>Runtime Guardrails</strong>: Llama Guard等のクラシファイア</li>
</ol>
</li>
<li></li>
<li><strong>主要な安全性課題</strong>:</li>
<li>
<ul>
<li><strong>Jailbreak</strong>: 制約を回避する悪意ある入力</li>
</ul>
</li>
<li>
<ul>
<li><strong>Prompt Injection</strong>: 外部テキストで指示を上書き</li>
</ul>
</li>
<li>
<ul>
<li><strong>Misuse Prevention</strong>: 武器・詐欺等への悪用</li>
</ul>
</li>
<li>
<ul>
<li><strong>Alignment Tax</strong>: 安全制約が性能を若干低下させる</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="93" data-class="lead" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="lead" data-marpit-pagination="93" style="--class:lead;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>

<h1 id="chapter-13-%E3%81%BE%E3%81%A8%E3%82%81">Chapter 13: まとめ</h1>
<ul>
<li>Key Takeaways · 今後の方向性 · 参考文献</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="94" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="94" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="key-takeaways--15%E3%81%AE%E3%82%A8%E3%83%83%E3%82%BB%E3%83%B3%E3%82%B9">Key Takeaways — 15のエッセンス</h1>
<ul>
<li><strong>アーキテクチャ</strong>:</li>
<li>
<ul>
<li>Self-Attention は O(n²) だが全位置を直接接続 → 表現力と並列性</li>
</ul>
</li>
<li>
<ul>
<li>√d_k スケーリングは softmax飽和を防ぐ数学的必然</li>
</ul>
</li>
<li>
<ul>
<li>Pre-Norm + RoPE + SwiGLU が現代LLMの標準構成</li>
</ul>
</li>
<li></li>
<li><strong>スケーリング</strong>:</li>
<li>
<ul>
<li>Chinchilla: 最適は N:D ≈ 1:20 (tokens)</li>
</ul>
</li>
<li>
<ul>
<li>創発能力は予測が難しい — スケールアップは驚きをもたらす</li>
</ul>
</li>
<li></li>
<li><strong>効率化</strong>:</li>
<li>
<ul>
<li>Flash Attention + PagedAttention は本番で必須</li>
</ul>
</li>
<li>
<ul>
<li>QLoRA: 個人GPUで70B規模のFTが可能</li>
</ul>
</li>
<li></li>
<li><strong>アライメント</strong>:</li>
<li>
<ul>
<li>DPO は RLHF の実用的代替 — よりシンプルで安定</li>
</ul>
</li>
<li>
<ul>
<li>データ品質 &gt; データ量 (LIMA)</li>
</ul>
</li>
<li></li>
<li><strong>次の波</strong>:</li>
<li>
<ul>
<li>Test-Time Compute (o1/o3) が新しいスケーリング軸</li>
</ul>
</li>
<li>
<ul>
<li>Mamba/RWKV が O(n²) の壁を解消しつつある</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="95" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="95" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E5%8F%82%E8%80%83%E8%AB%96%E6%96%87%E3%83%BB%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9-12">参考論文・リソース (1/2)</h1>
<ul>
<li><strong>基礎論文</strong>:</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need (2017)</a> — Vaswani et al.</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/1810.04805">BERT (2018)</a> — Devlin et al.</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2005.14165">GPT-3 (2020)</a> — Brown et al.</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2010.11929">ViT (2020)</a> — Dosovitskiy et al.</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/1910.10683">T5 (2019)</a> — Raffel et al.</li>
</ul>
</li>
<li></li>
<li><strong>効率化</strong>:</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2205.14135">Flash Attention (2022)</a> — Dao et al.</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2307.08691">Flash Attention 2 (2023)</a> — Dao</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2106.09685">LoRA (2021)</a> — Hu et al.</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2305.14314">QLoRA (2023)</a> — Dettmers et al.</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2309.06180">vLLM PagedAttention (2023)</a> — Kwon et al.</li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="96" data-class="invert" data-paginate="true" data-header="AI Transformer 完全ガイド 2026" data-footer="© 2026" data-theme="gaia" data-style="section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
" lang="en-US" class="invert" data-marpit-pagination="96" style="--class:invert;--paginate:true;--header:AI Transformer 完全ガイド 2026;--footer:© 2026;--theme:gaia;--style:section {
  font-size: 1.05em;
}
section pre code {
  font-size: 0.56em;
  line-height: 1.35;
}
section h1 {
  font-size: 1.6em;
}
section h2 {
  font-size: 1.3em;
}
;" data-marpit-pagination-total="96" data-size="16:9">
<header>AI Transformer 完全ガイド 2026</header>
<h1 id="%E5%8F%82%E8%80%83%E8%AB%96%E6%96%87%E3%83%BB%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9-22">参考論文・リソース (2/2)</h1>
<ul>
<li><strong>スケーリング &amp; アライメント</strong>:</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2001.08361">Scaling Laws (2020)</a> — Kaplan et al.</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2203.15556">Chinchilla (2022)</a> — Hoffmann et al.</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2206.07682">Emergent Abilities (2022)</a> — Wei et al.</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2203.02155">InstructGPT/RLHF (2022)</a> — OpenAI</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2305.18290">DPO (2023)</a> — Rafailov et al.</li>
</ul>
</li>
<li></li>
<li><strong>最新アーキテクチャ</strong>:</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2312.00752">Mamba (2023)</a> — Gu &amp; Dao</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2104.09864">RoPE (2021)</a> — Su et al.</li>
</ul>
</li>
<li>
<ul>
<li><a href="https://arxiv.org/abs/2108.12409">ALiBi (2022)</a> — Press et al.</li>
</ul>
</li>
<li></li>
<li><strong>学習リソース</strong>:</li>
<li>
<ul>
<li><a href="https://huggingface.co/learn">HuggingFace Course</a></li>
</ul>
</li>
<li>
<ul>
<li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Andrej Karpathy - Let's build GPT</a></li>
</ul>
</li>
<li>
<ul>
<li><a href="https://lilianweng.github.io">Lilian Weng's Blog</a></li>
</ul>
</li>
</ul>
<footer>© 2026</footer>
</section>
<script>!function(){"use strict";const t={h1:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"1"},style:"display: block; font-size: 2em; margin-block-start: 0.67em; margin-block-end: 0.67em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},h2:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"2"},style:"display: block; font-size: 1.5em; margin-block-start: 0.83em; margin-block-end: 0.83em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},h3:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"3"},style:"display: block; font-size: 1.17em; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},h4:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"4"},style:"display: block; margin-block-start: 1.33em; margin-block-end: 1.33em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},h5:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"5"},style:"display: block; font-size: 0.83em; margin-block-start: 1.67em; margin-block-end: 1.67em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},h6:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"6"},style:"display: block; font-size: 0.67em; margin-block-start: 2.33em; margin-block-end: 2.33em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},span:{proto:()=>HTMLSpanElement},pre:{proto:()=>HTMLElement,style:"display: block; font-family: monospace; white-space: pre; margin: 1em 0; --marp-auto-scaling-white-space: pre;"}},e="data-marp-auto-scaling-wrapper",i="data-marp-auto-scaling-svg",n="data-marp-auto-scaling-container";class s extends HTMLElement{container;containerSize;containerObserver;svg;svgComputedStyle;svgPreserveAspectRatio="xMinYMid meet";wrapper;wrapperSize;wrapperObserver;constructor(){super();const t=t=>([e])=>{const{width:i,height:n}=e.contentRect;this[t]={width:i,height:n},this.updateSVGRect()};this.attachShadow({mode:"open"}),this.containerObserver=new ResizeObserver(t("containerSize")),this.wrapperObserver=new ResizeObserver((...e)=>{t("wrapperSize")(...e),this.flushSvgDisplay()})}static get observedAttributes(){return["data-downscale-only"]}connectedCallback(){this.shadowRoot.innerHTML=`\n<style>\n  svg[${i}] { display: block; width: 100%; height: auto; vertical-align: top; }\n  span[${n}] { display: table; white-space: var(--marp-auto-scaling-white-space, nowrap); width: max-content; }\n</style>\n<div ${e}>\n  <svg part="svg" ${i}>\n    <foreignObject><span ${n}><slot></slot></span></foreignObject>\n  </svg>\n</div>\n    `.split(/\n\s*/).join(""),this.wrapper=this.shadowRoot.querySelector(`div[${e}]`)??void 0;const t=this.svg;this.svg=this.wrapper?.querySelector(`svg[${i}]`)??void 0,this.svg!==t&&(this.svgComputedStyle=this.svg?window.getComputedStyle(this.svg):void 0),this.container=this.svg?.querySelector(`span[${n}]`)??void 0,this.observe()}disconnectedCallback(){this.svg=void 0,this.svgComputedStyle=void 0,this.wrapper=void 0,this.container=void 0,this.observe()}attributeChangedCallback(){this.observe()}flushSvgDisplay(){const{svg:t}=this;t&&(t.style.display="inline",requestAnimationFrame(()=>{t.style.display=""}))}observe(){this.containerObserver.disconnect(),this.wrapperObserver.disconnect(),this.wrapper&&this.wrapperObserver.observe(this.wrapper),this.container&&this.containerObserver.observe(this.container),this.svgComputedStyle&&this.observeSVGStyle(this.svgComputedStyle)}observeSVGStyle(t){const e=()=>{const i=(()=>{const e=t.getPropertyValue("--preserve-aspect-ratio");if(e)return e.trim();return`x${(({textAlign:t,direction:e})=>{if(t.endsWith("left"))return"Min";if(t.endsWith("right"))return"Max";if("start"===t||"end"===t){let i="rtl"===e;return"end"===t&&(i=!i),i?"Max":"Min"}return"Mid"})(t)}YMid meet`})();i!==this.svgPreserveAspectRatio&&(this.svgPreserveAspectRatio=i,this.updateSVGRect()),t===this.svgComputedStyle&&requestAnimationFrame(e)};e()}updateSVGRect(){let t=Math.ceil(this.containerSize?.width??0);const e=Math.ceil(this.containerSize?.height??0);void 0!==this.dataset.downscaleOnly&&(t=Math.max(t,this.wrapperSize?.width??0));const i=this.svg?.querySelector(":scope > foreignObject");if(i?.setAttribute("width",`${t}`),i?.setAttribute("height",`${e}`),this.svg&&(this.svg.setAttribute("viewBox",`0 0 ${t} ${e}`),this.svg.setAttribute("preserveAspectRatio",this.svgPreserveAspectRatio),this.svg.style.height=t<=0||e<=0?"0":""),this.container){const t=this.svgPreserveAspectRatio.toLowerCase();this.container.style.marginLeft=t.startsWith("xmid")||t.startsWith("xmax")?"auto":"0",this.container.style.marginRight=t.startsWith("xmi")?"auto":"0"}}}const r=(t,{attrs:e={},style:i})=>class extends t{constructor(...t){super(...t);for(const[t,i]of Object.entries(e))this.hasAttribute(t)||this.setAttribute(t,i);this._shadow()}static get observedAttributes(){return["data-auto-scaling"]}connectedCallback(){this._update()}attributeChangedCallback(){this._update()}_shadow(){if(!this.shadowRoot)try{this.attachShadow({mode:"open"})}catch(t){if(!(t instanceof Error&&"NotSupportedError"===t.name))throw t}return this.shadowRoot}_update(){const t=this._shadow();if(t){const e=i?`<style>:host { ${i} }</style>`:"";let n="<slot></slot>";const{autoScaling:s}=this.dataset;if(void 0!==s){n=`<marp-auto-scaling exportparts="svg:auto-scaling" ${"downscale-only"===s?"data-downscale-only":""}>${n}</marp-auto-scaling>`}t.innerHTML=e+n}}};let o;const a=Symbol(),l=()=>o??(o=!!document.createElement("div",{is:"marp-auto-scaling"}).outerHTML.startsWith("<div is"),o);let c;const d="marpitSVGPolyfill:setZoomFactor,",h=Symbol(),g=Symbol();const p=()=>{const t="Apple Computer, Inc."===navigator.vendor,e=t?[v]:[],i={then:e=>(t?(async()=>{if(void 0===c){const t=document.createElement("canvas");t.width=10,t.height=10;const e=t.getContext("2d"),i=new Image(10,10),n=new Promise(t=>{i.addEventListener("load",()=>t())});i.crossOrigin="anonymous",i.src="data:image/svg+xml;charset=utf8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2210%22%20height%3D%2210%22%20viewBox%3D%220%200%201%201%22%3E%3CforeignObject%20width%3D%221%22%20height%3D%221%22%20requiredExtensions%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%3E%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22width%3A%201px%3B%20height%3A%201px%3B%20background%3A%20red%3B%20position%3A%20relative%22%3E%3C%2Fdiv%3E%3C%2FforeignObject%3E%3C%2Fsvg%3E",await n,e.drawImage(i,0,0),c=e.getImageData(5,5,1,1).data[3]<128}return c})().then(t=>{null==e||e(t?[v]:[])}):null==e||e([]),i)};return Object.assign(e,i)};let m,u;function v(t){const e="object"==typeof t&&t.target||document,i="object"==typeof t?t.zoom:t;window[g]||(Object.defineProperty(window,g,{configurable:!0,value:!0}),document.body.style.zoom=1.0001,document.body.offsetHeight,document.body.style.zoom=1,window.addEventListener("message",({data:t,origin:e})=>{if(e===window.origin)try{if(t&&"string"==typeof t&&t.startsWith(d)){const[,e]=t.split(","),i=Number.parseFloat(e);Number.isNaN(i)||(u=i)}}catch(t){console.error(t)}}));let n=!1;Array.from(e.querySelectorAll("svg[data-marpit-svg]"),t=>{var e,s,r,o;t.style.transform||(t.style.transform="translateZ(0)");const a=i||u||t.currentScale||1;m!==a&&(m=a,n=a);const l=t.getBoundingClientRect(),{length:c}=t.children;for(let i=0;i<c;i+=1){const n=t.children[i];if(n.getScreenCTM){const t=n.getScreenCTM();if(t){const i=null!==(s=null===(e=n.x)||void 0===e?void 0:e.baseVal.value)&&void 0!==s?s:0,c=null!==(o=null===(r=n.y)||void 0===r?void 0:r.baseVal.value)&&void 0!==o?o:0,d=n.children.length;for(let e=0;e<d;e+=1){const s=n.children[e];if("SECTION"===s.tagName){const{style:e}=s;e.transformOrigin||(e.transformOrigin=`${-i}px ${-c}px`),e.transform=`scale(${a}) matrix(${t.a}, ${t.b}, ${t.c}, ${t.d}, ${t.e-l.left}, ${t.f-l.top}) translateZ(0.0001px)`;break}}}}}}),!1!==n&&Array.from(e.querySelectorAll("iframe"),({contentWindow:t})=>{null==t||t.postMessage(`${d}${n}`,"null"===window.origin?"*":window.origin)})}function w({once:t=!1,target:e=document}={}){const i=function(t=document){if(t[h])return t[h];let e=!0;const i=()=>{e=!1,delete t[h]};Object.defineProperty(t,h,{configurable:!0,value:i});let n=[],s=!1;(async()=>{try{n=await p()}finally{s=!0}})();const r=()=>{for(const e of n)e({target:t});s&&0===n.length||e&&window.requestAnimationFrame(r)};return r(),i}(e);return t?(i(),()=>{}):i}m=1,u=void 0;const b=Symbol(),y=(e=document)=>{if("undefined"==typeof window)throw new Error("Marp Core's browser script is valid only in browser context.");if(((e=document)=>{const i=window[a];i||customElements.define("marp-auto-scaling",s);for(const n of Object.keys(t)){const s=`marp-${n}`,o=t[n].proto();l()&&o!==HTMLElement?i||customElements.define(s,r(o,{style:t[n].style}),{extends:n}):(i||customElements.define(s,r(HTMLElement,t[n])),e.querySelectorAll(`${n}[is="${s}"]`).forEach(t=>{t.outerHTML=t.outerHTML.replace(new RegExp(`^<${n}`,"i"),`<${s}`).replace(new RegExp(`</${n}>$`,"i"),`</${s}>`)}))}window[a]=!0})(e),e[b])return e[b];const i=w({target:e}),n=()=>{i(),delete e[b]},o=Object.assign(n,{cleanup:n,update:()=>y(e)});return Object.defineProperty(e,b,{configurable:!0,value:o}),o},f=document.currentScript;y(f?f.getRootNode():document)}();
</script></foreignObject></svg></div><script>/*!! License: https://unpkg.com/@marp-team/marp-cli@4.2.3/lib/bespoke.js.LICENSE.txt */
!function(){"use strict";function e(e){return e&&e.__esModule&&Object.prototype.hasOwnProperty.call(e,"default")?e.default:e}var t,n,r=(n||(n=1,t={from:function(e,t){var n,r=1===(e.parent||e).nodeType?e.parent||e:document.querySelector(e.parent||e),o=[].filter.call("string"==typeof e.slides?r.querySelectorAll(e.slides):e.slides||r.children,function(e){return"SCRIPT"!==e.nodeName}),a={},i=function(e,t){return(t=t||{}).index=o.indexOf(e),t.slide=e,t},s=function(e,t){a[e]=(a[e]||[]).filter(function(e){return e!==t})},c=function(e,t){return(a[e]||[]).reduce(function(e,n){return e&&!1!==n(t)},!0)},l=function(e,t){o[e]&&(n&&c("deactivate",i(n,t)),n=o[e],c("activate",i(n,t)))},d=function(e,t){var r=o.indexOf(n)+e;c(e>0?"next":"prev",i(n,t))&&l(r,t)},u={off:s,on:function(e,t){return(a[e]||(a[e]=[])).push(t),s.bind(null,e,t)},fire:c,slide:function(e,t){if(!arguments.length)return o.indexOf(n);c("slide",i(o[e],t))&&l(e,t)},next:d.bind(null,1),prev:d.bind(null,-1),parent:r,slides:o,destroy:function(e){c("destroy",i(n,e)),a={}}};return(t||[]).forEach(function(e){e(u)}),n||l(0),u}}),t),o=e(r);const a=document.body,i=(...e)=>history.replaceState(...e),s="",c="presenter",l="next",d=["",c,l],u="bespoke-marp-",f=`data-${u}`,m=(e,{protocol:t,host:n,pathname:r,hash:o}=location)=>{const a=e.toString();return`${t}//${n}${r}${a?"?":""}${a}${o}`},g=()=>a.dataset.bespokeView,p=e=>new URLSearchParams(location.search).get(e),v=(e,t={})=>{const n={location,setter:i,...t},r=new URLSearchParams(n.location.search);for(const t of Object.keys(e)){const n=e[t];"string"==typeof n?r.set(t,n):r.delete(t)}try{n.setter({...window.history.state??{}},"",m(r,n.location))}catch(e){console.error(e)}},h=(()=>{const e="bespoke-marp";try{return localStorage.setItem(e,e),localStorage.removeItem(e),!0}catch{return!1}})(),y=e=>{try{return localStorage.getItem(e)}catch{return null}},b=(e,t)=>{try{return localStorage.setItem(e,t),!0}catch{return!1}},w=e=>{try{return localStorage.removeItem(e),!0}catch{return!1}},x=(e,t)=>{const n="aria-hidden";t?e.setAttribute(n,"true"):e.removeAttribute(n)},k=e=>{e.parent.classList.add(`${u}parent`),e.slides.forEach(e=>e.classList.add(`${u}slide`)),e.on("activate",t=>{const n=`${u}active`,r=t.slide,o=r.classList,a=!o.contains(n);if(e.slides.forEach(e=>{e.classList.remove(n),x(e,!0)}),o.add(n),x(r,!1),a){const e=`${n}-ready`;o.add(e),document.body.clientHeight,o.remove(e)}})},$=e=>{let t=0,n=0;Object.defineProperty(e,"fragments",{enumerable:!0,value:e.slides.map(e=>[null,...e.querySelectorAll("[data-marpit-fragment]")])});const r=r=>void 0!==e.fragments[t][n+r],o=(r,o)=>{t=r,n=o,e.fragments.forEach((e,t)=>{e.forEach((e,n)=>{if(null==e)return;const a=t<r||t===r&&n<=o;e.setAttribute(`${f}fragment`,(a?"":"in")+"active");const i=`${f}current-fragment`;t===r&&n===o?e.setAttribute(i,"current"):e.removeAttribute(i)})}),e.fragmentIndex=o;const a={slide:e.slides[r],index:r,fragments:e.fragments[r],fragmentIndex:o};e.fire("fragment",a)};e.on("next",({fragment:a=!0})=>{if(a){if(r(1))return o(t,n+1),!1;const a=t+1;e.fragments[a]&&o(a,0)}else{const r=e.fragments[t].length;if(n+1<r)return o(t,r-1),!1;const a=e.fragments[t+1];a&&o(t+1,a.length-1)}}),e.on("prev",({fragment:a=!0})=>{if(r(-1)&&a)return o(t,n-1),!1;const i=t-1;e.fragments[i]&&o(i,e.fragments[i].length-1)}),e.on("slide",({index:t,fragment:n})=>{let r=0;if(void 0!==n){const o=e.fragments[t];if(o){const{length:e}=o;r=-1===n?e-1:Math.min(Math.max(n,0),e-1)}}o(t,r)}),o(0,0)},E=document,L=()=>!(!E.fullscreenEnabled&&!E.webkitFullscreenEnabled),S=()=>!(!E.fullscreenElement&&!E.webkitFullscreenElement),P=e=>{e.fullscreen=()=>{L()&&(async()=>{S()?(E.exitFullscreen||E.webkitExitFullscreen)?.call(E):((e=E.body)=>{(e.requestFullscreen||e.webkitRequestFullscreen)?.call(e)})()})()},document.addEventListener("keydown",t=>{"f"!==t.key&&"F11"!==t.key||t.altKey||t.ctrlKey||t.metaKey||!L()||(e.fullscreen(),t.preventDefault())})},_=`${u}inactive`,T=(e=2e3)=>({parent:t,fire:n})=>{const r=t.classList,o=e=>n(`marp-${e?"":"in"}active`);let a;const i=()=>{a&&clearTimeout(a),a=setTimeout(()=>{r.add(_),o()},e),r.contains(_)&&(r.remove(_),o(!0))};for(const e of["mousedown","mousemove","touchend"])document.addEventListener(e,i);setTimeout(i,0)},I=["AUDIO","BUTTON","INPUT","SELECT","TEXTAREA","VIDEO"],M=e=>{e.parent.addEventListener("keydown",e=>{if(!e.target)return;const t=e.target;(I.includes(t.nodeName)||"true"===t.contentEditable)&&e.stopPropagation()})},O=e=>{window.addEventListener("load",()=>{for(const t of e.slides){const e=t.querySelector("marp-auto-scaling, [data-auto-scaling], [data-marp-fitting]");t.setAttribute(`${f}load`,e?"":"hideable")}})},A=({interval:e=250}={})=>t=>{document.addEventListener("keydown",e=>{if(" "===e.key&&e.shiftKey)t.prev();else if("ArrowLeft"===e.key||"ArrowUp"===e.key||"PageUp"===e.key)t.prev({fragment:!e.shiftKey});else if(" "!==e.key||e.shiftKey)if("ArrowRight"===e.key||"ArrowDown"===e.key||"PageDown"===e.key)t.next({fragment:!e.shiftKey});else if("End"===e.key)t.slide(t.slides.length-1,{fragment:-1});else{if("Home"!==e.key)return;t.slide(0)}else t.next();e.preventDefault()});let n,r,o=0;t.parent.addEventListener("wheel",a=>{let i=!1;const s=(e,t)=>{e&&(i=i||((e,t)=>((e,t)=>{const n="X"===t?"Width":"Height";return e[`client${n}`]<e[`scroll${n}`]})(e,t)&&((e,t)=>{const{overflow:n}=e,r=e[`overflow${t}`];return"auto"===n||"scroll"===n||"auto"===r||"scroll"===r})(getComputedStyle(e),t))(e,t)),e?.parentElement&&s(e.parentElement,t)};if(0!==a.deltaX&&s(a.target,"X"),0!==a.deltaY&&s(a.target,"Y"),i)return;a.preventDefault();const c=Math.sqrt(a.deltaX**2+a.deltaY**2);if(void 0!==a.wheelDelta){if(void 0===a.webkitForce&&Math.abs(a.wheelDelta)<40)return;if(a.deltaMode===a.DOM_DELTA_PIXEL&&c<4)return}else if(a.deltaMode===a.DOM_DELTA_PIXEL&&c<12)return;r&&clearTimeout(r),r=setTimeout(()=>{n=0},e);const l=Date.now()-o<e,d=c<=n;if(n=c,l||d)return;let u;(a.deltaX>0||a.deltaY>0)&&(u="next"),(a.deltaX<0||a.deltaY<0)&&(u="prev"),u&&(t[u](),o=Date.now())})},C=(e=`.${u}osc`)=>{const t=document.querySelector(e);if(!t)return()=>{};const n=(e,n)=>{t.querySelectorAll(`[${f}osc=${JSON.stringify(e)}]`).forEach(n)};return L()||n("fullscreen",e=>e.style.display="none"),h||n("presenter",e=>{e.disabled=!0,e.title="Presenter view is disabled due to restricted localStorage."}),e=>{t.addEventListener("click",t=>{if(t.target instanceof HTMLElement){const{bespokeMarpOsc:n}=t.target.dataset;n&&t.target.blur();const r={fragment:!t.shiftKey};"next"===n?e.next(r):"prev"===n?e.prev(r):"fullscreen"===n?e?.fullscreen():"presenter"===n&&e.openPresenterView()}}),e.parent.appendChild(t),e.on("activate",({index:t})=>{n("page",n=>n.textContent=`Page ${t+1} of ${e.slides.length}`)}),e.on("fragment",({index:t,fragments:r,fragmentIndex:o})=>{n("prev",e=>e.disabled=0===t&&0===o),n("next",n=>n.disabled=t===e.slides.length-1&&o===r.length-1)}),e.on("marp-active",()=>x(t,!1)),e.on("marp-inactive",()=>x(t,!0)),L()&&(e=>{for(const t of["","webkit"])E.addEventListener(t+"fullscreenchange",e)})(()=>n("fullscreen",e=>e.classList.toggle("exit",L()&&S())))}},D=e=>{window.addEventListener("message",t=>{if(t.origin!==window.origin)return;const[n,r]=t.data.split(":");if("navigate"===n){const[t,n]=r.split(",");let o=Number.parseInt(t,10),a=Number.parseInt(n,10)+1;a>=e.fragments[o].length&&(o+=1,a=0),e.slide(o,{fragment:a})}})};var N,B,q,K,F,j,V,U={exports:{}},X=(N||(N=1,U.exports=(B=["area","base","br","col","command","embed","hr","img","input","keygen","link","meta","param","source","track","wbr"],q=function(e){return String(e).replace(/[&<>"']/g,function(e){return"&"+K[e]+";"})},K={"&":"amp","<":"lt",">":"gt",'"':"quot","'":"apos"},F="dangerouslySetInnerHTML",j={className:"class",htmlFor:"for"},V={},function(e,t){var n=[],r="";t=t||{};for(var o=arguments.length;o-- >2;)n.push(arguments[o]);if("function"==typeof e)return t.children=n.reverse(),e(t);if(e){if(r+="<"+e,t)for(var a in t)!1!==t[a]&&null!=t[a]&&a!==F&&(r+=" "+(j[a]?j[a]:q(a))+'="'+q(t[a])+'"');r+=">"}if(-1===B.indexOf(e)){if(t[F])r+=t[F].__html;else for(;n.length;){var i=n.pop();if(i)if(i.pop)for(var s=i.length;s--;)n.push(i[s]);else r+=!0===V[i]?i:q(i)}r+=e?"</"+e+">":""}return V[r]=!0,r})),U.exports),H=e(X);const R=({children:e})=>H(null,null,...e),W=`${u}presenter-`,J={container:`${W}container`,dragbar:`${W}dragbar-container`,next:`${W}next`,nextContainer:`${W}next-container`,noteContainer:`${W}note-container`,noteWrapper:`${W}note-wrapper`,noteButtons:`${W}note-buttons`,infoContainer:`${W}info-container`,infoPage:`${W}info-page`,infoPageText:`${W}info-page-text`,infoPagePrev:`${W}info-page-prev`,infoPageNext:`${W}info-page-next`,noteButtonsBigger:`${W}note-bigger`,noteButtonsSmaller:`${W}note-smaller`,infoTime:`${W}info-time`,infoTimer:`${W}info-timer`},Y=e=>{const{title:t}=document;document.title="[Presenter view]"+(t?` - ${t}`:"");const n={},r=e=>(n[e]=n[e]||document.querySelector(`.${e}`),n[e]);document.body.appendChild((e=>{const t=document.createElement("div");return t.className=J.container,t.appendChild(e),t.insertAdjacentHTML("beforeend",H(R,null,H("div",{class:J.nextContainer},H("iframe",{class:J.next,src:"?view=next"})),H("div",{class:J.dragbar}),H("div",{class:J.noteContainer},H("div",{class:J.noteWrapper}),H("div",{class:J.noteButtons},H("button",{class:J.noteButtonsSmaller,tabindex:"-1",title:"Smaller notes font size"},"Smaller notes font size"),H("button",{class:J.noteButtonsBigger,tabindex:"-1",title:"Bigger notes font size"},"Bigger notes font size"))),H("div",{class:J.infoContainer},H("div",{class:J.infoPage},H("button",{class:J.infoPagePrev,tabindex:"-1",title:"Previous"},"Previous"),H("span",{class:J.infoPageText}),H("button",{class:J.infoPageNext,tabindex:"-1",title:"Next"},"Next")),H("time",{class:J.infoTime,title:"Current time"}),H("time",{class:J.infoTimer,title:"Timer"})))),t})(e.parent)),(e=>{let t=!1;r(J.dragbar).addEventListener("mousedown",()=>{t=!0,r(J.dragbar).classList.add("active")}),window.addEventListener("mouseup",()=>{t=!1,r(J.dragbar).classList.remove("active")}),window.addEventListener("mousemove",e=>{if(!t)return;const n=e.clientX/document.documentElement.clientWidth*100;r(J.container).style.setProperty("--bespoke-marp-presenter-split-ratio",`${Math.max(0,Math.min(100,n))}%`)}),r(J.nextContainer).addEventListener("click",()=>e.next());const n=r(J.next),o=(a=n,(e,t)=>a.contentWindow?.postMessage(`navigate:${e},${t}`,"null"===window.origin?"*":window.origin));var a;n.addEventListener("load",()=>{r(J.nextContainer).classList.add("active"),o(e.slide(),e.fragmentIndex),e.on("fragment",({index:e,fragmentIndex:t})=>o(e,t))});const i=document.querySelectorAll(".bespoke-marp-note");i.forEach(e=>{e.addEventListener("keydown",e=>e.stopPropagation()),r(J.noteWrapper).appendChild(e)}),e.on("activate",()=>i.forEach(t=>t.classList.toggle("active",t.dataset.index==e.slide())));let s=0;const c=e=>{s=Math.max(-5,s+e),r(J.noteContainer).style.setProperty("--bespoke-marp-note-font-scale",(1.2**s).toFixed(4))},l=()=>c(1),d=()=>c(-1),u=r(J.noteButtonsBigger),f=r(J.noteButtonsSmaller);u.addEventListener("click",()=>{u.blur(),l()}),f.addEventListener("click",()=>{f.blur(),d()}),document.addEventListener("keydown",e=>{"+"===e.key&&l(),"-"===e.key&&d()},!0),e.on("activate",({index:t})=>{r(J.infoPageText).textContent=`${t+1} / ${e.slides.length}`});const m=r(J.infoPagePrev),g=r(J.infoPageNext);m.addEventListener("click",t=>{m.blur(),e.prev({fragment:!t.shiftKey})}),g.addEventListener("click",t=>{g.blur(),e.next({fragment:!t.shiftKey})}),e.on("fragment",({index:t,fragments:n,fragmentIndex:r})=>{m.disabled=0===t&&0===r,g.disabled=t===e.slides.length-1&&r===n.length-1});let p=new Date;const v=()=>{const e=new Date,t=e=>`${Math.floor(e)}`.padStart(2,"0"),n=e.getTime()-p.getTime(),o=t(n/1e3%60),a=t(n/1e3/60%60),i=t(n/36e5%24);r(J.infoTime).textContent=e.toLocaleTimeString(),r(J.infoTimer).textContent=`${i}:${a}:${o}`};v(),setInterval(v,250),r(J.infoTimer).addEventListener("click",()=>{p=new Date})})(e)},z=e=>{if(!(e=>e.syncKey&&"string"==typeof e.syncKey)(e))throw new Error("The current instance of Bespoke.js is invalid for Marp bespoke presenter plugin.");Object.defineProperties(e,{openPresenterView:{enumerable:!0,value:G},presenterUrl:{enumerable:!0,get:Q}}),h&&document.addEventListener("keydown",t=>{"p"!==t.key||t.altKey||t.ctrlKey||t.metaKey||(t.preventDefault(),e.openPresenterView())})};function G(){const{max:e,floor:t}=Math,n=e(t(.85*window.innerWidth),640),r=e(t(.85*window.innerHeight),360);return window.open(this.presenterUrl,W+this.syncKey,`width=${n},height=${r},menubar=no,toolbar=no`)}function Q(){const e=new URLSearchParams(location.search);return e.set("view","presenter"),e.set("sync",this.syncKey),m(e)}const Z=e=>{const t=g();return t===l&&e.appendChild(document.createElement("span")),{[s]:z,[c]:Y,[l]:D}[t]},ee=e=>{e.on("activate",t=>{document.querySelectorAll(".bespoke-progress-parent > .bespoke-progress-bar").forEach(n=>{n.style.flexBasis=100*t.index/(e.slides.length-1)+"%"})})},te=e=>{const t=Number.parseInt(e,10);return Number.isNaN(t)?null:t},ne=(e={})=>{const t={history:!0,...e};return e=>{let n=!0;const r=e=>{const t=n;try{return n=!0,e()}finally{n=t}},o=(t={fragment:!0})=>{let n=t.fragment?te(p("f")||""):null;((t,n)=>{const{min:r,max:o}=Math,{fragments:a,slides:i}=e,s=o(0,r(t,i.length-1)),c=o(0,r(n||0,a[s].length-1));s===e.slide()&&c===e.fragmentIndex||e.slide(s,{fragment:c})})((()=>{if(location.hash){const[t]=location.hash.slice(1).split(":~:");if(/^\d+$/.test(t))return(te(t)??1)-1;const r=document.getElementById(t)||document.querySelector(`a[name="${CSS.escape(t)}"]`);if(r){const{length:t}=e.slides;for(let o=0;o<t;o+=1)if(e.slides[o].contains(r)){const t=e.fragments?.[o],a=r.closest("[data-marpit-fragment]");if(t&&a){const e=t.indexOf(a);e>=0&&(n=e)}return o}}}return 0})(),n)};e.on("fragment",({index:e,fragmentIndex:r})=>{n||v({f:0===r||r.toString()},{location:{...location,hash:`#${e+1}`},setter:(...e)=>t.history?history.pushState(...e):history.replaceState(...e)})}),setTimeout(()=>{o(),window.addEventListener("hashchange",()=>r(()=>{o({fragment:!1}),v({f:void 0})})),window.addEventListener("popstate",()=>{n||r(()=>o())}),n=!1},0)}},re=(e={})=>{const t=e.key||window.history.state?.marpBespokeSyncKey||Math.random().toString(36).slice(2),n=`bespoke-marp-sync-${t}`;var r;r={marpBespokeSyncKey:t},v({},{setter:(e,...t)=>i({...e,...r},...t)});const o=()=>{const e=y(n);return e?JSON.parse(e):Object.create(null)},a=e=>{const t=o(),r={...t,...e(t)};return b(n,JSON.stringify(r)),r},s=()=>{window.removeEventListener("pageshow",s),a(e=>({reference:(e.reference||0)+1}))};return e=>{s(),Object.defineProperty(e,"syncKey",{value:t,enumerable:!0});let r=!0;setTimeout(()=>{e.on("fragment",e=>{r&&a(()=>({index:e.index,fragmentIndex:e.fragmentIndex}))})},0),window.addEventListener("storage",t=>{if(t.key===n&&t.oldValue&&t.newValue){const n=JSON.parse(t.oldValue),o=JSON.parse(t.newValue);if(n.index!==o.index||n.fragmentIndex!==o.fragmentIndex)try{r=!1,e.slide(o.index,{fragment:o.fragmentIndex,forSync:!0})}finally{r=!0}}});const i=()=>{const{reference:e}=o();void 0===e||e<=1?w(n):a(()=>({reference:e-1}))};window.addEventListener("pagehide",e=>{e.persisted&&window.addEventListener("pageshow",s),i()}),e.on("destroy",i)}},{PI:oe,abs:ae,sqrt:ie,atan2:se}=Math,ce={passive:!0},le=({slope:e=-.7,swipeThreshold:t=30}={})=>n=>{let r;const o=n.parent,a=e=>{const t=o.getBoundingClientRect();return{x:e.pageX-(t.left+t.right)/2,y:e.pageY-(t.top+t.bottom)/2}};o.addEventListener("touchstart",({touches:e})=>{r=1===e.length?a(e[0]):void 0},ce),o.addEventListener("touchmove",e=>{if(r)if(1===e.touches.length){e.preventDefault();const t=a(e.touches[0]),n=t.x-r.x,o=t.y-r.y;r.delta=ie(ae(n)**2+ae(o)**2),r.radian=se(n,o)}else r=void 0}),o.addEventListener("touchend",o=>{if(r){if(r.delta&&r.delta>=t&&r.radian){const t=(r.radian-e+oe)%(2*oe)-oe;n[t<0?"next":"prev"](),o.stopPropagation()}r=void 0}},ce)},de=new Map;de.clear(),de.set("none",{backward:{both:void 0,incoming:void 0,outgoing:void 0},forward:{both:void 0,incoming:void 0,outgoing:void 0}});const ue={both:"",outgoing:"outgoing-",incoming:"incoming-"},fe={forward:"",backward:"-backward"},me=e=>`--marp-bespoke-transition-animation-${e}`,ge=e=>`--marp-transition-${e}`,pe=me("name"),ve=me("duration"),he=e=>new Promise(t=>{const n={},r=document.createElement("div"),o=e=>{r.remove(),t(e)};r.addEventListener("animationstart",()=>o(n)),Object.assign(r.style,{animationName:e,animationDuration:"1s",animationFillMode:"both",animationPlayState:"paused",position:"absolute",pointerEvents:"none"}),document.body.appendChild(r);const a=getComputedStyle(r).getPropertyValue(ge("duration"));a&&Number.parseFloat(a)>=0&&(n.defaultDuration=a),((e,t)=>{requestAnimationFrame(()=>{e.style.animationPlayState="running",requestAnimationFrame(()=>t(void 0))})})(r,o)}),ye=async e=>de.has(e)?de.get(e):(e=>{const t={},n=[];for(const[r,o]of Object.entries(ue))for(const[a,i]of Object.entries(fe)){const s=`marp-${o}transition${i}-${e}`;n.push(he(s).then(e=>{t[a]=t[a]||{},t[a][r]=e?{...e,name:s}:void 0}))}return Promise.all(n).then(()=>t)})(e).then(t=>(de.set(e,t),t)),be=e=>Object.values(e).flatMap(Object.values).every(e=>!e),we=(e,{type:t,backward:n})=>{const r=e[n?"backward":"forward"],o=(()=>{const e=r[t],n=e=>({[pe]:e.name});if(e)return n(e);if(r.both){const e=n(r.both);return"incoming"===t&&(e[me("direction")]="reverse"),e}})();return!o&&n?we(e,{type:t,backward:!1}):o||{[pe]:"__bespoke_marp_transition_no_animation__"}},xe=e=>{if(e)try{const t=JSON.parse(e);if((e=>{if("object"!=typeof e)return!1;const t=e;return"string"==typeof t.name&&(void 0===t.duration||"string"==typeof t.duration)})(t))return t}catch{}},ke="_tSId",$e="_tA",Ee="bespoke-marp-transition-warming-up",Le=window.matchMedia("(prefers-reduced-motion: reduce)"),Se="__bespoke_marp_transition_reduced_outgoing__",Pe="__bespoke_marp_transition_reduced_incoming__",_e={forward:{both:void 0,incoming:{name:Pe},outgoing:{name:Se}},backward:{both:void 0,incoming:{name:Pe},outgoing:{name:Se}}},Te=e=>{if(!document.startViewTransition)return;const t=t=>(void 0!==t&&(e._tD=t),e._tD);let n;t(!1),((...e)=>{CSS.registerProperty({name:ge("duration"),syntax:"<time>",inherits:!0,initialValue:"-1s"});const t=[...new Set(e).values()];return Promise.all(t.map(e=>ye(e))).then()})(...Array.from(document.querySelectorAll("section[data-transition], section[data-transition-back]")).flatMap(e=>[e.dataset.transition,e.dataset.transitionBack].flatMap(e=>{const t=xe(e);return[t?.name,t?.builtinFallback?`__builtin__${t.name}`:void 0]}).filter(e=>!!e))).then(()=>{document.querySelectorAll("style").forEach(e=>{e.innerHTML=e.innerHTML.replace(/--marp-transition-duration:[^;}]*[;}]/g,e=>e.slice(0,-1)+"!important"+e.slice(-1))})});const r=(n,{back:r,cond:o})=>a=>{const i=t();if(i)return!!a[$e]||!("object"!=typeof i||(i.skipTransition(),!a.forSync));if(!o(a))return!0;const s=e.slides[e.slide()],c=()=>a.back??r,l="data-transition"+(c()?"-back":""),d=s.querySelector(`section[${l}]`);if(!d)return!0;const u=xe(d.getAttribute(l)??void 0);return!u||((async(e,{builtinFallback:t=!0}={})=>{let n=await ye(e);if(be(n)){if(!t)return;return n=await ye(`__builtin__${e}`),be(n)?void 0:n}return n})(u.name,{builtinFallback:u.builtinFallback}).then(e=>{if(!e){t(!0);try{n(a)}finally{t(!1)}return}let r=e;Le.matches&&(console.warn("Use a constant animation to transition because preferring reduced motion by viewer has detected."),r=_e);const o=document.getElementById(ke);o&&o.remove();const i=document.createElement("style");i.id=ke,document.head.appendChild(i),((e,t)=>{const n=[`:root{${ge("direction")}:${t.backward?-1:1};}`,":root:has(.bespoke-marp-inactive){cursor:none;}"],r=t=>{const n=e[t].both?.defaultDuration||e[t].outgoing?.defaultDuration||e[t].incoming?.defaultDuration;return"forward"===t?n:n||r("forward")},o=t.duration||r(t.backward?"backward":"forward");void 0!==o&&n.push(`::view-transition-group(*){${ve}:${o};}`);const a=e=>Object.entries(e).map(([e,t])=>`${e}:${t};`).join("");return n.push(`::view-transition-old(root){${a(we(e,{...t,type:"outgoing"}))}}`,`::view-transition-new(root){${a(we(e,{...t,type:"incoming"}))}}`),n})(r,{backward:c(),duration:u.duration}).forEach(e=>i.sheet?.insertRule(e));const s=document.documentElement.classList;s.add(Ee);let l=!1;const d=()=>{l||(n(a),l=!0,s.remove(Ee))},f=()=>{t(!1),i.remove(),s.remove(Ee)};try{t(!0);const e=document.startViewTransition(d);t(e),e.finished.finally(f)}catch(e){console.error(e),d(),f()}}),!1)};e.on("prev",r(t=>e.prev({...t,[$e]:!0}),{back:!0,cond:e=>e.index>0&&!((e.fragment??1)&&n.fragmentIndex>0)})),e.on("next",r(t=>e.next({...t,[$e]:!0}),{cond:t=>t.index+1<e.slides.length&&!(n.fragmentIndex+1<n.fragments.length)})),setTimeout(()=>{e.on("slide",r(t=>e.slide(t.index,{...t,[$e]:!0}),{cond:t=>{const n=e.slide();return t.index!==n&&(t.back=t.index<n,!0)}}))},0),e.on("fragment",e=>{n=e})};let Ie;const Me=()=>(void 0===Ie&&(Ie="wakeLock"in navigator&&navigator.wakeLock),Ie),Oe=async()=>{const e=Me();if(e)try{return await e.request("screen")}catch(e){console.warn(e)}return null},Ae=async()=>{if(!Me())return;let e;const t=()=>{e&&"visible"===document.visibilityState&&Oe()};for(const e of["visibilitychange","fullscreenchange"])document.addEventListener(e,t);return e=await Oe(),e};((e=document.getElementById(":$p"))=>{(()=>{const e=p("view");a.dataset.bespokeView=e===l||e===c?e:""})();const t=(e=>{const t=p(e);return v({[e]:void 0}),t})("sync")||void 0;o.from(e,((...e)=>{const t=d.findIndex(e=>g()===e);return e.map(([e,n])=>e[t]&&n).filter(e=>e)})([[1,1,0],re({key:t})],[[1,1,1],Z(e)],[[1,1,0],M],[[1,1,1],k],[[1,0,0],T()],[[1,1,1],O],[[1,1,1],ne({history:!1})],[[1,1,0],A()],[[1,1,0],P],[[1,0,0],ee],[[1,1,0],le()],[[1,0,0],C()],[[1,0,0],Te],[[1,1,1],$],[[1,1,0],Ae]))})()}();</script></body></html>